{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT tutorial\n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "GAP preprocessing example\n",
    "https://www.kaggle.com/code/sunilcube/text-data-gendered-pronoun-resolution\n",
    "\n",
    "Transformer Explanation: https://nlp.seas.harvard.edu/2018/04/03/attention.html http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "from typing import *\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Display the entire text\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertTokenizer,\n",
    "    BertModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../model/data/train.tsv\"\n",
    "valid_path = \"../../model/data/dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path: str) -> List[Dict]:\n",
    "    samples: List[Dict] = []\n",
    "    pron_counter = Counter()\n",
    "    with open(path) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            (\n",
    "                id,\n",
    "                text,\n",
    "                pron,\n",
    "                p_offset,\n",
    "                entity_A,\n",
    "                offset_A,\n",
    "                is_coref_A,\n",
    "                entity_B,\n",
    "                offset_B,\n",
    "                is_coref_B,\n",
    "                url,\n",
    "            ) = line.strip().split(\"\\t\")\n",
    "            pron_counter[pron.lower()] += 1\n",
    "            samples.append(\n",
    "                {\n",
    "                    \"id\": id,\n",
    "                    \"text\": text,\n",
    "                    \"pron\": pron,\n",
    "                    \"p_offset\": int(p_offset),\n",
    "                    \"entity_A\": entity_A,\n",
    "                    \"offset_A\": int(offset_A),\n",
    "                    \"is_coref_A\": is_coref_A,\n",
    "                    \"entity_B\": entity_B,\n",
    "                    \"offset_B\": int(offset_B),\n",
    "                    \"is_coref_B\": is_coref_B,\n",
    "                    \"url\": url,\n",
    "                }\n",
    "            )\n",
    "    print(pron_counter)\n",
    "    return samples, pron_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'his': 904, 'her': 773, 'he': 610, 'she': 555, 'him': 157})\n",
      "Counter({'her': 140, 'his': 108, 'he': 93, 'she': 87, 'him': 26})\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_pron_counter = read_dataset(train_path)\n",
    "valid_dataset, valid_pron_counter = read_dataset(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training dataset there is a slightly bias towards the male pronouns (1671 M vs 1328 F)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 1328, 'M': 1671}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gender_pron_counter = {\n",
    "    \"F\": train_pron_counter['her'] + train_pron_counter['she'],\n",
    "    \"M\": train_pron_counter['his'] + train_pron_counter['him'] + train_pron_counter['he']\n",
    "}\n",
    "train_gender_pron_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation dataset is perfectly balanced between gender pronouns (227 pronous for both Female and Male)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 227, 'M': 227}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_gender_pron_counter = {\n",
    "    \"F\": valid_pron_counter['her'] + valid_pron_counter['she'],\n",
    "    \"M\": valid_pron_counter['his'] + valid_pron_counter['him'] + valid_pron_counter['he']\n",
    "}\n",
    "valid_gender_pron_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "def plot_freq(frequencies: dict, title: str = \"plot\"):\n",
    "    \"\"\"\n",
    "    A bar chart with frequency of tokens.\n",
    "    \"\"\"\n",
    "    figure(figsize=(8, 4), dpi=80)\n",
    "\n",
    "    words = list(frequencies.keys())\n",
    "    freq = list(frequencies.values())\n",
    "\n",
    "    plt.bar(range(len(frequencies)), freq, tick_label=words)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_freq(train_pron_counter)\n",
    "# plot_freq(valid_pron_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_valid = pd.DataFrame(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.translate(str.maketrans(\"`\", \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_valid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].map(clean_text)\n",
    "df_train['entity_A'] = df_train['entity_A'].map(clean_text) \n",
    "df_train['entity_B'] = df_train['entity_B'].map(clean_text) \n",
    "\n",
    "df_valid['text'] = df_valid['text'].map(clean_text)\n",
    "df_valid['entity_A'] = df_valid['entity_A'].map(clean_text)\n",
    "df_valid['entity_B'] = df_valid['entity_B'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean_path = \"../../model/data/train_clean.tsv\"\n",
    "# valid_clean_path = \"../../model/data/valid_clean.tsv\"\n",
    "# df_train.to_csv(path_or_buf=train_clean_path, sep=\"\\t\", index=False)\n",
    "# df_valid.to_csv(path_or_buf=valid_clean_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'his': 904, 'her': 773, 'he': 610, 'she': 555, 'him': 157})\n"
     ]
    }
   ],
   "source": [
    "prova = read_dataset(train_clean_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pd.read_csv(filepath_or_buffer=train_clean_path, sep=\"\\t\")\n",
    "# v = pd.read_csv(filepath_or_buffer=valid_clean_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_A</th>\n",
       "      <th>is_coref_A</th>\n",
       "      <th>entity_B</th>\n",
       "      <th>is_coref_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angeloz</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hell</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>Martin</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Robert Brandon</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>Arthur Davies</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>John Frederick Mowbray-Clarke</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Katharine Anthony</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Madge Jenison</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Carole</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Lillian Grey</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>Wilcox</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Ivor Novello</td>\n",
       "      <td>FALSE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2999 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               entity_A is_coref_A                       entity_B is_coref_B\n",
       "0        Cheryl Cassidy       TRUE                        Pauline      FALSE\n",
       "1             MacKenzie       TRUE                  Bernard Leach      FALSE\n",
       "2               Angeloz      FALSE                     De la Sota       TRUE\n",
       "3                  Hell      FALSE                Henry Rosenthal       TRUE\n",
       "4     Kitty Oppenheimer      FALSE                         Rivera       TRUE\n",
       "...                 ...        ...                            ...        ...\n",
       "2994             Martin       TRUE                 Robert Brandon      FALSE\n",
       "2995      Arthur Davies       TRUE  John Frederick Mowbray-Clarke      FALSE\n",
       "2996  Katharine Anthony      FALSE                  Madge Jenison      FALSE\n",
       "2997             Carole       TRUE                   Lillian Grey      FALSE\n",
       "2998             Wilcox       TRUE                   Ivor Novello      FALSE\n",
       "\n",
       "[2999 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_entities = df_train[['entity_A', 'is_coref_A', 'entity_B', 'is_coref_B']]\n",
    "df_train_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 2999\n",
      "# sentences neither: 315 (10.50%)\n",
      "# sentences A pronoun: 1331 (44.38%)\n",
      "# sentences B pronoun: 1353 (45.12%)\n"
     ]
    }
   ],
   "source": [
    "# Dataframe contiaining sentences where neither A nor B entities are the right coreference entities to the pronoun\n",
    "df_train_neither_ent = df_train.loc[(df_train['is_coref_A'] == \"FALSE\") & (df_train['is_coref_B'] == \"FALSE\")]\n",
    "\n",
    "df_train_A_ent = df_train.loc[df_train['is_coref_A'] == \"TRUE\"]\n",
    "df_train_B_ent = df_train.loc[df_train['is_coref_B'] == \"TRUE\"]\n",
    "\n",
    "print(\"# sentences:\", df_train.shape[0])\n",
    "print(f\"# sentences neither: {df_train_neither_ent.shape[0]} ({((df_train_neither_ent.shape[0] / df_train.shape[0]) * 100):.2f}%)\")\n",
    "print(f\"# sentences A pronoun: {df_train_A_ent.shape[0]} ({((df_train_A_ent.shape[0] / df_train.shape[0]) * 100):.2f}%)\")\n",
    "print(f\"# sentences B pronoun: {df_train_B_ent.shape[0]} ({((df_train_B_ent.shape[0] / df_train.shape[0]) * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 454\n",
      "# sentences neither: 62 (13.66%)\n",
      "# sentences A pronoun: 187 (41.19%)\n",
      "# sentences B pronoun: 205 (45.15%)\n"
     ]
    }
   ],
   "source": [
    "# Dataframe contiaining sentences where neither A nor B entities are the right coreference entities to the pronoun\n",
    "df_valid_neither_ent = df_valid.loc[(df_valid['is_coref_A'] == \"FALSE\") & (df_valid['is_coref_B'] == \"FALSE\")]\n",
    "\n",
    "df_valid_A_ent = df_valid.loc[df_valid['is_coref_A'] == \"TRUE\"]\n",
    "df_valid_B_ent = df_valid.loc[df_valid['is_coref_B'] == \"TRUE\"]\n",
    "\n",
    "print(\"# sentences:\", df_valid.shape[0])\n",
    "print(f\"# sentences neither: {df_valid_neither_ent.shape[0]} ({((df_valid_neither_ent.shape[0] / df_valid.shape[0]) * 100):.2f}%)\")\n",
    "print(f\"# sentences A pronoun: {df_valid_A_ent.shape[0]} ({((df_valid_A_ent.shape[0] / df_valid.shape[0]) * 100):.2f}%)\")\n",
    "print(f\"# sentences B pronoun: {df_valid_B_ent.shape[0]} ({((df_valid_B_ent.shape[0] / df_valid.shape[0]) * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are pretty balanced with respect the A B pronouns; but we have fewer examples of \"neither\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1779    Jeanine Basinger (born 3 February 1936), a film historian, is Corwin-Fuller Professor of Film Studies and Founder and Curator of The Cinema Archives at Wesleyan University, Middletown, Connecticut. She is also a Trustee of the American Film Institute (which awarded her an honorary degree, a Doctorate of Humane Letters, on June 7, 2006 ), a member of the Steering Committee of the National Center for Film and Video Preservation, and one of the Board of Advisors for the Association of Independent Video and Filmmakers.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_text_only = df_train['text']\n",
    "df_train_text_only.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_text_only.map(lambda sentence: len(tokenizer.tokenize(sentence))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean lenght: 430.92\n",
      "Min lenght: 69.00\n",
      "Max lenght: 1347.00\n"
     ]
    }
   ],
   "source": [
    "df_train_text_lens = df_train_text_only.map(lambda sentence: len(sentence))\n",
    "\n",
    "print(f\"Mean lenght: {df_train_text_lens.mean():.2f}\")\n",
    "print(f\"Min lenght: {df_train_text_lens.min():.2f}\")\n",
    "print(f\"Max lenght: {df_train_text_lens.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125    It soon moved to Castle Hill, home of Lord and Lady Fortescue at Filleigh in North Devon until the end of the war, when in 1945 St Peters moved back to its old home in Seaford, now vacated by the Army, and resumed normal service. In 1956 Pat and Marjorie retired and Basil Talbot, an assistant Headmaster, a member of the team from the 1930s briefly took over but he retired through ill health.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_text_only = df_valid['text']\n",
    "df_valid_text_only.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean lenght: 426.39\n",
      "Min lenght: 147.00\n",
      "Max lenght: 1012.00\n"
     ]
    }
   ],
   "source": [
    "df_valid_text_lens = df_valid_text_only.map(lambda sentence: len(sentence))\n",
    "\n",
    "print(f\"Mean lenght: {df_valid_text_lens.mean():.2f}\")\n",
    "print(f\"Min lenght: {df_valid_text_lens.min():.2f}\")\n",
    "print(f\"Max lenght: {df_valid_text_lens.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2342    1347\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_text_lens[df_train_text_lens.map(lambda lenght: lenght == 1347)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     train-2343\n",
       "text          Upon her arrival, she was granted Drottningholm Palace as her summer residence, where the ''Young Court'', as it was called, amused themselves with picnics, masquerades and French language amateur theater. The crown prince's court was dominated by Carl Gustaf Tessin, who escorted Louisa Ulrika to Sweden and remained an influential favorite during her years as crown princess. Adolf Frederick never cared much for Tessin, but Louisa Ulrika had him appointed marshal at court and eventually royal governor of her son prince Gustav. Tessin was behind many amusements in the circle of the crown princess, and it was said that he was only too eager to please Louisa Ulrika in any way possible. There were unconfirmed rumors that Tessin was the lover of Louisa Ulrika during her tenure as crown princess. Her son Gustav III later addressed these rumors, that although Count Tessin had been in love with her, his feelings were one-sided and not answered by his mother, as a love affair with a noble contradicted the ''natural contempt'' which Louisa Ulrika herself as a royal felt for every subject, noble or not. Her circle at court included Henrika Juliana von Liewen, who immediately became her favorite among her ladies-in-waiting; the intellectual Cath*rine Charlotte De la Gardie, the scientist Eva Ekeblad, and the witty Anders Johan von H*pken.\n",
       "pron                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           he\n",
       "p_offset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      624\n",
       "entity_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Tessin\n",
       "offset_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      726\n",
       "is_coref_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   TRUE\n",
       "entity_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Gustav III\n",
       "offset_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      809\n",
       "is_coref_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FALSE\n",
       "url                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         http://en.wikipedia.org/wiki/Louisa_Ulrika_of_Prussia\n",
       "Name: 2342, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[2342]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[2342]['text'][624:626]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[(df_train['is_coref_A'] == \"FALSE\") & (df_train['offset_B'] > 512)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>pron</th>\n",
       "      <th>p_offset</th>\n",
       "      <th>entity_A</th>\n",
       "      <th>offset_A</th>\n",
       "      <th>is_coref_A</th>\n",
       "      <th>entity_B</th>\n",
       "      <th>offset_B</th>\n",
       "      <th>is_coref_B</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train-92</td>\n",
       "      <td>After a few years of almost no work -- although he was a guest star on Lou Grant and Charlie's Angels in the late 1970s, he once summed up the 1970s as ''I cried and did a lot of gardening'' -- he was hired in 1979 for his best-known role, self-made millionaire Palmer Cortlandt on ABC's long-running soap opera All My Children. Initially hired for only one year, he remained on contract through 2009. For much of his first decade on the show, Palmer was a ruthless villain, totally possessive of his daughter, Nina and violently threatening his ex-wife Daisy with being attacked by dobermans when she came back from the dead.</td>\n",
       "      <td>she</td>\n",
       "      <td>598</td>\n",
       "      <td>Nina</td>\n",
       "      <td>511</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Daisy</td>\n",
       "      <td>554</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/James_Mitchell_(actor)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>train-158</td>\n",
       "      <td>On 17 June 2005, after 12 years at Birmingham, Bennett transferred to Leeds United who already had Scottish international goalkeeper Neil Sullivan as first-choice goalkeeper. Despite playing the pre-season friendlies, he was limited to four league appearances during the 2005-06 season, obtained deputising for the injured Sullivan. In July 2006, Bennett transferred for an undisclosed fee to newly promoted Premiership club Sheffield United, signing on a two-year deal to provide competition to the Blades first-choice goalkeeper, Paddy Kenny. He played the first game of his second spell at the club at Bramall Lane against Reading on 16 September 2006.</td>\n",
       "      <td>He</td>\n",
       "      <td>545</td>\n",
       "      <td>Bennett</td>\n",
       "      <td>347</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Paddy Kenny</td>\n",
       "      <td>532</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Ian_Bennett_(footballer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>train-210</td>\n",
       "      <td>In 1851 Barlow was in England, where he published a short work ''Industry on Christian Principles, London, 1851. He published at London ''Letteratura Dantesca: Remarks on the Reading of the 114th Verse of the 7th Canto of the Paradise of the ''Divina Commedia'''' (1857), and two years afterwards ''Francesca da Rimini, her Lament and Vindication; with a brief Notice of the Malatesti'' (1859, 2nd edition, 1875). An Italian translation, ''Francesca da Rimini, suo Lamento e Difesa,'' &amp;c., in Filippo Scolari's ''Esercitazioni Dantesche,'' appeared at Venice in 1865. Barlow published in 1862 ''Il Gran Rifiuto, what it was, who made it, and how fatal to Dante Allighieri,'' on verses 58 to 63 of the 3rd canto of the Inferno; an Italian translation by G. G(uiscardi) appeared at Naples in 1864. Barlow also issued in 1862 ''Il Conte Ugolino e l'Arcivescovo Ruggieri: a Sketch from the Pisan Chronicles,'' and a fragment of English history, entitled ''The Young King and Bertrand de Born,'' from which the author deduced an amended reading in line 135 of the 28th canto of the ''Inferno.'' In 1864 Barlow published the final result of his work on the ''Divina Commedia,'' ''Critical, Historical, and Philosophical Contributions to the Study of the ''Divina Commedia.''''</td>\n",
       "      <td>his</td>\n",
       "      <td>1135</td>\n",
       "      <td>Bertrand de Born</td>\n",
       "      <td>971</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Barlow</td>\n",
       "      <td>1098</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Henry_Clark_Barlow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>train-275</td>\n",
       "      <td>Homer follows one of the raccoons into the family's home under a tree stump and prepares to take them out, despite Bart warning him that he always loses fights with animals, as his battle with the earthworms proved--but Homer claims that that was phased withdrawal, but after seeing that their family is basically the raccoon version of his family, Homer cannot bring himself to do it. During a windy break, Lisa can't inhale any smoke. She realizes her only alternative is to actually smoke a cigarette, and picks one up. Right as she is about to smoke it, her father arrives and takes it away, by throwing it on the ground, squishing it with his foot, and then shooting it with a gun several times, and he is shown to be putting the gun away into his jacket. (which is also filled with other guns and weapons) Appalled, he goes to tell Marge that Lisa needs to be taken out of the ballet academy, but discovers how proud she is of Lisa; Homer is unable bear to destroy Marge's happiness.</td>\n",
       "      <td>her</td>\n",
       "      <td>558</td>\n",
       "      <td>Marge</td>\n",
       "      <td>838</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>849</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Smoke_on_the_Daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>train-291</td>\n",
       "      <td>Llewellyn failed to take a wicket in this first Test and was promptly omitted from the remainder of the series but responded by performing impressively in the 1897--98 and 1898--99 Currie Cups, which led to his recall to the national team for the first Test of the 1898--99 series against England. Llewellyn impressed by taking five wickets but was surprisingly left out of the second Test. At the end of the 1898--99 series Llewellyn, perturbed by the actions of the selectors and seeking financial security, left South Africa to play for English county side Hampshire County Cricket Club as a professional, on the recommendation of South African team-mate Major Robert Poore, an ex-Hampshire cricketer on military assignment. He would star for Hampshire for over a decade, scoring 8772 runs at 27.58 and snaring 711 wickets at 24.66.</td>\n",
       "      <td>He</td>\n",
       "      <td>728</td>\n",
       "      <td>Llewellyn</td>\n",
       "      <td>425</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Robert Poore</td>\n",
       "      <td>664</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Charlie_Llewellyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>train-2850</td>\n",
       "      <td>A different appraisal, two months later, noting the governor had recently survived an impeachment attempt, said that rather than improving social services and generating employment, after two years in power Akande had implemented massive staff lay offs in the public service, and had caused virtual collapse of public infrastructure. On December 24, 2001, Akande's supporter Bola Ige, the minister of justice, was murdered in his house in Ibadan. The newspaper This Day said that the murder could have been linked to the feud between Akande and the deputy governor Iyiola Omisore. The murder followed another murder the previous week of Osun State legislator Odunayo Olagbaju, who was bludgeoned to death outside his home.</td>\n",
       "      <td>his</td>\n",
       "      <td>713</td>\n",
       "      <td>Akande</td>\n",
       "      <td>534</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Iyiola Omisore</td>\n",
       "      <td>565</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Adebisi_Akande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>train-2869</td>\n",
       "      <td>1998: The Pasukan Gerakan Khas and the Grup Gerak Khas were deployed to provide security and were on standby for hostage rescue, close protection and counter-terrorism duties during the 1998 Commonwealth Games held at National Stadium, Bukit Jalil, Kuala Lumpur on 11 to 21 September 1998. 20 September 1998: In the twilight hours, by orders from the then Prime Minister to the Inspector General of Police, Tan Sri Rahim Noor, 69th Commando PGK operatives led by Inspector Mazlan arrested the ex-Deputy Prime Minister Dato' Sri Anwar Ibrahim in his home 18 days after his ejection from the Cabinet, for inciting anti-Mahathir reforms in Kuala Lumpur.</td>\n",
       "      <td>his</td>\n",
       "      <td>545</td>\n",
       "      <td>Inspector Mazlan</td>\n",
       "      <td>463</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Dato' Sri Anwar Ibrahim</td>\n",
       "      <td>518</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Pasukan_Gerakan_Khas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>train-2891</td>\n",
       "      <td>After several months of captivity Duane is released by Granny Ruth, who is preparing to take everyone on a road trip to the home of her ex-husband Doctor Hal Rockwell, who will help in giving birth to Belial's equally misshapen girlfriend Eve's babies; before leaving for the trip, Granny Ruth sternly tells Duane to stay away from Belial, who has stopped speaking to Duane telepathically after Duane's attempt to put them back together. While traveling via bus to Hal's house in Peachtree County the group stop at a drug store, where Granny Ruth meets local sheriff Andrew Griffin while Duane, attempting to wriggle out a bus window, meets the sheriff's daughter Opal, who he tries to convince help him and Belial escape.</td>\n",
       "      <td>he</td>\n",
       "      <td>674</td>\n",
       "      <td>Hal</td>\n",
       "      <td>465</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Andrew Griffin</td>\n",
       "      <td>567</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Basket_Case_3:_The_Progeny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>train-2922</td>\n",
       "      <td>''In the early spring he stood above the heights of Miles Canyon ... the line 'I have gazed on naked grandeur where there's nothing else to gaze on' came into his mind and again he hammered out a complete poem, ''The Call of the Wild''. Conversations with locals led Service to write about things he had not seen (some of which had not actually happened) as well. He did not set foot in Dawson City until 1908, arriving in the Klondike ten years after the Gold Rush when his renown as a writer was already established. After having collected enough poems for a book, Service ''sent the poems to his father, who had emigrated to Toronto, and asked him to find a printing house so they could make it into a booklet. He enclosed a cheque to cover the costs and intended to give these booklets away to his friends in Whitehorse'' for Christmas. His father took the manuscript to William Briggs in Toronto, whose employees loved the book. ''The foreman and printers recited the ballads while they worked. A salesman read the proofs out loud as they came off the typesetting machines.'' An ''enterprising salesman sold 1700 copies in advance orders from galley proofs.'' The publisher ''sent Robert's cheque back to him and offered a ten percent royalty contract for the book.'</td>\n",
       "      <td>him</td>\n",
       "      <td>1210</td>\n",
       "      <td>William Briggs</td>\n",
       "      <td>875</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Robert</td>\n",
       "      <td>1186</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Songs_of_a_Sourdough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>train-2987</td>\n",
       "      <td>Through 2007, the company had presented more than 20 different productions in Adelaide and toured internationally (USA, Hong Kong, Japan, New Zealand), nationally (Sydney, Perth, Brisbane, Melbourne) and through regional South Australia and Victoria. Works produced by Cate Fowler include Wilfrid Gordon Macdonald Partridge, Brundibar, The Snow Queen, Riverland, Afternoon of the Elves, Midnite, Moonfleet, The Sad Ballad of Penny Dreadful, Two Weeks with the Queen, The Adventures of Snugglepot and Cuddlepie and Little Ragged Blossom, The Little Gentleman, and The Clockwork Forest. Additionally, she has directed groundbreaking work in the early childhood area, The Green Sheep and Cat.</td>\n",
       "      <td>she</td>\n",
       "      <td>599</td>\n",
       "      <td>Cate Fowler</td>\n",
       "      <td>269</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Penny Dreadful</td>\n",
       "      <td>425</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Cate_Fowler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  \\\n",
       "91      train-92   \n",
       "157    train-158   \n",
       "209    train-210   \n",
       "274    train-275   \n",
       "290    train-291   \n",
       "...          ...   \n",
       "2849  train-2850   \n",
       "2868  train-2869   \n",
       "2890  train-2891   \n",
       "2921  train-2922   \n",
       "2986  train-2987   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         After a few years of almost no work -- although he was a guest star on Lou Grant and Charlie's Angels in the late 1970s, he once summed up the 1970s as ''I cried and did a lot of gardening'' -- he was hired in 1979 for his best-known role, self-made millionaire Palmer Cortlandt on ABC's long-running soap opera All My Children. Initially hired for only one year, he remained on contract through 2009. For much of his first decade on the show, Palmer was a ruthless villain, totally possessive of his daughter, Nina and violently threatening his ex-wife Daisy with being attacked by dobermans when she came back from the dead.   \n",
       "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           On 17 June 2005, after 12 years at Birmingham, Bennett transferred to Leeds United who already had Scottish international goalkeeper Neil Sullivan as first-choice goalkeeper. Despite playing the pre-season friendlies, he was limited to four league appearances during the 2005-06 season, obtained deputising for the injured Sullivan. In July 2006, Bennett transferred for an undisclosed fee to newly promoted Premiership club Sheffield United, signing on a two-year deal to provide competition to the Blades first-choice goalkeeper, Paddy Kenny. He played the first game of his second spell at the club at Bramall Lane against Reading on 16 September 2006.   \n",
       "209    In 1851 Barlow was in England, where he published a short work ''Industry on Christian Principles, London, 1851. He published at London ''Letteratura Dantesca: Remarks on the Reading of the 114th Verse of the 7th Canto of the Paradise of the ''Divina Commedia'''' (1857), and two years afterwards ''Francesca da Rimini, her Lament and Vindication; with a brief Notice of the Malatesti'' (1859, 2nd edition, 1875). An Italian translation, ''Francesca da Rimini, suo Lamento e Difesa,'' &c., in Filippo Scolari's ''Esercitazioni Dantesche,'' appeared at Venice in 1865. Barlow published in 1862 ''Il Gran Rifiuto, what it was, who made it, and how fatal to Dante Allighieri,'' on verses 58 to 63 of the 3rd canto of the Inferno; an Italian translation by G. G(uiscardi) appeared at Naples in 1864. Barlow also issued in 1862 ''Il Conte Ugolino e l'Arcivescovo Ruggieri: a Sketch from the Pisan Chronicles,'' and a fragment of English history, entitled ''The Young King and Bertrand de Born,'' from which the author deduced an amended reading in line 135 of the 28th canto of the ''Inferno.'' In 1864 Barlow published the final result of his work on the ''Divina Commedia,'' ''Critical, Historical, and Philosophical Contributions to the Study of the ''Divina Commedia.''''   \n",
       "274                                                                                                                                                                                                                                                                                             Homer follows one of the raccoons into the family's home under a tree stump and prepares to take them out, despite Bart warning him that he always loses fights with animals, as his battle with the earthworms proved--but Homer claims that that was phased withdrawal, but after seeing that their family is basically the raccoon version of his family, Homer cannot bring himself to do it. During a windy break, Lisa can't inhale any smoke. She realizes her only alternative is to actually smoke a cigarette, and picks one up. Right as she is about to smoke it, her father arrives and takes it away, by throwing it on the ground, squishing it with his foot, and then shooting it with a gun several times, and he is shown to be putting the gun away into his jacket. (which is also filled with other guns and weapons) Appalled, he goes to tell Marge that Lisa needs to be taken out of the ballet academy, but discovers how proud she is of Lisa; Homer is unable bear to destroy Marge's happiness.   \n",
       "290                                                                                                                                                                                                                                                                                                                                                                                                                                                       Llewellyn failed to take a wicket in this first Test and was promptly omitted from the remainder of the series but responded by performing impressively in the 1897--98 and 1898--99 Currie Cups, which led to his recall to the national team for the first Test of the 1898--99 series against England. Llewellyn impressed by taking five wickets but was surprisingly left out of the second Test. At the end of the 1898--99 series Llewellyn, perturbed by the actions of the selectors and seeking financial security, left South Africa to play for English county side Hampshire County Cricket Club as a professional, on the recommendation of South African team-mate Major Robert Poore, an ex-Hampshire cricketer on military assignment. He would star for Hampshire for over a decade, scoring 8772 runs at 27.58 and snaring 711 wickets at 24.66.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...   \n",
       "2849                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       A different appraisal, two months later, noting the governor had recently survived an impeachment attempt, said that rather than improving social services and generating employment, after two years in power Akande had implemented massive staff lay offs in the public service, and had caused virtual collapse of public infrastructure. On December 24, 2001, Akande's supporter Bola Ige, the minister of justice, was murdered in his house in Ibadan. The newspaper This Day said that the murder could have been linked to the feud between Akande and the deputy governor Iyiola Omisore. The murder followed another murder the previous week of Osun State legislator Odunayo Olagbaju, who was bludgeoned to death outside his home.   \n",
       "2868                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1998: The Pasukan Gerakan Khas and the Grup Gerak Khas were deployed to provide security and were on standby for hostage rescue, close protection and counter-terrorism duties during the 1998 Commonwealth Games held at National Stadium, Bukit Jalil, Kuala Lumpur on 11 to 21 September 1998. 20 September 1998: In the twilight hours, by orders from the then Prime Minister to the Inspector General of Police, Tan Sri Rahim Noor, 69th Commando PGK operatives led by Inspector Mazlan arrested the ex-Deputy Prime Minister Dato' Sri Anwar Ibrahim in his home 18 days after his ejection from the Cabinet, for inciting anti-Mahathir reforms in Kuala Lumpur.   \n",
       "2890                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       After several months of captivity Duane is released by Granny Ruth, who is preparing to take everyone on a road trip to the home of her ex-husband Doctor Hal Rockwell, who will help in giving birth to Belial's equally misshapen girlfriend Eve's babies; before leaving for the trip, Granny Ruth sternly tells Duane to stay away from Belial, who has stopped speaking to Duane telepathically after Duane's attempt to put them back together. While traveling via bus to Hal's house in Peachtree County the group stop at a drug store, where Granny Ruth meets local sheriff Andrew Griffin while Duane, attempting to wriggle out a bus window, meets the sheriff's daughter Opal, who he tries to convince help him and Belial escape.   \n",
       "2921  ''In the early spring he stood above the heights of Miles Canyon ... the line 'I have gazed on naked grandeur where there's nothing else to gaze on' came into his mind and again he hammered out a complete poem, ''The Call of the Wild''. Conversations with locals led Service to write about things he had not seen (some of which had not actually happened) as well. He did not set foot in Dawson City until 1908, arriving in the Klondike ten years after the Gold Rush when his renown as a writer was already established. After having collected enough poems for a book, Service ''sent the poems to his father, who had emigrated to Toronto, and asked him to find a printing house so they could make it into a booklet. He enclosed a cheque to cover the costs and intended to give these booklets away to his friends in Whitehorse'' for Christmas. His father took the manuscript to William Briggs in Toronto, whose employees loved the book. ''The foreman and printers recited the ballads while they worked. A salesman read the proofs out loud as they came off the typesetting machines.'' An ''enterprising salesman sold 1700 copies in advance orders from galley proofs.'' The publisher ''sent Robert's cheque back to him and offered a ten percent royalty contract for the book.'   \n",
       "2986                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Through 2007, the company had presented more than 20 different productions in Adelaide and toured internationally (USA, Hong Kong, Japan, New Zealand), nationally (Sydney, Perth, Brisbane, Melbourne) and through regional South Australia and Victoria. Works produced by Cate Fowler include Wilfrid Gordon Macdonald Partridge, Brundibar, The Snow Queen, Riverland, Afternoon of the Elves, Midnite, Moonfleet, The Sad Ballad of Penny Dreadful, Two Weeks with the Queen, The Adventures of Snugglepot and Cuddlepie and Little Ragged Blossom, The Little Gentleman, and The Clockwork Forest. Additionally, she has directed groundbreaking work in the early childhood area, The Green Sheep and Cat.   \n",
       "\n",
       "     pron  p_offset          entity_A  offset_A is_coref_A  \\\n",
       "91    she       598              Nina       511      FALSE   \n",
       "157    He       545           Bennett       347       TRUE   \n",
       "209   his      1135  Bertrand de Born       971      FALSE   \n",
       "274   her       558             Marge       838      FALSE   \n",
       "290    He       728         Llewellyn       425      FALSE   \n",
       "...   ...       ...               ...       ...        ...   \n",
       "2849  his       713            Akande       534      FALSE   \n",
       "2868  his       545  Inspector Mazlan       463      FALSE   \n",
       "2890   he       674               Hal       465      FALSE   \n",
       "2921  him      1210    William Briggs       875      FALSE   \n",
       "2986  she       599       Cate Fowler       269       TRUE   \n",
       "\n",
       "                     entity_B  offset_B is_coref_B  \\\n",
       "91                      Daisy       554       TRUE   \n",
       "157               Paddy Kenny       532      FALSE   \n",
       "209                    Barlow      1098       TRUE   \n",
       "274                      Lisa       849       TRUE   \n",
       "290              Robert Poore       664       TRUE   \n",
       "...                       ...       ...        ...   \n",
       "2849           Iyiola Omisore       565      FALSE   \n",
       "2868  Dato' Sri Anwar Ibrahim       518       TRUE   \n",
       "2890           Andrew Griffin       567      FALSE   \n",
       "2921                   Robert      1186       TRUE   \n",
       "2986           Penny Dreadful       425      FALSE   \n",
       "\n",
       "                                                          url  \n",
       "91        http://en.wikipedia.org/wiki/James_Mitchell_(actor)  \n",
       "157     http://en.wikipedia.org/wiki/Ian_Bennett_(footballer)  \n",
       "209           http://en.wikipedia.org/wiki/Henry_Clark_Barlow  \n",
       "274        http://en.wikipedia.org/wiki/Smoke_on_the_Daughter  \n",
       "290            http://en.wikipedia.org/wiki/Charlie_Llewellyn  \n",
       "...                                                       ...  \n",
       "2849              http://en.wikipedia.org/wiki/Adebisi_Akande  \n",
       "2868        http://en.wikipedia.org/wiki/Pasukan_Gerakan_Khas  \n",
       "2890  http://en.wikipedia.org/wiki/Basket_Case_3:_The_Progeny  \n",
       "2921        http://en.wikipedia.org/wiki/Songs_of_a_Sourdough  \n",
       "2986                 http://en.wikipedia.org/wiki/Cate_Fowler  \n",
       "\n",
       "[123 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train['p_offset'] > 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [sentence.split() for sentence in df_train_text_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentences = list(df_train_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function taken from the 'evaluate.py' script\n",
    "def flat_list(l: List[List[Any]]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "        A single list containing all elements that\n",
    "        were in the input list.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    l: List[List[Any]]\n",
    "        A list of lists of any type\n",
    "    \"\"\"\n",
    "    return [_e for e in l for _e in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_most_common_tokens(dataset_text: List[List[str]], n: int = 20) -> dict:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "        The first n common tokens and their frequencies, where the tokens are\n",
    "        retrieved from the list 'dataset_text'.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_text: List[List[str]]\n",
    "        A list of lists of strings. \n",
    "        In this case each nested list is a sentence.\n",
    "    \n",
    "    n: int\n",
    "        Indicates how many tokens to consider.\n",
    "        If it is a negative number, \n",
    "        the function returns the frequencies of all the tokens in the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    # The input is flattened\n",
    "    tokens = flat_list(dataset_text)  \n",
    "\n",
    "    # If negative number, return the frequency of all the tokens\n",
    "    if n <= -1:\n",
    "        return dict(Counter(tokens).most_common(len(Counter(tokens))))\n",
    "    else:\n",
    "        return dict(Counter(tokens).most_common(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq = freq_most_common_tokens(df_train_text_only, n=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Cai` com'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text_only = df_train_text_only.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606                                                                                                                                                                                                                          The magazine also listed Pam Bouvier seventh on their list of worst Bond girls, saying Carey Lowell ''fumbled this attempt at giving 007 a modern, independent counterpart by turning her into a nagging pest.'' Norman Wilner of MSN considered Licence to Kill the second worst Bond film, above only A View to a Kill, but defended Dalton, saying he ''got a raw deal.\n",
       "46                                                                                                                                                                                                                                                      The Movie was either opening or closing night at more than half of those festivals. Director Dyanna Taylor's film about her grandmother, the photographer Dorothea Lange, Grab a Hunk of Lightning, aired on PBS's ''American Masters'' in 2014 and was produced by Grossman. She directed Above and Beyond (2015) for producer Nancy Spielberg.\n",
       "9                                                                                                                                                                                                          Shaftesbury's UK partners in the production of the series, British broadcaster UKTV and the international distributor ITV Studios Global Entertainment, were both interested in additional seasons. Christina Jennings approached Kirstine Stewart, executive vice-president of CBC's English services, about continuing the series, and she felt that ''a home at CBC made absolute sense''.\n",
       "1000                                                                                                                               Prytz appeared opposite fellow Swedish actors, Liv Ullmann and Max von Sydow, in a total of three films which were nominated for the Academy Award for Best Foreign Language Film: Jan Troell's The Emigrants in 1971; a sequel to The Emigrants, The New Land, which was also directed by Troell; and Sven Nykvist's The Ox in 1991. She also provided the voice of Gammel-Maja in the 1985 animated sequel, Peter-No-Tail in Americat (Pelle Svansl*s i Amerikatt).\n",
       "2391    Wendlinger recovered from his injuries, but when Sauber granted him an opportunity to drive in 1995 he was relieved of his duties before the Monaco event a year on from the accident, as his performances were disappointing. He was replaced by Jean-Christophe Boullion. Wendlinger came back briefly for Sauber in two Grands Prix at the end of the season, as Boullion also proved uncompetitive in relation to team-mate Heinz-Harald Frentzen. However, this return also proved to be a failure, and the 1995 Japanese and Australian Grands Prix proved to be his Formula One swansong.\n",
       "441                                                                                                                                                                                                                                            The instrumental of ''Let It Go'' was mixed with the a capella of ''Addicted to You'' to become ''Addicted to You (Avicii by Avicii)''. On 29 December 2012, Bergling released ''I Could Be the One'' with Nicky Romero. After first being debuted at his shows almost a year earlier, the track finally got released via Bergling's record label LE7ELS.\n",
       "1849                 She is also the oldest person never to have held the title of the world's oldest living person, since Jeanne Calment, who was about five months older, was still alive at the time of Hannah's death. Before her death, Hannah claimed to be a year older, but investigation by the Social Security Administration's ''Kestenbaum'' study in 2003 proved that her age at death was actually 117 years, 248 days. At the time of her death in March 1993, Hannah was the oldest American ever and second oldest person ever whose age has been verified, behind only Jeanne Calment.\n",
       "203                Nearly two-thirds of these students once lived and worked in Steung Meanchey, picking plastic and metal out of the mountains of burning, hazardous waste and selling them to local recycling centers. CCF also welcomes many children in need from impoverished rural regions of Cambodia. In 2007, Quincy Jones awarded Neeson the inaugural Harvard School of Public Health ''Q Prize'' in recognition of his extraordinary leadership in advocacy for children, calling Neeson's ''selfless, remarkable commitment to the children of Cambodia'' a ''genuine profile in courage.''\n",
       "1212                                                                                                                                                                                                                         Truth or Dare by Madonna is a lifestyle brand by American recording artist Madonna. It is the second enterprise from MG Icon LLC, a joint venture between Madonna, her manager Guy Oseary and Iconix Brand Group. This is the second brand from the venture after Material Girl, which is a fashion line designed by Madonna and her daughter Lourdes and launched in 2010.\n",
       "2426                                                                                                                                                                                                             Anderson and his wife, Jenna, live in Seattle, Washington. Anderson is an avid skateboarder and is sponsored by the American footwear and clothing company DVS Shoes. He has a signature skateboard shoe released through the company. During an interview with Dr. Drew Pinsky, Anderson revealed that he is a recovering alcoholic and his addiction left him homeless for two years.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_text_only.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_freq.keys())[-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372                                                                                                                                                                                                                                                                                            Felicia Rudolphina Scatcherd (1862 -- March 12, 1927) was a journalist and spiritualist. Felicia Scatcherd was born to Watson Scatcherd and Emily Frances Crofton. She lived with her parents in London until her mother's death in 1901.\n",
       "329                                                                                                                                                 In 1806, still commanding his cavalry division (23rd, 29th and 30th Dragoon Regiments), he was present at the siege and capture of the fortress of Gaeta, on the west coast of Italy. During the second French invasion of Portugal in 1809, Mermet led a division under Marshal Nicolas Soult. He fought at the First Battle of Porto on 28 March and the Battle of Grij* on 11 May.\n",
       "2622    Longley's account of this murder differs from that of his later killings, where he was more inclined to brag about shooting men than to try to divert blame to others. Some versions of Green's killing claim he was a member of the Texas State Police; the TSP only existed from 1870 to 1873. Longley drifted around Texas for a time, and while gambling in saloons he became acquainted with noted gambler, Phil Coe In 1869, Longley and his brother-in-law, John Wilson, embarked on a crime spree through southern Texas.\n",
       "212                                                                                                                                                          She returned for a guest appearance from April to September 1997. She then returned to the show as a series regular again in April 1999, signing a two-year contract. She again exited the role in the fall 1999 when then head writer, Bradley Bell informed Teri Ann Linn that Kristen was no longer needed in the show's current storylines and that she would be let go.\n",
       "993                                  The final nail in the coffin was thought to be over the issue of finances between Ekta and Smriti, where the latter demanded a raise and the former rejected. Last episode that feature Smriti was telecasted on 6 June 2007, where she dies in a truck accident. After a long hiatus of 11 months, amidst the decline of Kyunki's viewership, Ekta Kapoor got back Smriti into her role in a highly publicized affair, where they both buried their hatchet with each other in a public appearance.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ` -> '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocast https://wandb.ai/wandb_fc/tips/reports/How-to-use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "\n",
    "Optimization https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_text_only' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2280\\727263226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train_text_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train_text_only\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Mean lenght: {df_train_text_lens.mean():.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Min lenght: {df_train_text_lens.min():.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Max lenght: {df_train_text_lens.max():.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_text_only' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_text_lens = df_train_text_only.map(lambda sentence: len(tokenizer.tokenize(sentence)))\n",
    "\n",
    "print(f\"Mean lenght: {df_train_text_lens.mean():.2f}\")\n",
    "print(f\"Min lenght: {df_train_text_lens.min():.2f}\")\n",
    "print(f\"Max lenght: {df_train_text_lens.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = df_train['text'][792]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_train.iloc[444]\n",
    "print(row['offset_A'], row['entity_A'], row['is_coref_A'])\n",
    "print(row['offset_B'], row['entity_B'], row['is_coref_B'])\n",
    "print(row['p_offset'], row['pron'])\n",
    "row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The order is important because we want that the pronoun comes after all the \n",
    "# coreferenced entities in the output, even if B could come after the pronoun. \n",
    "break_points = sorted([\n",
    "    (\"A\", row['offset_A'], row['entity_A']),\n",
    "    (\"B\", row['offset_B'], row['entity_B']),\n",
    "    (\"P\", row['p_offset'], row['pron'])\n",
    "], key=lambda x: x[0])\n",
    "\n",
    "tokens, spans, current_pos = [], {}, 0\n",
    "for name, offset, text in break_points:\n",
    "    tokens.extend(tokenizer.tokenize(row[\"text\"][current_pos:offset]))\n",
    "    # Make sure we do not get it wrong\n",
    "    assert row[\"text\"][offset:offset+len(text)] == text\n",
    "    # Tokenize the target\n",
    "    tmp_tokens = tokenizer.tokenize(row[\"text\"][offset:offset+len(text)])\n",
    "    \n",
    "    # [num_tokens until entity, num_tokens including the entity]\n",
    "    spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n",
    "    print(\"BEFORE\", tokens)\n",
    "    \n",
    "    # In the last iteration, the pronoun is appended to the end\n",
    "    tokens.extend(tmp_tokens)\n",
    "    # print()\n",
    "    print(\"AFTER\", tokens)\n",
    "    current_pos = offset + len(text)\n",
    "# print(tokens)\n",
    "tokens.extend(tokenizer.tokenize(row[\"text\"][current_pos:offset]))\n",
    "# print(\"\\n\",tokens)\n",
    "# The pronoun is a single token, so the span is the same\n",
    "assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "print(\"\\n\", tokens)\n",
    "off = spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]]\n",
    "print(off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(row['text'][316:316+len(row['entity_A'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 20, 30), (100, 200, 300), (144, 500, 3)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sorted([\n",
    "    (144, 500, 3),\n",
    "    (10, 20, 30),\n",
    "    (100, 200, 300)\n",
    "], key=lambda x: x[1])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create dataset with text and offsets of entities and pronoun\n",
    "2. Get contextualized embeddings from Bert\n",
    "3. Select through the offsets the embeddings of enities and pronoun\n",
    "4. Concat somehow the embeddings and pass them to a MLP + softmax to retrieve the probabilities\n",
    "5. The probabilities are about 3 classes: M F N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_label(is_coref_A: str, is_coref_B: str):\n",
    "    if is_coref_A == \"TRUE\" or is_coref_A is True:\n",
    "        return 0\n",
    "    elif is_coref_B == \"TRUE\" or is_coref_B is True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEMININE = 0\n",
    "MASCULINE = 1\n",
    "UNKNOWN = 2\n",
    "\n",
    "def get_gender(pronoun: str):\n",
    "    gender_mapping = {\n",
    "        'she': FEMININE,\n",
    "        'her': FEMININE,\n",
    "        'he': MASCULINE,\n",
    "        'his': MASCULINE,\n",
    "        'him': MASCULINE,\n",
    "    }\n",
    "    \n",
    "    return gender_mapping.get(pronoun.lower(), UNKNOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrepareDataFrame:\n",
    "    \n",
    "#     def __init__(self, dataset: List[Dict]):\n",
    "#         self.df = pd.DataFrame(dataset)\n",
    "        \n",
    "#         self.df['text'] = self.df['text'].map(clean_text)\n",
    "# #         self._extract_target(self.df)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def clean_text(text):\n",
    "#         return text.translate(str.maketrans(\"`\", \"'\"))\n",
    "    \n",
    "#     def __str__(self):\n",
    "#         return self.df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(df_train['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'] = [get_class_label(is_coref_A, is_coref_B) for is_coref_A, is_coref_B in zip(df_train['is_coref_A'],  df_train['is_coref_B'])]\n",
    "df_valid['target'] = [get_class_label(is_coref_A, is_coref_B) for is_coref_A, is_coref_B in zip(df_valid['is_coref_A'],  df_valid['is_coref_B'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.df = df\n",
    "        \n",
    "        self.labeled = labeled\n",
    "        self.tokenizer = tokenizer\n",
    "        self.offsets, self.tokens = [], []\n",
    "        \n",
    "        if labeled:\n",
    "            self.labels = df.target.values.astype(\"uint8\")\n",
    "        \n",
    "        \n",
    "        self._convert_tokens_to_ids()\n",
    "        \n",
    "#     @staticmethod\n",
    "#     def get_class_label(is_coref_A: str, is_coref_B: str):\n",
    "#         if is_coref_A == \"TRUE\":\n",
    "#                 return 0\n",
    "#         elif is_coref_B == \"TRUE\":\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 2\n",
    "    \n",
    "    def _convert_tokens_to_ids(self):\n",
    "        CLS = [self.tokenizer.cls_token]\n",
    "        SEP = [self.tokenizer.sep_token]\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            tokens, offsets = self._tokenize(row)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(self.tokenizer.convert_tokens_to_ids(\n",
    "                CLS + tokens + SEP))\n",
    "    \n",
    "    def _tokenize(self, row):\n",
    "        # The order is important because we want that the pronoun comes after all the \n",
    "        # coreferenced entities in the output, even if B could come after the pronoun. \n",
    "        break_points = sorted([\n",
    "            (\"A\", row['offset_A'], row['entity_A']),\n",
    "            (\"B\", row['offset_B'], row['entity_B']),\n",
    "            (\"P\", row['p_offset'], row['pron'])\n",
    "        ], key=lambda x: x[0])\n",
    "\n",
    "        tokens, spans, current_pos = [], {}, 0\n",
    "        for name, offset, text in break_points:\n",
    "            tokens.extend(self.tokenizer.tokenize(row[\"text\"][current_pos:offset]))\n",
    "            # Make sure we do not get it wrong\n",
    "            assert row[\"text\"][offset:offset+len(text)] == text\n",
    "            # Tokenize the target\n",
    "            tmp_tokens = self.tokenizer.tokenize(row[\"text\"][offset:offset+len(text)])\n",
    "\n",
    "            # [num_tokens until entity, num_tokens including the entity]\n",
    "            spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive \n",
    "            # In the last iteration, the pronoun is appended to the end\n",
    "            tokens.extend(tmp_tokens)\n",
    "            current_pos = offset + len(text)\n",
    "    \n",
    "        tokens.extend(self.tokenizer.tokenize(row[\"text\"][current_pos:offset]))\n",
    "\n",
    "        # The pronoun is a single token, so the span is the same\n",
    "        assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "        return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.labels[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = GAPDataset(df_train[:100], tokenizer)\n",
    "valid_ds = GAPDataset(df_valid[:50], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>pron</th>\n",
       "      <th>p_offset</th>\n",
       "      <th>entity_A</th>\n",
       "      <th>offset_A</th>\n",
       "      <th>is_coref_A</th>\n",
       "      <th>entity_B</th>\n",
       "      <th>offset_B</th>\n",
       "      <th>is_coref_B</th>\n",
       "      <th>url</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_(UK_TV_series)_characters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second oldest of five children including his brothers, Fred and Gordon and sisters, Marge (Peppy) and Marilyn. His high school days were spent at New Trier High School in Winnetka, Illinois. MacKenzie studied with Bernard Leach from 1949 to 1952. His simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai.</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train-3</td>\n",
       "      <td>He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_de_la_Sota</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train-4</td>\n",
       "      <td>The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers''. Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above. Rank has produced numerous films (under his real name, Henry Rosenthal) including the hit The Devil and Daniel Johnston.</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar. She sang on the subsequent Deutsche Grammophon recording of the opera. For his opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007. She has since sung several parts and roles in John Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Peter Sellars production at the New Crowned Hope Festival in Vienna.</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train-6</td>\n",
       "      <td>Sandra Collins is an American DJ. She got her start on the West Coast of the U.S. in Phoenix, Arizona and into residencies in Los Angeles, and eventually moved towards trance. She used American producers to give herself a unique sound. Collins performed for an estimated 80,000 people on the first night of Woodstock '99, and was the first female DJ featured in the Tranceport series of influential recordings. She recently has released two CD mixes under Paul Oakenfold's Perfecto label.</td>\n",
       "      <td>She</td>\n",
       "      <td>411</td>\n",
       "      <td>Collins</td>\n",
       "      <td>236</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>DJ</td>\n",
       "      <td>347</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Sandra_Collins</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train-7</td>\n",
       "      <td>Reb Chaim Yaakov's wife is the sister of Rabbi Moishe Sternbuch, as is the wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his uncles. Reb Asher's brother Rabbi Shlomo Arieli is the author of a critical edition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he later studied under his father-in-law in the Mirrer Yeshiva.</td>\n",
       "      <td>his</td>\n",
       "      <td>273</td>\n",
       "      <td>Reb Asher</td>\n",
       "      <td>152</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Akiva Eiger</td>\n",
       "      <td>253</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Asher_Arieli</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train-8</td>\n",
       "      <td>Slant Magazine's Sal Cinquemani viewed the album as formulaic and ''competently, often frustratingly more of the same from an artist who still seems capable of much more.'' Greg Kot of the Chicago Tribune perceived ''formula production and hack songwriting'', but complimented Pink's personality and its ''handful'' of worthy tracks. In his list for The Barnes &amp; Noble Review, Robert Christgau named The Truth About Love the fourth best album of 2012.</td>\n",
       "      <td>his</td>\n",
       "      <td>337</td>\n",
       "      <td>Greg Kot</td>\n",
       "      <td>173</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Robert Christgau</td>\n",
       "      <td>377</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_Truth_About_Love_(Pink_album)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train-9</td>\n",
       "      <td>Her father was an Englishman ''of rank and culture'' and her mother was a free woman of color, described as light-skinned. When Mary was six, her mother sent her to Alexandria (then part of the District of Columbia) to attend school. Living with her aunt Mary Paine, Kelsey studied for about ten years.</td>\n",
       "      <td>her</td>\n",
       "      <td>246</td>\n",
       "      <td>Mary Paine</td>\n",
       "      <td>255</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Kelsey</td>\n",
       "      <td>267</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mary_S._Peake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train-10</td>\n",
       "      <td>Shaftesbury's UK partners in the production of the series, British broadcaster UKTV and the international distributor ITV Studios Global Entertainment, were both interested in additional seasons. Christina Jennings approached Kirstine Stewart, executive vice-president of CBC's English services, about continuing the series, and she felt that ''a home at CBC made absolute sense''.</td>\n",
       "      <td>she</td>\n",
       "      <td>329</td>\n",
       "      <td>Christina Jennings</td>\n",
       "      <td>196</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Kirstine Stewart</td>\n",
       "      <td>226</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Murdoch_Mysteries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "0   train-1   \n",
       "1   train-2   \n",
       "2   train-3   \n",
       "3   train-4   \n",
       "4   train-5   \n",
       "5   train-6   \n",
       "6   train-7   \n",
       "7   train-8   \n",
       "8   train-9   \n",
       "9  train-10   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "0                                                                                                                                                                                                                                            Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.   \n",
       "1                                                                                                                                                                                                                                                            He grew up in Evanston, Illinois the second oldest of five children including his brothers, Fred and Gordon and sisters, Marge (Peppy) and Marilyn. His high school days were spent at New Trier High School in Winnetka, Illinois. MacKenzie studied with Bernard Leach from 1949 to 1952. His simple, wheel-thrown functional pottery is heavily influenced by the oriental aesthetic of Shoji Hamada and Kanjiro Kawai.   \n",
       "2                                                                                                                              He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.   \n",
       "3                                                                                                                                                                                                                                                                     The current members of Crime have also performed in San Francisco under the band name ''Remote Viewers''. Strike has published two works of fiction in recent years: Ports of Hell, which is listed in the Rock and Roll Hall of Fame Library, and A Loud Humming Sound Came from Above. Rank has produced numerous films (under his real name, Henry Rosenthal) including the hit The Devil and Daniel Johnston.   \n",
       "4  Her Santa Fe Opera debut in 2005 was as Nuria in the revised edition of Golijov's Ainadamar. She sang on the subsequent Deutsche Grammophon recording of the opera. For his opera Doctor Atomic, Adams rewrote the role of Kitty Oppenheimer, originally a mezzo-soprano role, for soprano voice, and Rivera sang the rewritten part of Kitty Oppenheimer at Lyric Opera of Chicago, De Nederlandse Opera, and the Metropolitan Opera., all in 2007. She has since sung several parts and roles in John Adams' works, including the soprano part in El Ni*o, and the role of Kumudha in A Flowering Tree in the Peter Sellars production at the New Crowned Hope Festival in Vienna.   \n",
       "5                                                                                                                                                                              Sandra Collins is an American DJ. She got her start on the West Coast of the U.S. in Phoenix, Arizona and into residencies in Los Angeles, and eventually moved towards trance. She used American producers to give herself a unique sound. Collins performed for an estimated 80,000 people on the first night of Woodstock '99, and was the first female DJ featured in the Tranceport series of influential recordings. She recently has released two CD mixes under Paul Oakenfold's Perfecto label.   \n",
       "6                                                                                                                                                                                                                                      Reb Chaim Yaakov's wife is the sister of Rabbi Moishe Sternbuch, as is the wife of Rabbi Meshulam Dovid Soloveitchik, making the two Rabbis his uncles. Reb Asher's brother Rabbi Shlomo Arieli is the author of a critical edition of the novallae of Rabbi Akiva Eiger. Before his marriage, Rabbi Arieli studied in the Ponevezh Yeshiva headed by Rabbi Shmuel Rozovsky, and he later studied under his father-in-law in the Mirrer Yeshiva.   \n",
       "7                                                                                                                                                                                                                   Slant Magazine's Sal Cinquemani viewed the album as formulaic and ''competently, often frustratingly more of the same from an artist who still seems capable of much more.'' Greg Kot of the Chicago Tribune perceived ''formula production and hack songwriting'', but complimented Pink's personality and its ''handful'' of worthy tracks. In his list for The Barnes & Noble Review, Robert Christgau named The Truth About Love the fourth best album of 2012.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                        Her father was an Englishman ''of rank and culture'' and her mother was a free woman of color, described as light-skinned. When Mary was six, her mother sent her to Alexandria (then part of the District of Columbia) to attend school. Living with her aunt Mary Paine, Kelsey studied for about ten years.   \n",
       "9                                                                                                                                                                                                                                                                                         Shaftesbury's UK partners in the production of the series, British broadcaster UKTV and the international distributor ITV Studios Global Entertainment, were both interested in additional seasons. Christina Jennings approached Kirstine Stewart, executive vice-president of CBC's English services, about continuing the series, and she felt that ''a home at CBC made absolute sense''.   \n",
       "\n",
       "  pron  p_offset            entity_A  offset_A is_coref_A          entity_B  \\\n",
       "0  her       274      Cheryl Cassidy       191       TRUE           Pauline   \n",
       "1  His       284           MacKenzie       228       TRUE     Bernard Leach   \n",
       "2  his       265             Angeloz       173      FALSE        De la Sota   \n",
       "3  his       321                Hell       174      FALSE   Henry Rosenthal   \n",
       "4  She       437   Kitty Oppenheimer       219      FALSE            Rivera   \n",
       "5  She       411             Collins       236       TRUE                DJ   \n",
       "6  his       273           Reb Asher       152      FALSE       Akiva Eiger   \n",
       "7  his       337            Greg Kot       173      FALSE  Robert Christgau   \n",
       "8  her       246          Mary Paine       255      FALSE            Kelsey   \n",
       "9  she       329  Christina Jennings       196       TRUE  Kirstine Stewart   \n",
       "\n",
       "   offset_B is_coref_B  \\\n",
       "0       207      FALSE   \n",
       "1       251      FALSE   \n",
       "2       246       TRUE   \n",
       "3       336       TRUE   \n",
       "4       294       TRUE   \n",
       "5       347      FALSE   \n",
       "6       253      FALSE   \n",
       "7       377       TRUE   \n",
       "8       267       TRUE   \n",
       "9       226      FALSE   \n",
       "\n",
       "                                                                       url  \\\n",
       "0  http://en.wikipedia.org/wiki/List_of_Teachers_(UK_TV_series)_characters   \n",
       "1                            http://en.wikipedia.org/wiki/Warren_MacKenzie   \n",
       "2                 http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_de_la_Sota   \n",
       "3                                http://en.wikipedia.org/wiki/Crime_(band)   \n",
       "4                              http://en.wikipedia.org/wiki/Jessica_Rivera   \n",
       "5                              http://en.wikipedia.org/wiki/Sandra_Collins   \n",
       "6                                http://en.wikipedia.org/wiki/Asher_Arieli   \n",
       "7           http://en.wikipedia.org/wiki/The_Truth_About_Love_(Pink_album)   \n",
       "8                               http://en.wikipedia.org/wiki/Mary_S._Peake   \n",
       "9                           http://en.wikipedia.org/wiki/Murdoch_Mysteries   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       0  \n",
       "6       2  \n",
       "7       1  \n",
       "8       1  \n",
       "9       0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = GAPDataset(df_test, tokenizer, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.zeros((5, 10), dtype=torch.int64, device=device)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, truncate_len=400):\n",
    "    \"\"\"Batch preparation.\n",
    "\n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"    \n",
    "    batch_features, batch_offsets, batch_labels  = zip(*batch)\n",
    "\n",
    "    max_len = min(\n",
    "        max((len(x) for x in batch_features)),\n",
    "        truncate_len\n",
    "    )\n",
    "    \n",
    "    # Features\n",
    "    features = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    \n",
    "    # Padding\n",
    "    for i, row in enumerate(batch_features):\n",
    "        features[i, :len(row)] = row\n",
    "   \n",
    "    features_tensor = torch.tensor(features, device=device)\n",
    "\n",
    "    # Offsets\n",
    "    offsets_tensor = torch.stack([\n",
    "        torch.tensor(x, dtype=torch.int64, device=device) for x in batch_offsets\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    \n",
    "    # Labels\n",
    "    if batch_labels[0] is None:\n",
    "        return features_tensor, offsets_tensor, None\n",
    "    \n",
    "    labels_tensor = torch.tensor(batch_labels, dtype=torch.uint8, device=device)\n",
    "    return features_tensor, offsets_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_batch(batch, truncate_len=400):\n",
    "#     \"\"\"Batch preparation.\n",
    "\n",
    "#     1. Pad the sequences\n",
    "#     2. Transform the target.\n",
    "#     \"\"\"    \n",
    "#     batch_features, batch_offsets, batch_labels  = zip(*batch)\n",
    "\n",
    "#     max_len = min(\n",
    "#         max((len(x) for x in batch_features)),\n",
    "#         truncate_len\n",
    "#     )\n",
    "    \n",
    "#     # Features\n",
    "#     features = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    \n",
    "#     # Padding\n",
    "#     for i, row in enumerate(batch_features):\n",
    "#         features[i, :len(row)] = row\n",
    "   \n",
    "#     features_tensor = torch.tensor(features)\n",
    "\n",
    "#     # Offsets\n",
    "#     offsets_tensor = torch.stack([\n",
    "#         torch.tensor(x, dtype=torch.int64) for x in batch_offsets\n",
    "#     ], dim=0) + 1 # Account for the [CLS] token\n",
    "    \n",
    "#     # Labels\n",
    "#     if batch_labels[0] is None:\n",
    "#         return features_tensor, offsets_tensor, None\n",
    "    \n",
    "#     labels_tensor = torch.tensor(batch_labels, dtype=torch.uint8)\n",
    "#     return features_tensor, offsets_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2002,  4914,  ...,     0,     0,     0],\n",
       "         [  101, 14559,  2025,  ...,     0,     0,     0],\n",
       "         [  101,  2043,  2016,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2016, 17708,  ...,     0,     0,     0],\n",
       "         [  101,  2006,  2258,  ...,     0,     0,     0],\n",
       "         [  101,  8242,  7607,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " tensor([[47, 52, 56, 57, 59],\n",
       "         [29, 29, 40, 40, 46],\n",
       "         [84, 86, 90, 90, 95],\n",
       "         ...,\n",
       "         [53, 53, 60, 60, 63],\n",
       "         [64, 65, 77, 79, 88],\n",
       "         [57, 59, 63, 63, 70]], device='cuda:0'),\n",
       " tensor([2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "         1, 2, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 2, 0, 0, 1,\n",
       "         2, 2, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "         1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 2, 2, 0,\n",
       "         0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 2,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 1,\n",
       "         0, 0, 1, 0, 2, 0, 1, 2, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 2, 1, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 2,\n",
       "         0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0,\n",
       "         0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0,\n",
       "         2, 0, 2, 0, 2, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1,\n",
       "         2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1, 2, 1, 1, 0, 0, 2, 1, 1,\n",
       "         0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0, 2, 2,\n",
       "         1, 1, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 1, 1, 0],\n",
       "        device='cuda:0', dtype=torch.uint8))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_batch(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str):\n",
    "        super().__init__()\n",
    "      \n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device, non_blocking=True)\n",
    "        self.head = CorefHead(self.bert_hidden_size).to(device, non_blocking=True)\n",
    "    \n",
    "    def forward(self, x, offsets):\n",
    "        bert_outputs =  self.bert(\n",
    "            x, attention_mask=(x > 0).long(), \n",
    "            token_type_ids=None, output_hidden_states=True)\n",
    "#         concat_bert = torch.cat((bert_outputs[-1],bert_outputs[-2],bert_outputs[-3]),dim=-1)\n",
    "        \n",
    "        last_layer = bert_outputs.last_hidden_state\n",
    "        head_outputs = self.head(last_layer, offsets)\n",
    "#         return concat_bert  \n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_entities_and_pron_embeddings(bert_embeddings, entities_and_pron_offsets):\n",
    "    embeddings_A = []\n",
    "    embeddings_B = []\n",
    "    embeddings_pron = []\n",
    "\n",
    "    # Consider embeddings and offsets in each batch separately\n",
    "    for embeddings, off in zip(bert_embeddings, entities_and_pron_offsets):\n",
    "        # The offsets of mention A are the first and the second\n",
    "        # in the 'off' tensor\n",
    "        offsets_ent_A = range(off[0], off[1]+1) \n",
    "        # The offsets of mention B are the third and the fourth\n",
    "        # in the 'off' tensor\n",
    "        offsets_ent_B = range(off[2], off[3]+1)\n",
    "        # The offset of the pronoun is the last in the 'off' tensor\n",
    "        offset_pron = off[-1]\n",
    "\n",
    "        # The embedding of a mention is the mean of\n",
    "        # all the subtokens embeddings that represent it\n",
    "        embeddings_A.append(embeddings[offsets_ent_A].mean(dim=0))\n",
    "        embeddings_B.append(embeddings[offsets_ent_B].mean(dim=0))\n",
    "        embeddings_pron.append(embeddings[offset_pron])\n",
    "\n",
    "    # Merge outputs\n",
    "    merged_entities_and_pron_embeddings = torch.cat([\n",
    "        torch.stack(embeddings_A, dim=0),\n",
    "        torch.stack(embeddings_B, dim=0),\n",
    "        torch.stack(embeddings_pron, dim=0)\n",
    "    ], dim=1)\n",
    "    # print(torch.stack(outputs_A, dim=0))\n",
    "    # torch.stack(outputs_B, dim=0)\n",
    "    # print(torch.stack(outputs_pron, dim=0))\n",
    "    \n",
    "    # shape: batch_size x (embedding_dim * 3)\n",
    "    return merged_entities_and_pron_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefHead(nn.Module):\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size \n",
    "        self.head_hidden_size = 512\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(bert_hidden_size * 3, 512),           \n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 3)\n",
    "#         )\n",
    "        self.fc = nn.Sequential(\n",
    "#             nn.BatchNorm1d(bert_hidden_size * 3),  \n",
    "#             nn.Dropout(0.5),      \n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(bert_hidden_size * 3, self.head_hidden_size), \n",
    "#             nn.BatchNorm1d(self.head_hidden_size),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(self.head_hidden_size, self.head_hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(self.head_hidden_size),\n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(bert_hidden_size * 3, self.head_hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(self.head_hidden_size),\n",
    "            nn.Linear(self.head_hidden_size, 3)\n",
    "        )\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.shape[2] == self.bert_hidden_size\n",
    "        embeddings = retrieve_entities_and_pron_embeddings(bert_outputs,\n",
    "                                                          offsets)\n",
    "        \n",
    "        return self.fc(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions_s, samples):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, label in zip(predictions_s, samples):\n",
    "        gold_pron_offset = label[\"p_offset\"]\n",
    "        pred_pron_offset = pred[0][1] if len(pred[0]) > 0 else None\n",
    "        gold_pron = label[\"pron\"]\n",
    "        pred_pron = pred[0][0] if len(pred[0]) > 0 else None\n",
    "        gold_both_wrong = label[\"is_coref_A\"] == \"FALSE\" and label[\"is_coref_B\"] == \"FALSE\"\n",
    "        pred_entity_offset = pred[1][1] if len(pred[1]) > 0 else None\n",
    "        pred_entity = pred[1][0] if len(pred[1]) > 0 else None\n",
    "        if gold_both_wrong:\n",
    "            if pred_entity is None and gold_pron_offset == pred_pron_offset and gold_pron == pred_pron:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            gold_entity_offset = (\n",
    "                label[\"offset_A\"] if label[\"is_coref_A\"] == \"TRUE\" else label[\"offset_B\"]\n",
    "            )\n",
    "            gold_entity = (\n",
    "                label[\"entity_A\"] if label[\"is_coref_A\"] == \"TRUE\" else label[\"entity_B\"]\n",
    "            )\n",
    "            if (\n",
    "                gold_pron_offset == pred_pron_offset\n",
    "                and gold_pron == pred_pron\n",
    "                and gold_entity_offset == pred_entity_offset\n",
    "                and gold_entity == pred_entity\n",
    "            ):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(f\"# instances: {total}\")\n",
    "    acc = float(correct) / total\n",
    "    print(f\"# accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path=\"bert-base-uncased\"\n",
    "model = GAPModel(model_name_or_path).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0045, -0.0120,  0.0158,  ..., -0.0187, -0.0100,  0.0104],\n",
       "        [-0.0047, -0.0175, -0.0168,  ...,  0.0109, -0.0086, -0.0152],\n",
       "        [-0.0086,  0.0104,  0.0057,  ..., -0.0009,  0.0120,  0.0015],\n",
       "        ...,\n",
       "        [-0.0086,  0.0204, -0.0032,  ..., -0.0065, -0.0132,  0.0075],\n",
       "        [-0.0075, -0.0087,  0.0196,  ...,  0.0146, -0.0177, -0.0118],\n",
       "        [ 0.0087, -0.0112, -0.0129,  ...,  0.0022, -0.0154,  0.0030]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head.fc[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        args: TrainingArguments,\n",
    "        train_dataloader: DataLoader,\n",
    "        valid_dataloader: DataLoader,\n",
    "        criterion: torch.nn,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler = None,\n",
    "        \n",
    "    ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        if args is None:\n",
    "            output_dir = \"../../model/tmp_trainer\"\n",
    "            print(f\"No 'TrainingArguments' passed, using 'output_dir={output_dir}'.\")\n",
    "            args = TrainingArguments(output_dir=output_dir)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def train(self):\n",
    "        args = self.args\n",
    "        train_dataloader = self.train_dataloader\n",
    "        valid_dataloader = self.valid_dataloader\n",
    "        \n",
    "        train_losses = []\n",
    "        train_acc_list = []\n",
    "        valid_losses = []\n",
    "        valid_acc_list = []\n",
    "        \n",
    "        epochs = args.num_train_epochs\n",
    "        train_loss = 0.0\n",
    "        train_acc, total_count = 0.0, 0.0\n",
    "        \n",
    "        scaler = GradScaler()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for step, (features, offsets, labels) in enumerate(train_dataloader):\n",
    "                # Empty gradients\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Forward\n",
    "                predictions = self.model(features, offsets)\n",
    "                \n",
    "                \n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                train_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "                total_count += labels.shape[0]\n",
    "                \n",
    "#                 # Backward  \n",
    "#                 loss.backward()\n",
    "                # Backward pass without mixed precision\n",
    "                # It's not recommended to use mixed precision for backward pass\n",
    "                # Because we need more precise loss\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clipping)\n",
    "                \n",
    "                # Update weights \n",
    "#                 self.optimizer.step()\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "        \n",
    "                \n",
    "                epoch_loss += loss.tolist()\n",
    "\n",
    "                if step % args.logging_steps == args.logging_steps - 1:\n",
    "                    mid_loss = epoch_loss / (step + 1)\n",
    "                    mid_acc = train_acc / total_count\n",
    "#                     print('\\t[E: {:2d} @ step {}] current avg loss = {:0.4f}'.format(epoch, step, mid_loss))\n",
    "                    print(f'\\t| step {step+1:3d}/{len(train_dataloader):d} | train_loss: {mid_loss:.3f} | ' \\\n",
    "                    f'train_acc: {mid_acc:.3f} |')\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "            train_loss += avg_epoch_loss\n",
    "            train_losses.append(train_loss)\n",
    "            train_acc_list.append(train_acc / total_count)\n",
    "            \n",
    "#             print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))  # print train loss at the end of the epoch\n",
    "            \n",
    "    \n",
    "            valid_loss, valid_acc = self.evaluate(valid_dataloader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_acc_list.append(valid_acc)\n",
    "            \n",
    "#             print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
    "            print('-' * 75)\n",
    "            print(f'| epoch {epoch+1:3d}/{epochs:d} | train_loss: {avg_epoch_loss:.3f} | ' \\\n",
    "                    f'valid_loss: {valid_loss:.3f} | valid_acc: {valid_acc:.3f} |')\n",
    "            print('-' * 75)\n",
    "            \n",
    "        avg_epoch_loss = train_loss / epochs\n",
    "        histories = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_acc\": train_acc_list,\n",
    "            \"valid_losses\": valid_losses,\n",
    "            \"valid_acc\": valid_acc_list,\n",
    "\n",
    "        }\n",
    "#         print(histories)\n",
    "        \n",
    "        return #avg_epoch_loss, histories\n",
    "            \n",
    "    def evaluate(self, eval_dataloader):\n",
    "        valid_loss = 0.0\n",
    "        eval_acc, total_count = 0, 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (features, offsets, labels) in eval_dataloader:\n",
    "                \n",
    "                predictions = self.model(features, offsets)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "                valid_loss += loss.tolist()\n",
    "\n",
    "                eval_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "                total_count += labels.shape[0]\n",
    "        \n",
    "        return valid_loss / len(eval_dataloader), eval_acc / total_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-05"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "# yaml_file = \"reproduce.yaml\"\n",
    "yaml_file = \"./train.yaml\"\n",
    "# yaml_file = \"predict.yaml\"\n",
    "\n",
    "# Read configuration file with all the necessary parameters\n",
    "with open(yaml_file) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "training_args = TrainingArguments(**config['training_args'])\n",
    "\n",
    "# Make sure that the learning rate is read as a number and not as a string\n",
    "training_args.learning_rate = float(training_args.learning_rate)\n",
    "training_args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device=device, non_blocking=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_args.learning_rate)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                              collate_fn=collate_batch, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, \n",
    "                              collate_fn=collate_batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, training_args, \n",
    "                  train_dataloader, valid_dataloader, \n",
    "                  criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| step   5/25 | train_loss: 1.068 |train_acc: 0.450\n",
      "\t| step  10/25 | train_loss: 1.105 |train_acc: 0.400\n",
      "\t| step  15/25 | train_loss: 1.156 |train_acc: 0.283\n",
      "\t| step  20/25 | train_loss: 1.145 |train_acc: 0.300\n",
      "\t| step  25/25 | train_loss: 1.098 |train_acc: 0.370\n",
      "---------------------------------------------------------------------------\n",
      "| epoch   1/2 | train_loss: 1.098 | valid_loss: 0.957 | valid_acc: 0.560\n",
      "---------------------------------------------------------------------------\n",
      "\t| step   5/25 | train_loss: 1.139 |train_acc: 0.358\n",
      "\t| step  10/25 | train_loss: 1.091 |train_acc: 0.393\n",
      "\t| step  15/25 | train_loss: 1.065 |train_acc: 0.406\n",
      "\t| step  20/25 | train_loss: 1.072 |train_acc: 0.389\n",
      "\t| step  25/25 | train_loss: 1.049 |train_acc: 0.405\n",
      "---------------------------------------------------------------------------\n",
      "| epoch   2/2 | train_loss: 1.049 | valid_loss: 0.842 | valid_acc: 0.560\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4222, device='cuda:0') 1.4222300052642822\n",
      "tensor(0.7423, device='cuda:0') 0.7422657608985901\n",
      "tensor(2.1204, device='cuda:0') 2.1204404830932617\n",
      "tensor(2.1204, device='cuda:0') 2.120441198348999\n",
      "tensor(0.7423, device='cuda:0') 0.7422659993171692\n",
      "tensor(2.8095, device='cuda:0') 2.809528350830078\n",
      "tensor(1.4314, device='cuda:0') 1.4313530921936035\n",
      "tensor(1.4314, device='cuda:0') 1.4313533306121826\n",
      "tensor(2.8095, device='cuda:0') 2.80952787399292\n",
      "tensor(2.1204, device='cuda:0') 2.12044095993042\n",
      "tensor(2.8095, device='cuda:0') 2.809528350830078\n",
      "tensor(2.1204, device='cuda:0') 2.1204402446746826\n",
      "tensor(2.1204, device='cuda:0') 2.1204416751861572\n",
      "1 50 0.02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9077121019363403"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5529680384\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer(sent, padding='max_length', truncation=True,  max_length=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,oaa,_ = list(zip(*train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1510"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for elem in oaa:\n",
    "    if elem[1] - elem[0] == 1 or elem[3] - elem[2] == 1:\n",
    "        count += 1\n",
    "#         print(elem[0], elem[1], elem)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                              collate_fn=collate_batch, shuffle=False)\n",
    "# valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, \n",
    "#                               collate_fn=collate_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\flori/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\flori/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "bert = BertModel.from_pretrained(bert_model_name).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head = CorefHead(768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, offsets, labels in train_dataloader:\n",
    "    off = offsets\n",
    "    output = bert(features, attention_mask=(features > 0).long(), \n",
    "                  token_type_ids=None, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "#     res = head(output.hidden_states[-1], offsets)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = output.last_hidden_state\n",
    "# last[0][42]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 43, 45, 45, 62],\n",
       "        [51, 51, 54, 55, 61]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for embeddings in last:\n",
    "#     print(embeddings[42])\n",
    "#     break\n",
    "\n",
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7714],\n",
       "        [ 0.1712,  0.2298, -0.3611,  ...,  0.5080, -0.7124,  0.5206]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of = offsets[:, range(0,2)]\n",
    "\n",
    "batch_size = last.shape[0]\n",
    "offsets_ent_A = offsets[:, range(0,2)]\n",
    "offsets_ent_B = offsets[:, range(2,4)]\n",
    "\n",
    "outputs_A = []\n",
    "outputs_B = []\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "    outputs_A.append(last[batch_idx, range(offsets_ent_A[batch_idx][0], \n",
    "                                offsets_ent_A[batch_idx][1] + 1)].mean(dim=0))\n",
    "    outputs_B.append(last[batch_idx, range(offsets_ent_B[batch_idx][0], \n",
    "                                offsets_ent_B[batch_idx][1] + 1)].mean(dim=0))\n",
    "#     print(out_B)\n",
    "\n",
    "torch.stack(outputs_A, dim=0)\n",
    "torch.stack(outputs_B, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT 0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "def JIT_retrieve_entities_and_pron_embeddings(bert_embeddings, entities_and_pron_offsets):\n",
    "    embeddings_A = []\n",
    "    embeddings_B = []\n",
    "    embeddings_pron = []\n",
    "\n",
    "    # Consider embeddings and offsets in each batch separately\n",
    "    for embeddings, off in zip(bert_embeddings, entities_and_pron_offsets):\n",
    "        # The offsets of mention A are the first and the second\n",
    "        # in the 'off' tensor\n",
    "        offsets_ent_A = range(off[0], off[1]+1) \n",
    "        # The offsets of mention B are the third and the fourth\n",
    "        # in the 'off' tensor\n",
    "        offsets_ent_B = range(off[2], off[3]+1)\n",
    "        # The offset of the pronoun is the last in the 'off' tensor\n",
    "        offset_pron = off[-1]\n",
    "\n",
    "        # The embedding of a mention is the mean of\n",
    "        # all the subtokens embeddings that represent it\n",
    "        \n",
    "#         embeddings_A.append(embeddings[offsets_ent_A].mean(dim=0))\n",
    "#         embeddings_B.append(embeddings[offsets_ent_B].mean(dim=0))\n",
    "        embeddings_A.append(average_tensors(embeddings[offsets_ent_A]))\n",
    "        embeddings_B.append(average_tensors(embeddings[offsets_ent_B]))\n",
    "        embeddings_pron.append(embeddings[offset_pron])\n",
    "\n",
    "    # Merge outputs\n",
    "    merged_entities_and_pron_embeddings = torch.cat([\n",
    "        torch.stack(embeddings_A, dim=0),\n",
    "        torch.stack(embeddings_B, dim=0),\n",
    "        torch.stack(embeddings_pron, dim=0)\n",
    "    ], dim=1)\n",
    "    # print(torch.stack(outputs_A, dim=0))\n",
    "    # torch.stack(outputs_B, dim=0)\n",
    "    # print(torch.stack(outputs_pron, dim=0))\n",
    "    return merged_entities_and_pron_embeddings\n",
    "end = time.time()\n",
    "print(\"JIT\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6823, -0.3178,  0.3278,  ...,  0.2920,  0.7835,  0.1617],\n",
       "        [ 0.7445, -0.5703,  0.8773,  ...,  0.7930,  0.3856, -0.4485]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_entities_and_pron_embeddings(last, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def average_tensors(tensor):\n",
    "    return tensor.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]],\n",
       "\n",
       "         [[ 0.4035, -0.6086,  0.5744,  ..., -0.3491,  0.4572, -1.3359]]],\n",
       "\n",
       "\n",
       "        [[[-0.7060,  0.1587, -0.8657,  ...,  0.5993, -0.2363, -0.4840]],\n",
       "\n",
       "         [[-0.0975, -0.2683, -0.6387,  ...,  0.7930,  0.3856, -0.4485]]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pron_off = offsets[:, [4]]\n",
    "\n",
    "last[:,pron_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.3807e-01,  4.6052e-02,  5.1328e-01,  ..., -4.7629e-01,\n",
       "           2.4409e-01,  4.8122e-01],\n",
       "         [ 8.2649e-01, -6.8165e-01,  1.4226e-01,  ..., -3.3018e-01,\n",
       "           3.0405e-01, -3.0243e-01]],\n",
       "\n",
       "        [[ 6.7679e-01, -8.6487e-02, -4.3221e-01,  ...,  2.4297e-01,\n",
       "           3.4243e-01, -2.5505e-05],\n",
       "         [ 3.7130e-02, -3.2842e-01, -3.6175e-01,  ...,  3.9216e-01,\n",
       "           4.6714e-01, -8.1941e-02]]], device='cuda:0',\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last[:, range(42, 44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.7506e-02, -2.6827e-01, -6.3870e-01, -9.7911e-01,  1.6228e-01,\n",
       "         1.3821e-01,  2.0999e-01,  1.4227e+00, -2.0495e-01, -6.8178e-01,\n",
       "         9.7347e-02, -1.7352e-01, -4.7426e-02,  7.3415e-01, -5.3100e-01,\n",
       "        -6.1785e-03,  5.9988e-01, -5.5689e-01,  2.7243e-02,  9.7765e-01,\n",
       "         1.3741e-01, -1.2155e-01, -4.9975e-01,  6.1627e-01,  6.5771e-01,\n",
       "         3.9494e-01, -8.6346e-02,  2.1224e-01, -4.6273e-01,  3.0996e-01,\n",
       "         6.4127e-01, -2.3594e-01,  6.1242e-01, -5.5229e-01, -4.4161e-01,\n",
       "        -9.5874e-01, -5.2585e-02,  4.6089e-01, -2.9261e-01,  5.5857e-01,\n",
       "        -8.1286e-02, -4.7450e-01,  2.6060e-02, -1.5944e+00, -2.2374e-01,\n",
       "         1.9711e-01,  1.7964e+00, -1.3266e-01, -9.0319e-01,  3.3096e-01,\n",
       "         1.7975e-01, -9.3368e-02, -8.8890e-01,  1.9973e-01, -5.0889e-01,\n",
       "        -3.6299e-01,  3.0422e-01,  7.3791e-01, -6.6398e-01, -1.7381e-01,\n",
       "         7.2020e-01, -1.9225e-01,  4.7393e-03, -1.4993e-02,  1.0241e+00,\n",
       "         3.8244e-01,  1.5110e+00,  6.5938e-02, -1.5541e+00,  1.2973e+00,\n",
       "        -1.1703e+00,  2.3923e-01, -2.7773e-01, -6.7374e-01,  7.3416e-01,\n",
       "         4.5000e-01, -1.8241e-01,  1.4978e-01,  2.6867e-01, -1.4998e-01,\n",
       "         5.2191e-01,  4.6963e-01, -8.3884e-02,  3.5290e-01, -1.5164e-01,\n",
       "        -6.8776e-01, -5.6650e-01,  2.0841e-02, -1.9615e-03, -1.4490e-02,\n",
       "        -8.2556e-01, -6.9784e-01, -8.0296e-01, -2.7178e-01, -5.1078e-03,\n",
       "        -1.6646e-01, -7.2245e-01,  1.5868e-01, -1.2095e-01, -5.9115e-01,\n",
       "        -3.9740e-01, -5.0987e-01, -3.4102e-01, -2.1765e-01, -6.7981e-01,\n",
       "        -5.0332e-01, -5.7031e-02, -4.5478e-01, -9.8139e-01, -3.2532e-01,\n",
       "         7.5756e-01, -5.1247e-01,  6.9888e-01, -4.8778e-01, -6.4962e-01,\n",
       "         7.9803e-01, -4.7431e-01, -5.8192e-01,  3.2977e-01,  3.1161e-02,\n",
       "        -8.0351e-01, -3.6703e-01, -6.2371e-01,  1.5942e+00, -4.7112e-01,\n",
       "         1.0586e-01, -9.3129e-01,  7.9169e-02,  5.0164e-02,  5.9064e-02,\n",
       "         1.9739e-01,  1.3144e-01,  6.7838e-01,  3.1505e-01, -1.4143e+00,\n",
       "         3.8878e-01, -2.1899e-01, -3.9801e-01, -3.3359e-01, -4.1985e-01,\n",
       "         5.0226e-01, -1.1478e+00,  9.4681e-01,  2.8230e-01, -6.5561e-02,\n",
       "         1.5675e-01, -6.7398e-01, -5.4249e-01, -6.5511e-01,  2.2809e-01,\n",
       "         2.7412e-01,  1.1293e+00,  4.7738e-01, -7.6197e-01,  3.1379e-01,\n",
       "         5.2643e-01, -1.0632e-01,  1.0212e+00,  6.8349e-01, -1.9339e-01,\n",
       "         1.0176e+00,  8.5783e-01,  3.8570e-02, -1.3980e-01, -1.5194e-02,\n",
       "         4.7840e-01, -6.8290e-01,  1.4828e+00, -7.8371e-01,  4.0403e-01,\n",
       "         1.3565e-01, -3.4307e-02,  9.0433e-01, -3.6220e-01, -7.1096e-01,\n",
       "         6.2734e-01, -6.3275e-01,  3.2876e-01, -3.2719e-02, -6.3355e-01,\n",
       "        -5.6047e-01,  5.1549e-01,  1.6903e-01,  4.9882e-02,  1.1340e+00,\n",
       "        -7.7536e-01,  1.8818e-01, -6.6275e-01,  6.5836e-01,  3.5397e-01,\n",
       "        -6.6205e-01, -9.0679e-01, -5.0491e-01,  8.5472e-01, -3.2865e-01,\n",
       "        -1.0111e+00, -3.3891e-01, -9.4128e-01,  5.5662e-01, -5.5804e-01,\n",
       "        -2.6035e-01, -5.8008e-01, -2.3193e-01,  3.0736e-01, -6.1474e-01,\n",
       "         3.5577e-02,  6.1017e-01, -3.0423e-01,  5.9903e-01,  5.7670e-01,\n",
       "        -4.7224e-01,  4.6078e-01, -6.4298e-02,  1.3025e+00,  8.9497e-01,\n",
       "        -1.0040e-02,  4.5070e-01,  3.0575e-01, -6.0456e-01,  3.4795e-01,\n",
       "         7.6256e-01, -2.4415e-01, -9.4799e-01, -3.9635e-01, -1.2461e+00,\n",
       "         8.1008e-01,  1.6499e-01, -9.8210e-01,  2.7727e-01, -1.8246e-01,\n",
       "        -4.7916e-01, -8.0330e-01,  5.4506e-01, -8.9649e-01, -9.8892e-02,\n",
       "         4.7335e-01,  2.9984e-01,  3.9579e-01,  4.5944e-02, -6.8851e-01,\n",
       "        -8.4782e-01,  1.5640e-01,  2.8245e-01, -1.3912e-01,  3.2699e-02,\n",
       "         4.8402e-01,  1.1923e+00,  3.4578e-01,  1.0200e-01,  3.7083e-01,\n",
       "         4.4915e-01, -7.1967e-01, -5.5011e-01, -2.8137e-01, -5.9447e-01,\n",
       "        -5.9579e-01,  1.5635e-01, -7.6333e-02,  7.8671e-01, -1.5539e-01,\n",
       "        -9.2124e-02, -8.9953e-02,  2.2157e+00,  9.0510e-02,  9.4464e-02,\n",
       "        -1.5436e-01,  9.3507e-01,  1.2714e-01,  6.0334e-01,  1.5731e-01,\n",
       "         2.5227e-01, -4.6683e-01, -4.6672e-01,  3.8064e-01, -3.4213e-01,\n",
       "        -3.3129e-01,  1.4852e-02, -3.2189e-01,  5.6648e-01, -5.9460e-02,\n",
       "         1.8525e-02,  8.8839e-01, -1.4743e+00, -2.4709e-01, -9.1390e-02,\n",
       "         8.0603e-01, -1.0488e+00,  4.7306e-01, -1.0736e-01, -1.0761e+00,\n",
       "        -4.7489e-01,  1.0208e+00,  9.7228e-02,  1.0469e-01, -8.2553e-03,\n",
       "         3.2124e-01, -3.9689e-01, -1.0484e+00,  2.8486e-01, -4.1178e-02,\n",
       "        -7.1318e-01, -4.2509e-01,  9.7829e-01,  2.5873e-01,  4.9530e-01,\n",
       "        -2.6344e-01,  4.6436e-01, -5.2410e-02, -2.5369e+00, -5.8548e-01,\n",
       "        -1.6927e-01, -3.4412e-01,  5.1542e-01,  1.7552e-01, -6.6228e-01,\n",
       "        -3.7443e-01, -1.8101e-01,  4.8785e-01, -5.2751e-01,  1.3295e-01,\n",
       "         6.3653e-01,  7.7862e-01,  7.0716e-01, -8.7274e-01,  9.7013e-02,\n",
       "        -9.5710e-01, -7.4257e-01,  1.0353e+00, -1.0471e+00, -1.1839e+00,\n",
       "         2.2669e-02,  1.4338e-01, -3.6186e-01,  9.3087e-01, -3.1279e-01,\n",
       "         4.1092e-01, -2.1580e+00, -7.0557e-01, -2.3723e-02, -4.0093e-01,\n",
       "         6.3424e-01,  9.4187e-01,  5.0417e-01, -1.7657e-01,  2.8769e-01,\n",
       "        -5.7191e-01,  1.5691e+00,  1.9001e-01,  8.3942e-01,  1.5819e-02,\n",
       "         5.5343e-02, -1.8017e-01,  1.0880e+00,  3.3742e-01,  1.1963e+00,\n",
       "        -1.0275e+00, -1.9618e-01, -2.3278e-01,  7.7330e-01,  4.3601e-01,\n",
       "        -2.3989e-01, -5.3362e-01,  2.0220e-01, -5.0570e-01,  2.0617e-01,\n",
       "         2.8360e-01, -9.8679e-01, -5.9940e-01, -1.7351e-01, -3.3145e-01,\n",
       "        -1.6348e-01,  7.1620e-03, -1.0334e+00, -5.0020e-01, -2.5944e-01,\n",
       "        -4.7915e-01,  8.1322e-01,  1.8759e-01, -3.8195e-01, -3.5615e-02,\n",
       "        -3.2854e-01, -1.4819e+00, -5.4000e-01, -3.7249e-02,  7.4910e-01,\n",
       "        -2.5781e-01, -1.8254e-01, -4.5513e-01,  2.8242e-01,  1.7445e-01,\n",
       "        -4.5298e-01,  2.4975e-01, -1.5127e-01,  1.0232e+00,  8.2359e-01,\n",
       "        -2.3193e-01, -2.4543e-01, -3.9209e-01,  4.6203e-01,  7.5173e-03,\n",
       "         7.9451e-01,  4.9991e-01,  6.6980e-01, -9.0375e-01,  3.2411e-01,\n",
       "        -2.1423e-02,  1.4408e-01,  7.6866e-01, -7.8530e-01, -1.7488e-01,\n",
       "         1.5903e-01,  1.0121e+00,  3.7705e-01, -5.2591e-01, -1.1373e+00,\n",
       "         7.3970e-04, -4.7074e-02,  2.9660e-01, -2.2156e-01, -5.6646e-01,\n",
       "        -2.0272e-01, -4.2854e-01, -6.0799e-01, -4.2879e-02,  9.7398e-02,\n",
       "         9.9034e-02, -4.9224e-01,  3.3906e-01,  2.6994e-01,  2.7088e-01,\n",
       "        -1.6121e-02, -4.8938e-01, -6.1467e-02, -8.2294e-01,  4.7480e-01,\n",
       "        -5.3293e-01, -1.2708e-01,  3.3502e-01,  2.5698e-01,  2.8260e-01,\n",
       "         4.9709e-01,  4.7716e-01,  8.2065e-02, -1.1987e+00, -9.3084e-01,\n",
       "        -8.8431e-01,  1.5481e-01,  3.8580e-01,  5.1680e-02,  4.9106e-01,\n",
       "        -4.4690e-01,  5.4420e-01,  4.9054e-01,  2.6624e-01,  9.4131e-01,\n",
       "         8.1529e-01,  9.9846e-02,  8.3427e-01,  7.5087e-01, -5.3817e-01,\n",
       "         7.4060e-01,  7.3868e-01, -7.8591e-01, -7.4681e-02, -8.8482e-02,\n",
       "        -5.2443e-01, -6.4075e-02, -1.1488e+00, -6.8935e-02,  4.9291e-01,\n",
       "         8.8006e-01, -9.2895e-02,  4.8713e-01,  1.2805e+00, -2.2071e-01,\n",
       "         5.8645e-01, -1.1960e+00,  2.5429e-02,  7.9612e-01, -1.1274e+00,\n",
       "        -3.1984e-01, -3.0685e-01,  3.7235e-02,  2.0826e-01,  1.6879e-01,\n",
       "        -9.8183e-01, -1.1467e-01, -5.3448e-01,  1.3154e-02,  2.4778e-02,\n",
       "         2.0256e-01, -8.9125e-02,  4.5023e-01,  3.8494e-01, -1.1532e+00,\n",
       "         1.5901e-01, -3.5879e-01,  7.4875e-01, -7.2660e-01,  1.3949e-03,\n",
       "        -5.0581e-01,  8.2980e-01,  2.0177e-01,  3.3583e-01,  4.5725e-01,\n",
       "         1.0248e-01,  2.5928e-01,  3.6509e-02, -6.5361e-01, -4.9319e-01,\n",
       "        -9.9516e-02, -1.1186e+00,  3.2164e-01,  1.1395e-01, -3.8612e-01,\n",
       "         1.0714e+00, -7.0424e-01,  1.2356e-02, -3.0220e-01, -4.0492e-01,\n",
       "        -8.0803e-01, -3.8786e-01, -3.7464e-01,  1.5935e+00, -3.0096e-01,\n",
       "         5.4595e-01,  1.3516e+00, -1.0095e+00, -1.9386e-01, -5.1473e-01,\n",
       "         4.0537e-01, -9.2246e-02, -1.1293e-01, -1.0150e-01, -5.0299e-01,\n",
       "        -4.8465e-01,  8.8437e-01, -2.4490e-01, -2.6643e-01,  5.8813e-01,\n",
       "         4.5217e-02, -2.2178e-02,  5.5201e-01, -6.2936e-01, -2.3726e-01,\n",
       "         2.0804e-01,  7.0069e-01,  8.0596e-02, -2.2308e-01, -7.3003e-02,\n",
       "        -3.4100e-01,  9.1760e-01,  3.5667e-01, -8.4884e-02,  4.1701e-01,\n",
       "         6.3400e-01,  1.2408e+00,  8.7737e-01, -4.3368e-02, -1.3554e-01,\n",
       "         1.8187e-01, -3.2661e-01, -9.6127e-02, -7.9137e-01, -7.3068e-01,\n",
       "        -4.4226e-01,  3.2827e-01, -6.4458e-02,  2.2537e-02,  7.2965e-01,\n",
       "        -1.5514e-01, -4.6006e-01, -2.1741e-01, -3.5342e-01, -3.6288e-01,\n",
       "         2.4017e-02,  4.1622e-01,  8.5372e-01, -4.1868e-01, -6.1342e-02,\n",
       "         1.0338e-01,  6.1906e-01, -8.8818e-01, -1.4754e-01,  3.7735e-01,\n",
       "        -1.3711e+00, -7.9941e-01, -4.2863e-01,  1.0069e-01,  4.8554e-01,\n",
       "        -4.6248e-02,  4.9711e-02, -1.7200e-01, -3.9874e-02, -5.6866e-01,\n",
       "        -1.0898e+00, -3.9021e-01, -1.4062e-01,  3.3309e-01,  5.9235e-02,\n",
       "        -2.6518e-01,  7.0204e-01, -4.3947e-01, -5.1064e-01, -2.6540e-01,\n",
       "         8.2146e-02, -1.0918e+00,  4.0669e-01,  9.6873e-01,  1.4904e-01,\n",
       "        -2.9396e-01, -1.7973e-01, -2.2159e-01,  6.9543e-01,  4.0430e-01,\n",
       "        -6.2906e-01,  4.6877e-01,  2.5888e-01, -3.3053e-01, -5.8049e-01,\n",
       "        -3.1271e-01,  8.6266e-01, -3.1614e-01,  1.0188e-01,  5.0130e-01,\n",
       "        -3.9886e-01, -1.1815e-01,  3.4619e-01,  2.8316e-01, -4.6100e-01,\n",
       "         3.2765e-01,  3.2819e-01,  6.4314e-01,  6.5248e-01, -5.2806e-01,\n",
       "        -7.8399e-01, -2.8024e-01,  1.0933e+00, -7.3445e-01,  9.1014e-01,\n",
       "         2.9802e-01,  1.0576e+00,  1.0988e-01,  2.2319e-02,  9.6945e-01,\n",
       "         4.8579e-02, -4.9618e-01, -9.2081e-01, -2.2551e-01,  1.5585e-01,\n",
       "         7.9264e-01,  3.7656e-01, -3.5653e-01, -7.3193e-02, -5.2664e-01,\n",
       "        -1.0522e-01, -6.4597e-01,  1.1686e+00, -3.5806e-01,  3.4634e-01,\n",
       "        -2.0662e-01, -4.7635e-01,  4.3035e-01, -1.4522e-01, -2.5184e-01,\n",
       "         8.9191e-02, -1.1307e+00,  4.0959e-01,  1.8007e-01, -3.0827e-01,\n",
       "         1.7323e+00, -8.5395e-01, -8.8562e-01,  4.2004e-02, -4.0862e-01,\n",
       "        -7.9459e-01,  4.7035e-01, -8.0703e-01, -4.3333e-02,  1.3682e-01,\n",
       "        -4.0559e-01,  1.1960e+00,  2.6341e-03,  1.5710e-01,  6.3311e-01,\n",
       "        -3.5123e-01, -1.0260e+00, -1.4157e+00, -7.4929e-01, -1.7219e-01,\n",
       "        -7.1146e-01, -3.7240e-01,  1.7573e+00, -7.4256e-01,  1.3248e-01,\n",
       "         6.4806e-01, -2.5998e-01, -1.3640e-02,  2.3002e-01, -2.2589e-01,\n",
       "         4.2534e-01, -5.1076e-01,  3.5656e-01, -1.3306e-01,  1.2447e-01,\n",
       "        -4.3837e-01,  9.0855e-01,  1.2569e-01,  1.2895e+00, -1.0385e+00,\n",
       "        -1.9348e-01, -6.3714e-02, -3.0752e-01, -2.1984e-01,  3.5187e-01,\n",
       "         5.8658e-02, -3.2757e-01,  5.2852e-01,  2.8454e-01,  5.1498e-01,\n",
       "        -1.5297e-01,  7.2675e-01,  2.0965e-01, -8.3656e-02,  9.0399e-01,\n",
       "         5.1116e-01,  6.5397e-01,  1.9344e-02,  9.0622e-02, -8.6612e-03,\n",
       "        -6.2636e-02, -2.3439e-01, -3.0753e-02, -7.0516e-01, -5.2903e-01,\n",
       "         4.2852e-01,  3.9170e-01,  7.6670e-01, -7.1870e-01,  4.3203e-01,\n",
       "        -5.7521e-03,  1.7118e-01, -2.9177e-01, -3.7217e-01,  1.3042e-01,\n",
       "        -4.2152e-01, -2.8616e-02, -3.0971e-01,  7.2041e-01,  1.0198e-01,\n",
       "         2.7601e-01, -7.0373e-01,  5.5492e-01,  5.0371e-03, -1.2870e-02,\n",
       "         3.6044e-01,  7.0969e-01, -3.7476e-01,  9.2233e-01, -1.0546e+00,\n",
       "         1.0354e+00, -1.4478e-01, -1.1190e+00, -3.3042e-01,  3.4315e-01,\n",
       "         7.9302e-01,  3.8555e-01, -4.4848e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(last[:, 42])\n",
    "last[1, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 64, 64])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape (batch_size, num_heads, sequence_length, sequence_length)\n",
    "print(len(output.attentions))\n",
    "output.attentions[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 768])\n",
      "torch.Size([768])\n",
      "tensor([[ 0.5381,  0.0461,  0.5133,  ..., -0.4763,  0.2441,  0.4812],\n",
      "        [ 0.8265, -0.6816,  0.1423,  ..., -0.3302,  0.3041, -0.3024],\n",
      "        [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
      "        [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
      "        [-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([2, 5, 768])\n",
      "tensor([[[ 0.5381,  0.0461,  0.5133,  ..., -0.4763,  0.2441,  0.4812],\n",
      "         [ 0.8265, -0.6816,  0.1423,  ..., -0.3302,  0.3041, -0.3024],\n",
      "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
      "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
      "         [-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]],\n",
      "\n",
      "        [[ 0.7445, -0.5703,  0.8773,  ...,  0.7073,  0.9091,  0.2327],\n",
      "         [ 0.7445, -0.5703,  0.8773,  ...,  0.7073,  0.9091,  0.2327],\n",
      "         [ 0.0447,  0.5941, -0.6054,  ...,  0.9097, -0.6916, -0.2175],\n",
      "         [ 0.2977, -0.1345, -0.1168,  ...,  0.1062, -0.7332,  1.2586],\n",
      "         [-0.0975, -0.2683, -0.6387,  ...,  0.7930,  0.3856, -0.4485]]],\n",
      "       device='cuda:0', grad_fn=<GatherBackward0>)\n",
      "torch.Size([2, 3840])\n",
      "tensor([[ 0.5381,  0.0461,  0.5133,  ...,  0.2920,  0.7835,  0.1617],\n",
      "        [ 0.7445, -0.5703,  0.8773,  ...,  0.7930,  0.3856, -0.4485]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[42, 43],\n",
       "         [45, 45]]], device='cuda:0')"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "\n",
    "o = output.hidden_states[-1]\n",
    "print(o[0][0].shape)\n",
    "print(o[0][off[0]])\n",
    "\n",
    "# extracted_outputs = o.gather(\n",
    "#             1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2)) \n",
    "#         ).view(bert_outputs.size(0), -1)\n",
    "# print(off.unsqueeze(2).expand(-1, -1, o.size(2)))\n",
    "a = o.gather(1, off.unsqueeze(2).expand(-1, -1, o.size(2)))\n",
    "print(a.shape)\n",
    "print(a)\n",
    "b = a.view(o.shape[0], -1)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "off[0][:4].view(-1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mention is represented as a span then `sum` the two spans to produce only one embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off_2 = off[:, :4].view(-1, 2, 2)\n",
    "first_ent = off[:,:2].squeeze()\n",
    "second_ent = off[:,2:4].squeeze()\n",
    "# pron = off[:, ]\n",
    "# pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 43, 45, 45, 62],\n",
       "        [51, 51, 54, 55, 61]], device='cuda:0')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[42, 42, 42,  ..., 42, 42, 42],\n",
      "         [45, 45, 45,  ..., 45, 45, 45],\n",
      "         [62, 62, 62,  ..., 62, 62, 62]],\n",
      "\n",
      "        [[51, 51, 51,  ..., 51, 51, 51],\n",
      "         [54, 54, 54,  ..., 54, 54, 54],\n",
      "         [61, 61, 61,  ..., 61, 61, 61]]], device='cuda:0')\n",
      "torch.Size([2, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5381,  0.0461,  0.5133,  ..., -0.4763,  0.2441,  0.4812],\n",
       "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
       "         [-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]],\n",
       "\n",
       "        [[ 0.7445, -0.5703,  0.8773,  ...,  0.7073,  0.9091,  0.2327],\n",
       "         [ 0.0447,  0.5941, -0.6054,  ...,  0.9097, -0.6916, -0.2175],\n",
       "         [-0.0975, -0.2683, -0.6387,  ...,  0.7930,  0.3856, -0.4485]]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ofs = off[:, [0, 2, 4]].unsqueeze(2).expand(-1, -1, 768)\n",
    "print(ofs)\n",
    "fin = torch.gather(o, 1, ofs)\n",
    "print(fin.shape)\n",
    "fin.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5381,  0.0461,  0.5133,  ...,  0.2920,  0.7835,  0.1617],\n",
       "        [ 0.7445, -0.5703,  0.8773,  ...,  0.7930,  0.3856, -0.4485]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.view(o.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.2767e-02,  1.0985e-02, -3.4086e-02,  ..., -9.8872e-04,\n",
       "          2.1345e-02,  8.9290e-03],\n",
       "        [ 2.4348e-02, -7.4422e-04,  1.8980e-02,  ...,  1.5269e-02,\n",
       "         -2.6324e-02,  7.5837e-06],\n",
       "        [-2.6595e-02,  1.9694e-03,  7.3221e-03,  ...,  1.2078e-02,\n",
       "         -3.5289e-02,  4.1908e-04]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(768, 3, device=device)\n",
    "lin.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4154, 0.5315, 0.5313],\n",
       "         [0.5333, 0.3944, 0.4611],\n",
       "         [0.3473, 0.3948, 0.5761]],\n",
       "\n",
       "        [[0.5846, 0.4685, 0.4687],\n",
       "         [0.4667, 0.6056, 0.5389],\n",
       "         [0.6527, 0.6052, 0.4239]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outtt = lin(fin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5381,  0.0461,  0.5133,  ..., -0.4763,  0.2441,  0.4812],\n",
       "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
       "         [-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]],\n",
       "\n",
       "        [[ 0.7445, -0.5703,  0.8773,  ...,  0.7073,  0.9091,  0.2327],\n",
       "         [ 0.0447,  0.5941, -0.6054,  ...,  0.9097, -0.6916, -0.2175],\n",
       "         [-0.0975, -0.2683, -0.6387,  ...,  0.7930,  0.3856, -0.4485]]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(10, 3, device=device)\n",
    "i = torch.LongTensor([[0,2,0,5], [0,2,0,5]]).to(device)\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9968, -0.8154,  1.0260],\n",
       "         [-0.5402, -0.0711, -1.1686],\n",
       "         [ 0.9968, -0.8154,  1.0260],\n",
       "         [-0.9610, -1.3230,  0.3743]],\n",
       "\n",
       "        [[ 0.9968, -0.8154,  1.0260],\n",
       "         [-0.5402, -0.0711, -1.1686],\n",
       "         [ 0.9968, -0.8154,  1.0260],\n",
       "         [-0.9610, -1.3230,  0.3743]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_out = emb(i)\n",
    "em_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1013,  0.3226],\n",
       "         [ 0.4605, -0.3026],\n",
       "         [-1.1013,  0.3226],\n",
       "         [-0.3290,  0.0594]],\n",
       "\n",
       "        [[-1.1013,  0.3226],\n",
       "         [ 0.4605, -0.3026],\n",
       "         [-1.1013,  0.3226],\n",
       "         [-0.3290,  0.0594]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = nn.Linear(3, 2, device=device)\n",
    "\n",
    "l(em_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 768])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([fin, fin], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_spans_embeddings(embeddings, single_dim_offsets):\n",
    "    if single_dim_offsets[0] == single_dim_offsets[1]:\n",
    "        ent_offset = single_dim_offsets[0].view(1, -1, 1)\n",
    "\n",
    "    # Span of two tokens\n",
    "    elif single_dim_offsets[1] - single_dim_offsets[0] == 1: \n",
    "        ent_offset = single_dim_offsets.view(1, -1, 1)\n",
    "        \n",
    "    else: # Span of multiple tokens\n",
    "        ent_offset = torch.tensor(range(single_dim_offsets[0], \n",
    "                                        single_dim_offsets[1]+1),\n",
    "                                  dtype=torch.int64,\n",
    "                                  device=device).view(1, -1, 1)\n",
    "    \n",
    "    ent_offset_expand = ent_offset.expand(-1, -1, embeddings.shape[2])\n",
    "    entity_embeddings = torch.gather(embeddings, 1, ent_offset_expand)\n",
    "        \n",
    "    # Sum the embeddings representing an entity (A or B)\n",
    "    # to produce a single representation for it\n",
    "    return entity_embeddings.sum(dim=1).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3936\\605440868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munify_spans_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_ent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3936\\2903034516.py\u001b[0m in \u001b[0;36munify_spans_embeddings\u001b[1;34m(embeddings, single_dim_offsets)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munify_spans_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_dim_offsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msingle_dim_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msingle_dim_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0ment_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msingle_dim_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Span of two tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "unify_spans_embeddings(o, first_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([1,1])\n",
    "# test\n",
    "t = torch.tensor(range(test[0], test[1]+1), device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ent.view(1,-1,1).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3262,  0.3077,  0.3330,  ...,  0.0246,  0.3267, -0.2383],\n",
       "         [ 1.5212, -0.4738,  0.9732,  ...,  0.8894,  0.1102,  1.0896],\n",
       "         [ 0.8939, -0.2292,  0.4933,  ...,  1.0340,  0.1831, -0.2677]]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(o, 1, t.view(1,-1,1).expand(-1,-1,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0889e+00, -3.9526e-01,  1.7995e+00, -2.0763e+00,  2.0231e+00,\n",
       "          2.8712e+00,  1.3911e+00,  5.0796e-01, -9.6528e-01, -1.1719e+00,\n",
       "          6.3579e-01, -5.0594e-01,  3.9600e-01, -5.8539e-01, -1.0251e+00,\n",
       "          9.8997e-01,  1.2978e+00,  7.6494e-01, -9.2404e-01, -1.7566e+00,\n",
       "          5.3220e-01, -4.5137e-01, -2.0958e+00,  3.4687e+00,  4.5470e-01,\n",
       "          5.9785e-01,  3.2067e-02, -9.5322e-02,  1.6850e-01,  1.2676e-01,\n",
       "          3.8936e+00, -1.8275e-02, -1.9699e-01, -4.9471e-02,  2.4774e-01,\n",
       "         -1.8513e-01, -1.5461e+00, -1.6436e-01, -5.8126e-01,  5.8290e-01,\n",
       "         -7.0645e-01, -1.8000e+00, -1.6877e+00,  2.1550e-01, -5.6485e-01,\n",
       "          1.0550e-02,  7.7738e-01,  1.1309e-01,  1.8419e+00, -6.3968e-01,\n",
       "         -3.6347e+00,  1.0722e+00,  9.5572e-01, -1.0473e+00,  1.3042e+00,\n",
       "          3.1889e+00, -2.7895e+00, -1.7851e+00,  9.5010e-01,  9.7430e-01,\n",
       "         -2.0797e-01,  9.7973e-01,  7.5529e-01, -2.1953e+00,  3.5696e-01,\n",
       "         -5.5551e-01, -1.2940e+00,  9.7933e-01, -1.6308e-01,  1.9688e+00,\n",
       "         -8.5871e-01, -1.9658e+00,  1.2016e-01, -2.7659e-01, -9.2107e-01,\n",
       "         -6.0355e-02,  5.7196e-01,  1.6084e+00,  1.2627e+00, -4.5945e-01,\n",
       "          8.4850e-01, -2.2574e-01, -1.5110e+00,  3.4896e+00,  1.0413e+00,\n",
       "          4.0863e-01, -6.5152e-01,  1.3053e+00, -3.6360e-01,  3.7228e+00,\n",
       "         -3.8126e-01, -7.9490e-01, -1.4969e+00, -3.0686e-01,  6.8365e-01,\n",
       "         -5.4099e-01, -3.0556e-01,  5.8262e-01,  2.3331e+00,  2.5680e-02,\n",
       "          1.0487e+00, -2.6138e+00,  1.8073e+00, -1.2489e+00, -3.1918e+00,\n",
       "         -1.4157e+00, -1.1914e+00, -1.3174e+00,  4.0835e-02, -2.0195e+00,\n",
       "          1.3312e+00,  5.4018e-01, -9.1024e-01, -1.6973e+00, -3.2051e-01,\n",
       "          6.4217e-01, -7.9477e-01,  5.4416e-01,  8.3670e-01,  9.2179e-01,\n",
       "          1.4929e+00,  2.5385e+00,  3.9500e-01,  3.4598e+00, -5.9603e-02,\n",
       "         -3.4160e-01, -3.1319e-01, -3.6384e-01, -1.3230e+00,  7.8419e-02,\n",
       "          5.7300e-01,  1.6451e+00, -2.9077e+00, -1.9882e-01, -1.8897e+00,\n",
       "          2.0892e-01,  6.9450e-01,  9.2367e-01, -5.6568e-01,  2.3959e+00,\n",
       "         -8.3777e-01, -4.8687e-01, -1.3986e+00,  1.3531e+00,  6.2763e-01,\n",
       "         -4.3303e-01, -2.2816e-01,  2.8881e-01,  7.4780e-01,  1.9156e+00,\n",
       "          6.7221e-01,  4.3788e-01, -3.5314e-01,  6.6195e-01, -2.0051e+00,\n",
       "         -1.1660e+00,  4.5186e-01,  1.6139e+00, -1.9381e-01, -6.1800e-01,\n",
       "          2.0741e+00, -1.0046e+00,  1.5068e+00,  3.7490e-01, -5.2560e-01,\n",
       "          6.6000e-01, -1.3867e+00,  1.4807e+00, -2.7348e+00, -5.1473e-01,\n",
       "          6.0335e-01, -2.1285e+00,  1.9061e+00, -1.2021e+00, -9.2809e-01,\n",
       "         -6.0365e-01, -3.4112e-01,  1.4583e+00, -1.1679e+00,  1.3350e+00,\n",
       "         -1.4243e+00,  1.0099e+00,  4.9415e-01,  6.8374e-01,  1.5817e+00,\n",
       "         -1.1944e+00, -3.1443e+00, -1.9736e+00, -9.1479e-02,  8.3389e-01,\n",
       "         -2.5184e+00, -1.7572e+00,  1.5091e+00, -3.2407e-01,  2.5282e+00,\n",
       "         -1.9577e+00,  1.5987e+00, -1.4165e+00, -1.5075e+00, -2.0218e-01,\n",
       "         -8.1975e-01, -9.7136e-01, -1.4041e+00,  1.5792e+00, -7.3187e-01,\n",
       "          8.4031e-01, -1.2966e-01, -8.6349e-01,  1.1541e+00,  4.4010e-01,\n",
       "         -2.7097e-01,  1.5995e-01, -7.4210e-01,  1.0626e+00, -1.6879e+00,\n",
       "          5.7920e-03, -3.5652e-01, -1.3891e+00,  1.1183e+00, -4.4128e-01,\n",
       "         -7.1154e-01,  2.5391e+00, -5.2670e-02, -1.0966e-01, -1.6245e+00,\n",
       "          4.4728e+00,  4.7458e-01, -1.4378e-01, -1.2128e-01, -1.3523e+00,\n",
       "          4.9572e-01,  6.0686e-01,  4.3046e-01, -5.3161e-01,  5.4159e-01,\n",
       "         -4.8266e-01, -1.5720e+00, -7.2184e-01, -1.3916e+00, -2.6303e+00,\n",
       "         -3.3111e-01,  1.3934e+00,  8.3656e-01, -2.1902e-01, -3.3910e-01,\n",
       "          1.1362e+00, -1.9219e-02, -8.9331e-01, -1.5707e+00, -5.3102e-01,\n",
       "         -1.4475e+00,  2.7814e+00, -7.7135e-01, -8.8099e-01,  2.5657e-01,\n",
       "         -3.4560e-01, -1.6553e+00,  4.4570e-01,  7.5045e-01,  5.9822e-01,\n",
       "          2.1191e+00,  6.2411e-03,  2.5290e+00,  1.1232e+00, -4.7996e-01,\n",
       "         -2.9966e+00,  5.9448e-01,  5.4400e-01,  6.0369e-01,  1.3878e+00,\n",
       "          1.4657e+00,  1.1251e+00,  1.2354e+00, -3.0040e-01,  4.1510e-01,\n",
       "          2.0032e+00,  3.4004e-01, -1.0194e+00, -1.7040e-01, -1.8613e+00,\n",
       "         -1.7068e-01,  2.6757e+00,  1.0918e-01,  2.6622e-01, -2.4575e+00,\n",
       "         -6.5599e-01,  1.8078e+00, -2.3927e-01, -1.3765e-01, -1.9344e+00,\n",
       "         -4.6474e-01,  2.2007e+00, -2.1517e+00, -9.2135e-01,  9.3850e-01,\n",
       "         -7.8850e-01,  6.0418e-01, -7.3335e-01,  3.7201e-01, -7.0125e-01,\n",
       "         -2.5072e-01,  4.5284e-01, -4.1468e-02, -1.5207e-01,  1.6677e+00,\n",
       "         -2.1060e+00, -4.0196e-01, -1.5484e+00, -6.6657e+00,  5.0792e-01,\n",
       "          7.9609e-02, -1.5572e+00,  1.3623e+00, -3.6120e-01,  6.8360e-01,\n",
       "          8.4762e-02, -3.3002e+00,  2.9849e+00,  6.6834e-01, -1.1718e+00,\n",
       "          1.1923e+00, -3.7509e-01,  1.0807e+00,  9.1442e-01, -7.3215e-01,\n",
       "         -7.4426e-01,  8.7294e-01,  8.1796e-02, -4.3799e-01, -1.8884e+00,\n",
       "          4.3796e-01, -4.9446e-01,  7.4073e-01,  1.5004e+00, -1.3926e+00,\n",
       "         -4.1831e-01, -5.5781e-01,  8.1824e-01, -1.9214e+00, -1.0574e+00,\n",
       "          1.6375e-02,  1.1616e+00, -9.4093e-01, -1.0908e+00,  4.6611e-02,\n",
       "         -1.2006e+00, -2.1011e+00, -1.5259e-01,  6.2968e-01, -8.8592e-01,\n",
       "         -3.4759e+00,  2.7736e-01,  2.7758e+00,  7.8124e-01,  4.8940e-01,\n",
       "          4.5908e-01, -1.2213e+00, -1.1611e-01, -1.2597e+00,  1.5061e+00,\n",
       "          6.2410e-03, -1.0621e+00, -1.6522e-01,  5.3792e-02,  1.7829e+00,\n",
       "          9.6265e-01, -1.5167e+00, -6.5989e-01, -2.2846e+00, -1.2096e+00,\n",
       "         -1.5550e+00,  1.3205e+00,  1.3458e+00, -1.8384e+00, -2.2654e+00,\n",
       "         -1.0606e+00,  1.3121e+00, -1.1873e+00, -5.3693e-01, -9.0636e-02,\n",
       "          1.6837e+00, -2.9200e+00, -1.5073e+00, -4.8377e-01,  2.0769e+00,\n",
       "          3.1036e-01, -1.0589e+00, -1.6251e+00, -2.1849e+00, -2.8306e+00,\n",
       "         -1.0223e-01, -1.1643e+00, -1.1202e+00, -1.2257e+00, -1.0495e+00,\n",
       "         -3.0695e+00, -1.3710e+00,  9.8023e-03,  1.4037e+00, -2.1181e+00,\n",
       "          1.2547e+00,  1.1272e+00, -7.7435e-01,  1.9686e+00,  3.7313e-02,\n",
       "         -9.3371e-01,  8.4809e-01, -9.5434e-01, -1.8056e+00,  1.0764e-01,\n",
       "          1.4219e+00,  8.9407e-01,  9.5175e-01, -8.4169e-01, -2.2618e-02,\n",
       "         -3.1692e-01, -1.7936e+00, -2.7068e+00,  1.3065e+00, -9.6798e-01,\n",
       "         -1.5422e+00, -2.9327e-01, -3.2837e+00, -1.5797e+00, -4.1611e-01,\n",
       "          9.3082e-01, -1.6994e+00, -3.2817e-01, -1.0532e+00,  1.2618e+00,\n",
       "         -8.3842e-01, -1.6012e-02,  2.5941e-01,  7.3317e-01, -1.4212e-01,\n",
       "          1.2013e+00, -1.4194e-01,  3.6803e-01, -5.8068e-01, -5.2131e-01,\n",
       "         -3.0148e-01, -5.2438e-02, -7.8069e-01, -4.8645e-01, -2.6218e+00,\n",
       "         -1.7472e+00, -2.5757e+00,  5.2875e-01, -2.2698e-02, -1.0468e+00,\n",
       "          1.8204e+00,  1.5607e+00,  1.8226e+00,  9.5212e-01, -5.1112e-01,\n",
       "          1.1931e-01, -1.0458e+00,  6.8844e-01, -1.8918e+00, -1.3696e+00,\n",
       "          7.2618e-02, -3.3230e+00, -1.3036e-01, -2.7252e-01, -4.9662e-01,\n",
       "         -3.7677e-01,  2.4662e-01, -1.5580e+00, -7.8475e-01,  2.8041e-01,\n",
       "         -7.5462e-01,  6.4338e-01,  4.3265e-01,  1.2937e+00, -1.0299e+00,\n",
       "          2.0168e+00,  4.7207e-01,  1.5215e+00,  1.6087e-01,  5.9935e-01,\n",
       "          2.3110e-01,  4.8786e-04, -1.3903e+00,  2.1729e+00, -4.5570e-01,\n",
       "          2.8167e-01, -5.8407e-01,  8.0775e-01, -5.3663e-01, -4.8626e-01,\n",
       "         -7.1710e-01,  2.1326e-01, -1.4092e-01,  1.7208e+00,  6.4139e-02,\n",
       "         -2.9634e-01, -6.0353e-01, -5.3551e-01,  9.1599e-01,  1.2990e+00,\n",
       "          1.4492e+00,  1.7915e+00, -9.2874e-01, -2.5656e+00,  7.6840e-01,\n",
       "         -2.6978e+00,  7.9947e-01, -1.0895e-01, -6.7863e-01, -1.6035e+00,\n",
       "         -1.1323e-01,  6.6323e-01, -2.8121e-02,  1.5545e-01, -6.0379e-02,\n",
       "          2.5854e+00,  5.4138e-01,  1.6271e+00, -2.0514e+00, -1.1023e+00,\n",
       "         -4.7528e-01, -2.2811e+00, -3.7759e-01, -2.0202e+00, -4.6481e-01,\n",
       "          1.5423e+00,  1.5068e-01, -3.8664e-01, -2.0314e+00,  8.1279e-01,\n",
       "          5.3796e-01,  7.0334e-02, -6.0356e-01,  1.1901e+00,  2.9908e+00,\n",
       "         -1.8726e-01, -9.6546e-01, -4.1189e-01,  1.3395e+00, -4.1338e+00,\n",
       "          1.1047e+00,  1.4128e+00,  2.3465e+00, -6.1046e-01, -2.0088e+00,\n",
       "         -9.4366e-01, -7.1055e-01,  6.9646e-01,  1.3764e+00,  1.1573e-01,\n",
       "         -1.6166e-01, -1.9368e+00,  1.0686e+00,  1.6292e-01,  3.8288e-01,\n",
       "          2.6438e-01, -3.6117e-01, -1.5009e-01,  1.3853e+00,  4.0127e-01,\n",
       "          4.6748e-01, -2.6516e+00, -1.4233e+00, -1.1306e+00, -9.8943e-01,\n",
       "         -1.3572e-01, -1.3018e+00,  3.2249e-01,  1.0629e-01, -4.4649e-01,\n",
       "          1.2348e-01,  8.6119e-01,  1.9978e-01,  1.6872e+00,  1.3942e+00,\n",
       "         -3.2795e-01, -6.7594e-01,  5.8037e-02, -5.5602e-01, -1.9138e+00,\n",
       "         -1.1855e+00,  1.7457e-01, -1.1679e+00, -8.1278e-01,  7.1078e-03,\n",
       "          6.4056e-01,  3.8149e-01, -8.3052e-03,  2.4593e+00,  1.2727e+00,\n",
       "          1.3071e+00, -4.9538e-01,  4.7571e-01, -8.7282e-01, -1.2768e+00,\n",
       "          1.9087e+00,  2.1445e+00, -1.4636e-01,  2.7910e+00,  1.8689e+00,\n",
       "         -8.2689e-02, -9.5484e-01, -5.4968e-01,  2.1178e+00, -3.6358e-01,\n",
       "         -7.0291e-01, -1.2395e+00,  2.9434e-01, -5.8546e-01, -7.3593e-01,\n",
       "          1.2529e-01,  1.8059e+00,  2.2949e-01, -4.3770e-01, -8.9344e-01,\n",
       "         -2.9909e+00,  1.7135e+00, -1.2359e+00,  8.6275e-01, -1.1655e+00,\n",
       "          2.7859e+00,  5.8682e-02,  1.5755e+00, -1.0657e+00, -1.1517e+00,\n",
       "         -1.0347e+00,  1.6563e-01, -4.7675e-01, -7.2467e-01,  9.9677e-01,\n",
       "          6.5059e-02, -6.1599e-01,  2.1884e+00,  3.7793e+00,  1.7310e-01,\n",
       "         -9.6948e-01, -3.6633e-02,  8.1665e-01, -5.7268e-01,  6.0057e-01,\n",
       "         -3.0587e-01,  1.7908e+00,  2.1602e+00, -2.0118e+00,  7.1480e-01,\n",
       "          4.4534e-01,  2.3965e-01,  3.9617e-01,  2.8416e-01,  2.3042e+00,\n",
       "          1.2762e+00,  2.6998e+00,  1.7990e+00,  1.6087e+00,  1.5158e-01,\n",
       "          9.8960e-01,  2.3754e+00, -8.6230e-01,  1.7725e+00,  1.3756e+00,\n",
       "          5.0572e-01, -2.0272e+00, -1.6335e+00, -1.5205e+00,  3.3346e-01,\n",
       "          2.9104e-01, -4.9671e-02,  4.8177e-01,  9.3871e-01, -1.3726e+00,\n",
       "          1.9798e+00, -1.6455e+00,  3.7118e-01, -3.1509e-01,  1.0977e+00,\n",
       "          5.2156e-01,  3.6354e-01,  4.7326e-01,  1.1706e+00,  2.4658e-01,\n",
       "          1.0874e+00, -7.0295e-01,  1.8545e+00,  1.1353e+00,  2.2079e+00,\n",
       "         -1.2483e+00, -6.2190e-01,  5.9091e-02,  1.3239e-01, -7.0448e-01,\n",
       "         -1.1063e+00, -9.4160e-01, -1.9685e+00, -1.4399e+00, -1.7177e+00,\n",
       "          2.1501e-01, -5.6627e-01, -7.3496e-01, -4.5948e-01,  1.3616e+00,\n",
       "          6.6542e-01, -1.3795e+00, -1.1569e+00, -6.9513e-01,  2.2108e+00,\n",
       "          2.9650e+00,  9.9337e-01,  7.1290e-01,  1.1728e+00,  1.2568e+00,\n",
       "         -1.4786e+00, -2.3376e-01,  1.3280e+00,  6.9953e-01,  6.1274e-01,\n",
       "          3.4307e+00, -3.0890e-01, -7.7693e-02, -2.8365e-01,  8.6451e-01,\n",
       "         -1.5748e+00, -1.0591e+00, -1.6290e-01,  1.9936e+00,  9.3304e-01,\n",
       "          1.2506e+00, -9.4952e-01, -1.6642e+00,  2.7072e-01, -1.3340e+00,\n",
       "          6.1397e-01, -1.1689e+00,  8.0559e-02,  6.7741e-01,  5.0583e-01,\n",
       "         -9.3893e-01,  9.5458e-01, -2.4234e-01,  3.3581e-01,  1.9199e+00,\n",
       "         -1.2603e+00, -1.1451e+00, -2.0036e+00,  5.6724e-01,  7.1967e-01,\n",
       "          1.1137e+00, -1.3799e+00,  7.0144e-01,  9.6684e-01, -1.4583e+00,\n",
       "         -1.2645e+00,  1.6736e+00, -3.2464e+00, -1.6704e+00,  1.0583e+00,\n",
       "         -1.2678e+00,  1.7041e-01, -7.9121e-01, -2.0500e-01,  5.2735e-01,\n",
       "          2.0766e-01, -2.9067e-01, -1.2745e+00, -1.7950e+00,  2.9254e-01,\n",
       "          1.9480e+00,  6.2002e-01,  5.8360e-01]], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(o, 1, t.view(1,-1,1).expand(-1,-1,768)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(first_ent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[42],\n",
       "         [43]]], device='cuda:0')"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ent.view(1, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0889e+00, -3.9526e-01,  1.7995e+00, -2.0763e+00,  2.0231e+00,\n",
       "           2.8712e+00,  1.3911e+00,  5.0796e-01, -9.6528e-01, -1.1719e+00,\n",
       "           6.3579e-01, -5.0594e-01,  3.9600e-01, -5.8539e-01, -1.0251e+00,\n",
       "           9.8997e-01,  1.2978e+00,  7.6494e-01, -9.2404e-01, -1.7566e+00,\n",
       "           5.3220e-01, -4.5137e-01, -2.0958e+00,  3.4687e+00,  4.5470e-01,\n",
       "           5.9785e-01,  3.2067e-02, -9.5322e-02,  1.6850e-01,  1.2676e-01,\n",
       "           3.8936e+00, -1.8275e-02, -1.9699e-01, -4.9471e-02,  2.4774e-01,\n",
       "          -1.8513e-01, -1.5461e+00, -1.6436e-01, -5.8126e-01,  5.8290e-01,\n",
       "          -7.0645e-01, -1.8000e+00, -1.6877e+00,  2.1550e-01, -5.6485e-01,\n",
       "           1.0550e-02,  7.7738e-01,  1.1309e-01,  1.8419e+00, -6.3968e-01,\n",
       "          -3.6347e+00,  1.0722e+00,  9.5572e-01, -1.0473e+00,  1.3042e+00,\n",
       "           3.1889e+00, -2.7895e+00, -1.7851e+00,  9.5010e-01,  9.7430e-01,\n",
       "          -2.0797e-01,  9.7973e-01,  7.5529e-01, -2.1953e+00,  3.5696e-01,\n",
       "          -5.5551e-01, -1.2940e+00,  9.7933e-01, -1.6308e-01,  1.9688e+00,\n",
       "          -8.5871e-01, -1.9658e+00,  1.2016e-01, -2.7659e-01, -9.2107e-01,\n",
       "          -6.0355e-02,  5.7196e-01,  1.6084e+00,  1.2627e+00, -4.5945e-01,\n",
       "           8.4850e-01, -2.2574e-01, -1.5110e+00,  3.4896e+00,  1.0413e+00,\n",
       "           4.0863e-01, -6.5152e-01,  1.3053e+00, -3.6360e-01,  3.7228e+00,\n",
       "          -3.8126e-01, -7.9490e-01, -1.4969e+00, -3.0686e-01,  6.8365e-01,\n",
       "          -5.4099e-01, -3.0556e-01,  5.8262e-01,  2.3331e+00,  2.5680e-02,\n",
       "           1.0487e+00, -2.6138e+00,  1.8073e+00, -1.2489e+00, -3.1918e+00,\n",
       "          -1.4157e+00, -1.1914e+00, -1.3174e+00,  4.0835e-02, -2.0195e+00,\n",
       "           1.3312e+00,  5.4018e-01, -9.1024e-01, -1.6973e+00, -3.2051e-01,\n",
       "           6.4217e-01, -7.9477e-01,  5.4416e-01,  8.3670e-01,  9.2179e-01,\n",
       "           1.4929e+00,  2.5385e+00,  3.9500e-01,  3.4598e+00, -5.9603e-02,\n",
       "          -3.4160e-01, -3.1319e-01, -3.6384e-01, -1.3230e+00,  7.8419e-02,\n",
       "           5.7300e-01,  1.6451e+00, -2.9077e+00, -1.9882e-01, -1.8897e+00,\n",
       "           2.0892e-01,  6.9450e-01,  9.2367e-01, -5.6568e-01,  2.3959e+00,\n",
       "          -8.3777e-01, -4.8687e-01, -1.3986e+00,  1.3531e+00,  6.2763e-01,\n",
       "          -4.3303e-01, -2.2816e-01,  2.8881e-01,  7.4780e-01,  1.9156e+00,\n",
       "           6.7221e-01,  4.3788e-01, -3.5314e-01,  6.6195e-01, -2.0051e+00,\n",
       "          -1.1660e+00,  4.5186e-01,  1.6139e+00, -1.9381e-01, -6.1800e-01,\n",
       "           2.0741e+00, -1.0046e+00,  1.5068e+00,  3.7490e-01, -5.2560e-01,\n",
       "           6.6000e-01, -1.3867e+00,  1.4807e+00, -2.7348e+00, -5.1473e-01,\n",
       "           6.0335e-01, -2.1285e+00,  1.9061e+00, -1.2021e+00, -9.2809e-01,\n",
       "          -6.0365e-01, -3.4112e-01,  1.4583e+00, -1.1679e+00,  1.3350e+00,\n",
       "          -1.4243e+00,  1.0099e+00,  4.9415e-01,  6.8374e-01,  1.5817e+00,\n",
       "          -1.1944e+00, -3.1443e+00, -1.9736e+00, -9.1479e-02,  8.3389e-01,\n",
       "          -2.5184e+00, -1.7572e+00,  1.5091e+00, -3.2407e-01,  2.5282e+00,\n",
       "          -1.9577e+00,  1.5987e+00, -1.4165e+00, -1.5075e+00, -2.0218e-01,\n",
       "          -8.1975e-01, -9.7136e-01, -1.4041e+00,  1.5792e+00, -7.3187e-01,\n",
       "           8.4031e-01, -1.2966e-01, -8.6349e-01,  1.1541e+00,  4.4010e-01,\n",
       "          -2.7097e-01,  1.5995e-01, -7.4210e-01,  1.0626e+00, -1.6879e+00,\n",
       "           5.7920e-03, -3.5652e-01, -1.3891e+00,  1.1183e+00, -4.4128e-01,\n",
       "          -7.1154e-01,  2.5391e+00, -5.2670e-02, -1.0966e-01, -1.6245e+00,\n",
       "           4.4728e+00,  4.7458e-01, -1.4378e-01, -1.2128e-01, -1.3523e+00,\n",
       "           4.9572e-01,  6.0686e-01,  4.3046e-01, -5.3161e-01,  5.4159e-01,\n",
       "          -4.8266e-01, -1.5720e+00, -7.2184e-01, -1.3916e+00, -2.6303e+00,\n",
       "          -3.3111e-01,  1.3934e+00,  8.3656e-01, -2.1902e-01, -3.3910e-01,\n",
       "           1.1362e+00, -1.9219e-02, -8.9331e-01, -1.5707e+00, -5.3102e-01,\n",
       "          -1.4475e+00,  2.7814e+00, -7.7135e-01, -8.8099e-01,  2.5657e-01,\n",
       "          -3.4560e-01, -1.6553e+00,  4.4570e-01,  7.5045e-01,  5.9822e-01,\n",
       "           2.1191e+00,  6.2411e-03,  2.5290e+00,  1.1232e+00, -4.7996e-01,\n",
       "          -2.9966e+00,  5.9448e-01,  5.4400e-01,  6.0369e-01,  1.3878e+00,\n",
       "           1.4657e+00,  1.1251e+00,  1.2354e+00, -3.0040e-01,  4.1510e-01,\n",
       "           2.0032e+00,  3.4004e-01, -1.0194e+00, -1.7040e-01, -1.8613e+00,\n",
       "          -1.7068e-01,  2.6757e+00,  1.0918e-01,  2.6622e-01, -2.4575e+00,\n",
       "          -6.5599e-01,  1.8078e+00, -2.3927e-01, -1.3765e-01, -1.9344e+00,\n",
       "          -4.6474e-01,  2.2007e+00, -2.1517e+00, -9.2135e-01,  9.3850e-01,\n",
       "          -7.8850e-01,  6.0418e-01, -7.3335e-01,  3.7201e-01, -7.0125e-01,\n",
       "          -2.5072e-01,  4.5284e-01, -4.1468e-02, -1.5207e-01,  1.6677e+00,\n",
       "          -2.1060e+00, -4.0196e-01, -1.5484e+00, -6.6657e+00,  5.0792e-01,\n",
       "           7.9609e-02, -1.5572e+00,  1.3623e+00, -3.6120e-01,  6.8360e-01,\n",
       "           8.4762e-02, -3.3002e+00,  2.9849e+00,  6.6834e-01, -1.1718e+00,\n",
       "           1.1923e+00, -3.7509e-01,  1.0807e+00,  9.1442e-01, -7.3215e-01,\n",
       "          -7.4426e-01,  8.7294e-01,  8.1796e-02, -4.3799e-01, -1.8884e+00,\n",
       "           4.3796e-01, -4.9446e-01,  7.4073e-01,  1.5004e+00, -1.3926e+00,\n",
       "          -4.1831e-01, -5.5781e-01,  8.1824e-01, -1.9214e+00, -1.0574e+00,\n",
       "           1.6375e-02,  1.1616e+00, -9.4093e-01, -1.0908e+00,  4.6611e-02,\n",
       "          -1.2006e+00, -2.1011e+00, -1.5259e-01,  6.2968e-01, -8.8592e-01,\n",
       "          -3.4759e+00,  2.7736e-01,  2.7758e+00,  7.8124e-01,  4.8940e-01,\n",
       "           4.5908e-01, -1.2213e+00, -1.1611e-01, -1.2597e+00,  1.5061e+00,\n",
       "           6.2410e-03, -1.0621e+00, -1.6522e-01,  5.3792e-02,  1.7829e+00,\n",
       "           9.6265e-01, -1.5167e+00, -6.5989e-01, -2.2846e+00, -1.2096e+00,\n",
       "          -1.5550e+00,  1.3205e+00,  1.3458e+00, -1.8384e+00, -2.2654e+00,\n",
       "          -1.0606e+00,  1.3121e+00, -1.1873e+00, -5.3693e-01, -9.0636e-02,\n",
       "           1.6837e+00, -2.9200e+00, -1.5073e+00, -4.8377e-01,  2.0769e+00,\n",
       "           3.1036e-01, -1.0589e+00, -1.6251e+00, -2.1849e+00, -2.8306e+00,\n",
       "          -1.0223e-01, -1.1643e+00, -1.1202e+00, -1.2257e+00, -1.0495e+00,\n",
       "          -3.0695e+00, -1.3710e+00,  9.8023e-03,  1.4037e+00, -2.1181e+00,\n",
       "           1.2547e+00,  1.1272e+00, -7.7435e-01,  1.9686e+00,  3.7313e-02,\n",
       "          -9.3371e-01,  8.4809e-01, -9.5434e-01, -1.8056e+00,  1.0764e-01,\n",
       "           1.4219e+00,  8.9407e-01,  9.5175e-01, -8.4169e-01, -2.2618e-02,\n",
       "          -3.1692e-01, -1.7936e+00, -2.7068e+00,  1.3065e+00, -9.6798e-01,\n",
       "          -1.5422e+00, -2.9327e-01, -3.2837e+00, -1.5797e+00, -4.1611e-01,\n",
       "           9.3082e-01, -1.6994e+00, -3.2817e-01, -1.0532e+00,  1.2618e+00,\n",
       "          -8.3842e-01, -1.6012e-02,  2.5941e-01,  7.3317e-01, -1.4212e-01,\n",
       "           1.2013e+00, -1.4194e-01,  3.6803e-01, -5.8068e-01, -5.2131e-01,\n",
       "          -3.0148e-01, -5.2438e-02, -7.8069e-01, -4.8645e-01, -2.6218e+00,\n",
       "          -1.7472e+00, -2.5757e+00,  5.2875e-01, -2.2698e-02, -1.0468e+00,\n",
       "           1.8204e+00,  1.5607e+00,  1.8226e+00,  9.5212e-01, -5.1112e-01,\n",
       "           1.1931e-01, -1.0458e+00,  6.8844e-01, -1.8918e+00, -1.3696e+00,\n",
       "           7.2618e-02, -3.3230e+00, -1.3036e-01, -2.7252e-01, -4.9662e-01,\n",
       "          -3.7677e-01,  2.4662e-01, -1.5580e+00, -7.8475e-01,  2.8041e-01,\n",
       "          -7.5462e-01,  6.4338e-01,  4.3265e-01,  1.2937e+00, -1.0299e+00,\n",
       "           2.0168e+00,  4.7207e-01,  1.5215e+00,  1.6087e-01,  5.9935e-01,\n",
       "           2.3110e-01,  4.8786e-04, -1.3903e+00,  2.1729e+00, -4.5570e-01,\n",
       "           2.8167e-01, -5.8407e-01,  8.0775e-01, -5.3663e-01, -4.8626e-01,\n",
       "          -7.1710e-01,  2.1326e-01, -1.4092e-01,  1.7208e+00,  6.4139e-02,\n",
       "          -2.9634e-01, -6.0353e-01, -5.3551e-01,  9.1599e-01,  1.2990e+00,\n",
       "           1.4492e+00,  1.7915e+00, -9.2874e-01, -2.5656e+00,  7.6840e-01,\n",
       "          -2.6978e+00,  7.9947e-01, -1.0895e-01, -6.7863e-01, -1.6035e+00,\n",
       "          -1.1323e-01,  6.6323e-01, -2.8121e-02,  1.5545e-01, -6.0379e-02,\n",
       "           2.5854e+00,  5.4138e-01,  1.6271e+00, -2.0514e+00, -1.1023e+00,\n",
       "          -4.7528e-01, -2.2811e+00, -3.7759e-01, -2.0202e+00, -4.6481e-01,\n",
       "           1.5423e+00,  1.5068e-01, -3.8664e-01, -2.0314e+00,  8.1279e-01,\n",
       "           5.3796e-01,  7.0334e-02, -6.0356e-01,  1.1901e+00,  2.9908e+00,\n",
       "          -1.8726e-01, -9.6546e-01, -4.1189e-01,  1.3395e+00, -4.1338e+00,\n",
       "           1.1047e+00,  1.4128e+00,  2.3465e+00, -6.1046e-01, -2.0088e+00,\n",
       "          -9.4366e-01, -7.1055e-01,  6.9646e-01,  1.3764e+00,  1.1573e-01,\n",
       "          -1.6166e-01, -1.9368e+00,  1.0686e+00,  1.6292e-01,  3.8288e-01,\n",
       "           2.6438e-01, -3.6117e-01, -1.5009e-01,  1.3853e+00,  4.0127e-01,\n",
       "           4.6748e-01, -2.6516e+00, -1.4233e+00, -1.1306e+00, -9.8943e-01,\n",
       "          -1.3572e-01, -1.3018e+00,  3.2249e-01,  1.0629e-01, -4.4649e-01,\n",
       "           1.2348e-01,  8.6119e-01,  1.9978e-01,  1.6872e+00,  1.3942e+00,\n",
       "          -3.2795e-01, -6.7594e-01,  5.8037e-02, -5.5602e-01, -1.9138e+00,\n",
       "          -1.1855e+00,  1.7457e-01, -1.1679e+00, -8.1278e-01,  7.1078e-03,\n",
       "           6.4056e-01,  3.8149e-01, -8.3052e-03,  2.4593e+00,  1.2727e+00,\n",
       "           1.3071e+00, -4.9538e-01,  4.7571e-01, -8.7282e-01, -1.2768e+00,\n",
       "           1.9087e+00,  2.1445e+00, -1.4636e-01,  2.7910e+00,  1.8689e+00,\n",
       "          -8.2689e-02, -9.5484e-01, -5.4968e-01,  2.1178e+00, -3.6358e-01,\n",
       "          -7.0291e-01, -1.2395e+00,  2.9434e-01, -5.8546e-01, -7.3593e-01,\n",
       "           1.2529e-01,  1.8059e+00,  2.2949e-01, -4.3770e-01, -8.9344e-01,\n",
       "          -2.9909e+00,  1.7135e+00, -1.2359e+00,  8.6275e-01, -1.1655e+00,\n",
       "           2.7859e+00,  5.8682e-02,  1.5755e+00, -1.0657e+00, -1.1517e+00,\n",
       "          -1.0347e+00,  1.6563e-01, -4.7675e-01, -7.2467e-01,  9.9677e-01,\n",
       "           6.5059e-02, -6.1599e-01,  2.1884e+00,  3.7793e+00,  1.7310e-01,\n",
       "          -9.6948e-01, -3.6633e-02,  8.1665e-01, -5.7268e-01,  6.0057e-01,\n",
       "          -3.0587e-01,  1.7908e+00,  2.1602e+00, -2.0118e+00,  7.1480e-01,\n",
       "           4.4534e-01,  2.3965e-01,  3.9617e-01,  2.8416e-01,  2.3042e+00,\n",
       "           1.2762e+00,  2.6998e+00,  1.7990e+00,  1.6087e+00,  1.5158e-01,\n",
       "           9.8960e-01,  2.3754e+00, -8.6230e-01,  1.7725e+00,  1.3756e+00,\n",
       "           5.0572e-01, -2.0272e+00, -1.6335e+00, -1.5205e+00,  3.3346e-01,\n",
       "           2.9104e-01, -4.9671e-02,  4.8177e-01,  9.3871e-01, -1.3726e+00,\n",
       "           1.9798e+00, -1.6455e+00,  3.7118e-01, -3.1509e-01,  1.0977e+00,\n",
       "           5.2156e-01,  3.6354e-01,  4.7326e-01,  1.1706e+00,  2.4658e-01,\n",
       "           1.0874e+00, -7.0295e-01,  1.8545e+00,  1.1353e+00,  2.2079e+00,\n",
       "          -1.2483e+00, -6.2190e-01,  5.9091e-02,  1.3239e-01, -7.0448e-01,\n",
       "          -1.1063e+00, -9.4160e-01, -1.9685e+00, -1.4399e+00, -1.7177e+00,\n",
       "           2.1501e-01, -5.6627e-01, -7.3496e-01, -4.5948e-01,  1.3616e+00,\n",
       "           6.6542e-01, -1.3795e+00, -1.1569e+00, -6.9513e-01,  2.2108e+00,\n",
       "           2.9650e+00,  9.9337e-01,  7.1290e-01,  1.1728e+00,  1.2568e+00,\n",
       "          -1.4786e+00, -2.3376e-01,  1.3280e+00,  6.9953e-01,  6.1274e-01,\n",
       "           3.4307e+00, -3.0890e-01, -7.7693e-02, -2.8365e-01,  8.6451e-01,\n",
       "          -1.5748e+00, -1.0591e+00, -1.6290e-01,  1.9936e+00,  9.3304e-01,\n",
       "           1.2506e+00, -9.4952e-01, -1.6642e+00,  2.7072e-01, -1.3340e+00,\n",
       "           6.1397e-01, -1.1689e+00,  8.0559e-02,  6.7741e-01,  5.0583e-01,\n",
       "          -9.3893e-01,  9.5458e-01, -2.4234e-01,  3.3581e-01,  1.9199e+00,\n",
       "          -1.2603e+00, -1.1451e+00, -2.0036e+00,  5.6724e-01,  7.1967e-01,\n",
       "           1.1137e+00, -1.3799e+00,  7.0144e-01,  9.6684e-01, -1.4583e+00,\n",
       "          -1.2645e+00,  1.6736e+00, -3.2464e+00, -1.6704e+00,  1.0583e+00,\n",
       "          -1.2678e+00,  1.7041e-01, -7.9121e-01, -2.0500e-01,  5.2735e-01,\n",
       "           2.0766e-01, -2.9067e-01, -1.2745e+00, -1.7950e+00,  2.9254e-01,\n",
       "           1.9480e+00,  6.2002e-01,  5.8360e-01]]], device='cuda:0',\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unify_spans_embeddings(o, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3646e+00, -6.3559e-01,  6.5554e-01, -8.3733e-01,  2.5619e-01,\n",
       "          -1.5913e-01,  2.6387e+00, -3.5175e-01, -2.2441e-01,  2.8293e-01,\n",
       "           3.9986e-02, -7.5838e-01,  1.0023e+00, -1.6775e-01, -5.7158e-01,\n",
       "           1.1860e+00, -3.9145e-01,  7.9424e-01, -5.9099e-01,  4.9874e-01,\n",
       "           9.0436e-01,  3.1083e-01, -1.6028e+00,  8.1725e-01, -4.8959e-03,\n",
       "           8.5088e-01, -2.1822e-01,  9.7481e-01, -1.8579e-01,  1.5771e+00,\n",
       "           1.2519e+00,  1.0207e-01,  1.2020e+00,  5.3160e-01,  3.6940e-01,\n",
       "           7.1516e-01,  2.0164e-02, -2.0736e-01,  1.4634e-01, -4.7201e-01,\n",
       "           4.1562e-01, -1.2816e+00,  3.5776e-01, -6.5965e-02, -4.6385e-01,\n",
       "          -7.1219e-03,  1.8831e+00, -1.0559e+00,  5.5174e-01, -1.1139e+00,\n",
       "          -1.6550e+00,  6.0078e-01, -3.7074e-01, -1.3973e+00,  9.2192e-01,\n",
       "          -4.2870e-01, -6.6090e-01, -1.4654e+00, -8.8921e-01, -3.9149e-01,\n",
       "           7.8399e-01,  5.8704e-02, -1.2684e+00, -6.8383e-01,  1.4816e-01,\n",
       "           4.7611e-01,  1.3767e+00,  1.4127e+00,  2.9941e-01, -6.1147e-01,\n",
       "          -6.3564e-02, -5.6466e-01,  8.5647e-02,  6.8510e-03, -4.7770e-01,\n",
       "           1.0266e+00,  5.2475e-01,  4.2147e-02, -1.9474e-01,  1.4228e+00,\n",
       "          -6.1024e-01,  1.7380e+00,  5.6154e-01,  1.6044e+00,  1.1653e+00,\n",
       "           1.0077e-01, -1.3706e+00,  1.7128e+00, -1.2535e+00,  2.1814e+00,\n",
       "          -7.1935e-01, -1.1133e-01, -1.7122e-01,  6.9373e-03,  4.2726e-01,\n",
       "          -3.9971e-01,  1.0213e+00,  5.2533e-01,  7.3119e-01, -1.9880e+00,\n",
       "          -2.0672e+00, -1.1071e+00, -4.4691e-01, -5.8536e-01, -1.7970e+00,\n",
       "          -9.3768e-01, -1.0162e+00, -1.7252e+00, -6.1596e-01,  5.3598e-01,\n",
       "          -1.1715e+00, -1.3495e+00,  1.7217e-01,  1.1540e-02, -2.7684e-01,\n",
       "           1.1102e+00, -2.0248e-01, -1.8035e-01, -6.6710e-01,  5.6617e-01,\n",
       "           3.9068e-01,  1.5963e-01, -7.0031e-01,  2.0611e+00, -9.5576e-02,\n",
       "          -3.2778e-01,  1.1179e+00, -6.1949e-01, -8.3455e-01, -9.4691e-01,\n",
       "          -1.5756e-01,  2.4566e-01, -1.0601e+00, -2.8721e-01, -1.4686e+00,\n",
       "           5.2108e-01, -1.0073e+00,  8.7673e-01,  2.6866e-01,  1.2694e+00,\n",
       "           5.2797e-01,  7.5024e-01,  8.4949e-01,  1.2213e+00,  3.4887e-01,\n",
       "          -2.0821e-01,  1.5794e-02, -1.0330e+00, -4.4477e-01,  2.5190e-01,\n",
       "          -2.9840e-01,  8.1981e-02,  4.9274e-01,  2.2240e-01,  2.0218e-02,\n",
       "           5.1196e-01,  1.8987e+00,  6.8566e-01, -1.2596e+00, -4.1176e-01,\n",
       "           2.4812e-01, -1.2386e+00, -1.2643e+00,  2.4763e-01,  1.3981e+00,\n",
       "          -1.4987e-01, -1.2199e-01, -1.5128e-01, -1.9023e+00, -9.3825e-01,\n",
       "           1.1733e-01,  2.3094e-01, -3.8682e-01,  9.3231e-01, -1.5193e-01,\n",
       "           1.8437e-01,  6.7099e-01,  1.3321e+00,  1.5765e-01,  6.9781e-01,\n",
       "          -3.6677e-01, -5.0955e-01, -1.1813e+00, -2.5253e-01,  1.9138e+00,\n",
       "           7.3900e-01, -5.1781e-01, -1.7640e-02, -1.6370e-01,  5.9659e-01,\n",
       "          -1.9285e+00, -2.5099e+00,  1.1351e+00,  3.2074e-01,  1.3736e-01,\n",
       "          -4.9236e-01,  1.4772e+00,  2.6765e-01,  8.5390e-01,  1.8222e-01,\n",
       "          -3.2312e-01,  4.0009e-01, -1.8550e+00,  7.0030e-01, -7.1331e-01,\n",
       "           2.1647e-01,  1.2475e+00, -2.6929e-01,  1.4341e-01,  8.7877e-01,\n",
       "          -6.7876e-01, -9.5359e-01, -5.6043e-01,  1.2118e+00,  4.6733e-03,\n",
       "           1.5613e-02,  7.8248e-01, -1.1625e+00,  1.8479e+00, -1.7826e+00,\n",
       "           2.7352e-02,  5.9098e-01,  8.0230e-01, -3.9725e-01, -9.2006e-01,\n",
       "           3.1478e+00,  1.1168e+00, -3.5361e-01,  7.2734e-01, -1.1301e+00,\n",
       "           4.6648e-01, -1.7098e-01,  4.9704e-02, -1.7505e+00,  8.4197e-02,\n",
       "           1.4808e-01,  5.8840e-01, -1.3450e+00, -2.9791e-01, -1.0127e+00,\n",
       "          -6.7787e-02,  1.7454e+00, -9.0487e-01,  1.1730e+00,  4.1313e-01,\n",
       "           5.1417e-01, -8.4507e-02, -7.2266e-01, -4.6033e-01,  2.2452e-01,\n",
       "          -4.2853e-01,  2.0964e+00,  1.7901e-01, -4.5263e-01, -4.2386e-01,\n",
       "          -7.0384e-01, -3.0953e-01,  2.2898e-01,  8.7093e-01,  2.2219e-01,\n",
       "           1.1267e+00, -2.3081e-01,  8.0341e-02,  7.6885e-01, -2.2063e-01,\n",
       "          -4.1144e-01, -2.9453e-01, -6.2621e-01, -6.1871e-01,  9.5564e-01,\n",
       "          -4.8449e-01,  2.8942e-01, -1.0006e+00,  8.2999e-01, -6.3504e-01,\n",
       "           9.5288e-01, -1.6457e-01, -1.9260e-01,  1.0046e+00, -1.1443e+00,\n",
       "           4.1053e-01,  1.7572e+00, -3.8894e-01, -4.6529e-01, -9.3904e-01,\n",
       "          -3.9098e-01,  2.5958e-01, -2.6009e-01, -3.3137e-01, -1.1071e+00,\n",
       "          -7.1973e-02,  8.1723e-01, -1.1279e+00, -2.6108e-01,  2.5609e-01,\n",
       "          -8.8512e-01, -4.5250e-01, -1.9328e-01,  1.4070e+00, -7.1019e-01,\n",
       "          -8.6423e-01,  1.4506e+00, -2.6285e-01,  2.2746e-01,  1.3137e+00,\n",
       "           6.4763e-01, -7.1030e-01, -5.1782e-01, -8.0105e+00,  1.2785e-01,\n",
       "           4.4743e-02,  1.1733e-01,  1.3859e+00, -1.0332e+00,  1.4702e+00,\n",
       "          -3.1681e-01, -7.6016e-01,  1.7528e+00, -1.9511e-01,  2.0712e-01,\n",
       "          -2.1386e-01, -5.9322e-01,  1.8632e+00, -2.2120e-01,  1.4712e+00,\n",
       "          -1.8456e+00, -4.7964e-01,  6.7528e-01,  7.1003e-01, -1.0070e+00,\n",
       "           9.8505e-02, -3.6625e-02,  6.3051e-02, -1.2269e+00, -3.3420e-01,\n",
       "          -8.3716e-01, -5.1270e-01, -9.4716e-01, -3.7202e-01,  1.8844e-01,\n",
       "          -1.0123e+00,  1.0942e+00,  6.5189e-01,  3.6975e-01,  9.1195e-01,\n",
       "          -1.6191e+00, -8.8775e-01, -8.4001e-01, -1.2172e-01, -7.8970e-01,\n",
       "           4.7622e-01, -1.3429e-02,  6.3728e-01,  1.4694e-01, -3.2752e-01,\n",
       "          -1.2152e+00,  5.4756e-02,  8.4282e-01, -4.3466e-01,  1.8180e+00,\n",
       "          -6.7314e-01, -6.4515e-01, -2.7519e-01,  8.5862e-01,  9.5559e-01,\n",
       "          -3.2566e-01, -2.0549e+00, -3.6730e-01, -8.6963e-01, -1.3382e+00,\n",
       "          -1.7202e-01, -1.9770e-01,  1.1825e-01, -2.1109e-01, -4.8039e-01,\n",
       "          -7.9123e-01, -1.0071e+00, -7.0156e-01, -1.4482e+00, -1.1407e-01,\n",
       "           1.9939e+00, -1.4504e+00, -1.1431e+00, -2.1990e+00,  1.5803e+00,\n",
       "           1.0273e+00, -6.8816e-01, -5.2513e-01,  1.3747e+00, -5.8863e-01,\n",
       "           7.0065e-01,  1.6521e+00, -9.7309e-01,  4.6891e-01, -7.3507e-02,\n",
       "           3.3005e-01, -6.9332e-01, -3.4015e-03,  8.9057e-01, -3.6490e-01,\n",
       "           4.3810e-01,  3.7572e-01,  3.5635e-01,  1.7975e+00,  4.7466e-01,\n",
       "          -9.9087e-01, -3.3513e-01,  1.8447e-01,  5.3933e-01, -1.1106e+00,\n",
       "          -5.1192e-01,  5.3856e-01, -8.1792e-01, -8.1050e-01, -1.7244e+00,\n",
       "           1.2450e-01, -6.8499e-02, -8.0790e-01,  1.8825e-01, -1.2189e+00,\n",
       "          -8.8213e-01, -7.1251e-01, -1.1337e+00, -8.1277e-01, -6.9589e-01,\n",
       "           2.4630e+00,  5.5782e-01,  1.8712e-01,  5.3792e-01, -2.9650e-01,\n",
       "          -2.3645e+00, -4.9783e-01,  1.4530e-01,  6.8617e-01,  1.0438e+00,\n",
       "           1.1261e+00,  1.2425e+00,  2.3240e-01, -1.6979e+00, -9.5388e-01,\n",
       "          -1.1452e-01, -1.8342e-01,  1.4406e-01,  4.8950e-02, -1.5422e+00,\n",
       "          -2.2669e+00, -1.9210e+00, -2.9892e-02,  1.4153e+00, -2.1575e-01,\n",
       "           1.0059e+00,  1.0925e+00,  2.9017e+00,  9.7249e-01,  5.1827e-01,\n",
       "           5.9320e-01, -2.7121e-01,  9.0214e-01, -6.6611e-01, -1.2490e-01,\n",
       "          -1.0091e-01, -1.7284e+00,  1.1560e+00,  4.0994e-01,  9.2057e-01,\n",
       "           3.1300e-01,  1.6349e-01, -9.1878e-01,  4.3844e-01, -2.7733e-01,\n",
       "           9.0903e-01,  6.3661e-01, -1.5454e+00, -5.3887e-01, -5.1650e-01,\n",
       "           5.1611e-01, -8.7082e-01,  7.4860e-01, -5.8528e-01,  8.3027e-01,\n",
       "           3.9824e-01,  1.0921e+00,  2.2402e-02,  1.0215e+00, -1.5415e-01,\n",
       "           1.0857e-01, -1.5278e+00,  1.1890e+00, -6.2738e-01,  5.7314e-01,\n",
       "          -5.0830e-02,  9.2717e-01,  8.2660e-01, -4.7228e-01, -9.3790e-01,\n",
       "          -6.4655e-01,  3.8845e-02,  1.1623e+00,  1.1232e+00,  3.2202e-01,\n",
       "           6.2977e-01,  7.5168e-01, -3.5110e-01, -1.6355e+00,  1.5362e+00,\n",
       "          -2.5104e+00,  8.0431e-02,  2.9920e-01, -2.1683e-01,  4.9577e-02,\n",
       "          -1.4824e-01,  8.9585e-01,  6.9045e-01,  5.5144e-01, -1.0847e-01,\n",
       "           3.7632e-01, -5.5585e-03,  1.0913e-01, -7.0330e-01, -9.4721e-01,\n",
       "           7.9286e-01,  4.2405e-01,  5.7567e-01, -5.7280e-01, -3.8837e-01,\n",
       "          -6.6026e-01,  1.5018e-03,  3.8868e-01, -1.8708e-01, -3.7524e-01,\n",
       "          -1.1858e+00,  6.6751e-01, -5.6737e-01,  1.6255e+00, -4.6711e-01,\n",
       "           8.9408e-02, -1.0125e-01,  7.8042e-01,  2.5381e-01, -3.3741e+00,\n",
       "           1.9453e-01, -6.8634e-01,  5.8981e-01, -1.3422e+00, -2.3514e-01,\n",
       "          -4.9931e-03,  1.2845e+00,  5.9836e-02,  7.4731e-01,  1.4075e+00,\n",
       "          -1.7275e+00, -1.8305e+00, -1.5022e+00, -1.0995e+00, -5.9250e-01,\n",
       "          -9.5855e-02, -1.7813e-01,  1.6710e-01,  1.6285e+00, -8.9731e-01,\n",
       "           1.2799e+00, -1.2474e+00,  7.5706e-01, -2.0477e-01, -9.7909e-01,\n",
       "          -1.6941e+00,  5.5090e-01, -9.3921e-01,  7.0821e-03, -1.7588e+00,\n",
       "          -1.6222e+00,  2.5372e-01,  4.4373e-01,  4.7169e-01, -2.9220e-01,\n",
       "           2.0547e-01,  2.4736e-01, -1.3203e+00, -1.0003e+00, -2.0565e+00,\n",
       "          -1.5145e-02,  2.4110e-01, -1.6635e+00, -3.2739e-01, -1.2279e+00,\n",
       "           3.9480e-01,  1.1546e-01, -4.6163e-01,  8.5628e-01,  2.0434e-01,\n",
       "          -1.3025e+00,  9.9000e-02,  1.0041e+00, -2.4466e-02, -1.0346e-01,\n",
       "           6.6079e-01,  8.8355e-01,  9.1975e-01,  1.4071e-01, -1.0316e-01,\n",
       "          -3.6278e-01,  1.0215e+00, -1.0754e+00,  1.0055e-02,  2.3131e+00,\n",
       "           5.3347e-01,  4.8276e-01,  7.6990e-01, -6.0149e-02,  1.1067e-01,\n",
       "           5.4927e-01, -3.3628e-01, -4.5953e-01,  4.4902e-01, -8.8175e-01,\n",
       "          -1.9320e+00,  2.2126e-01,  1.2875e+00, -8.3943e-02, -2.3729e-01,\n",
       "           1.4111e+00, -1.0572e+00, -1.1997e+00, -2.0752e+00,  5.9471e-01,\n",
       "          -1.3227e-01, -6.1109e-01,  6.4236e-01, -5.5458e-01, -4.1323e-01,\n",
       "          -5.0688e-01,  1.8625e-01,  2.8894e-01,  1.5921e+00,  4.0930e-01,\n",
       "           6.4656e-01, -1.6657e-01,  1.8238e+00, -6.7967e-01, -6.6711e-01,\n",
       "           9.7859e-02, -2.3165e-01,  3.7120e-01, -5.5568e-01, -5.6793e-02,\n",
       "          -5.9256e-01,  1.3677e-01, -3.3791e-01,  6.5799e-02, -3.7054e-01,\n",
       "          -2.3110e+00,  1.1545e+00,  1.1176e+00,  4.7535e-01,  2.4243e-01,\n",
       "          -1.1632e+00,  3.1645e+00,  1.1132e+00,  8.1903e-01, -3.0075e-01,\n",
       "           3.8170e-01, -8.8250e-02,  2.5057e-01, -1.0245e+00, -8.7173e-01,\n",
       "           2.6359e-01, -9.5072e-01,  2.4428e+00,  1.0795e+00, -7.4722e-01,\n",
       "           9.2675e-01, -5.7119e-01, -6.1065e-01, -5.7658e-01,  2.3235e-01,\n",
       "          -3.4299e-01,  2.5346e-01,  2.1389e-01,  9.4487e-01,  9.0395e-02,\n",
       "          -3.6447e-02, -5.1921e-01,  1.1172e+00, -8.9591e-01,  2.0909e-01,\n",
       "           6.8460e-01, -4.8961e-01, -8.5686e-01,  2.2363e-01,  4.9895e-01,\n",
       "          -1.1776e-01, -6.8361e-01, -5.3811e-01,  1.6847e-01, -4.2878e-01,\n",
       "           4.2749e-01, -3.3006e-01, -7.7195e-01,  5.3293e-01,  3.3705e-01,\n",
       "          -8.0889e-01, -3.3785e-01, -2.2510e-02, -2.3237e-02,  3.0158e-01,\n",
       "           3.1137e-03,  2.0755e+00,  2.0717e-01,  6.2350e-01, -6.9085e-01,\n",
       "          -1.6788e+00,  4.3559e-01,  6.9840e-01,  1.6553e-01, -3.8562e-01,\n",
       "           1.5665e+00,  3.7226e-01, -4.0905e-01, -3.9662e-01,  6.2431e-01,\n",
       "          -4.5168e-01, -1.0480e+00, -1.5604e+00,  6.4949e-01,  8.2907e-01,\n",
       "           6.0026e-01,  3.8441e-01, -2.1837e-01, -1.1838e+00, -4.6421e-01,\n",
       "           9.5038e-01,  1.0966e+00,  5.3017e-01,  1.3934e+00,  5.3206e-02,\n",
       "          -2.0791e-02, -1.5693e+00,  4.9501e-01, -2.3549e-01,  9.4328e-01,\n",
       "           3.3831e-01,  1.1423e+00, -3.0871e-01, -5.4653e-01, -1.3302e+00,\n",
       "           6.2547e-01,  1.9757e-01,  5.1673e-01,  1.3001e-01, -1.4649e+00,\n",
       "           1.4412e-01,  1.0290e-02,  3.8108e-01,  8.4635e-01, -4.1363e-02,\n",
       "           7.5730e-01,  6.1371e-02,  3.0605e-02, -5.9437e-01,  8.7978e-01,\n",
       "          -1.3974e+00, -1.2127e+00,  1.2383e+00, -1.1366e+00, -5.8494e-01,\n",
       "          -8.0647e-01,  5.4814e-01,  1.7878e-01]]], device='cuda:0',\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unify_spans(o, first_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4.2591e-01,  7.1202e-02,  1.0741e+00, -5.9715e-01,  7.3422e-01,\n",
      "           8.5603e-01,  1.0841e+00, -2.2846e-01,  7.1181e-01, -1.3976e-01,\n",
      "           5.7261e-02, -2.8580e-01,  1.8605e-01,  2.5129e-01, -7.7597e-01,\n",
      "           5.7853e-01, -4.9510e-01,  1.2580e-01,  1.4575e-02,  8.6050e-01,\n",
      "           2.0907e-01, -2.0072e-01, -9.2557e-01,  4.1754e-01,  1.8603e-02,\n",
      "           8.5870e-01,  5.6372e-01,  5.2747e-01,  5.0323e-01,  4.8772e-01,\n",
      "           4.3810e-01,  1.6062e-01,  9.7859e-01,  7.7399e-02, -4.2207e-01,\n",
      "           3.6537e-01, -9.9465e-02,  7.3076e-01, -2.8857e-01, -5.9149e-01,\n",
      "           1.9069e-01, -1.3455e+00,  4.8455e-01, -8.1836e-02, -4.9453e-01,\n",
      "           2.5838e-01, -1.4900e-01, -4.1348e-01, -4.8747e-03, -1.7667e-01,\n",
      "          -1.3364e+00, -9.1852e-02,  2.3871e-01, -6.4757e-01,  1.0538e+00,\n",
      "           3.5170e-01, -1.1585e+00, -1.6642e+00, -2.1156e-01, -1.0385e-01,\n",
      "          -4.6887e-01,  7.8346e-01, -6.3073e-01, -8.2196e-01,  6.5401e-02,\n",
      "           9.8621e-03,  5.8867e-01,  9.9888e-01,  6.3805e-01, -9.3514e-01,\n",
      "           3.3200e-02, -1.0251e+00,  4.7446e-01, -7.6212e-01, -5.7938e-01,\n",
      "           3.6542e-01, -4.3916e-01, -6.5006e-02,  2.9967e-01,  1.0961e-01,\n",
      "           1.1369e-02,  1.2803e+00,  4.3730e-01,  8.5822e-02,  4.1250e-01,\n",
      "          -3.9359e-01,  2.8650e-02,  5.9118e-01, -2.2516e-01,  4.2512e-01,\n",
      "          -4.4763e-01, -6.6795e-01, -1.5676e-01, -1.5690e-01, -5.3498e-01,\n",
      "          -5.7457e-01,  3.3556e-01,  9.3528e-02,  6.7162e-01, -1.2842e+00,\n",
      "           9.7470e-01, -1.0724e+00, -2.0707e-01, -7.7371e-01, -2.9738e-01,\n",
      "          -1.5288e-01,  3.3048e-01, -1.3219e+00, -6.3804e-02, -3.5041e-02,\n",
      "           2.3578e-01, -1.1664e+00, -2.3239e-01, -1.2213e+00,  2.4074e-01,\n",
      "           1.2345e-01,  3.6891e-01, -1.2683e+00, -3.8013e-01,  9.5295e-02,\n",
      "           3.1748e-01,  2.7278e-01,  4.0416e-02,  9.9765e-01,  2.5599e-01,\n",
      "          -9.2193e-01,  1.0089e+00, -9.5075e-02, -2.4678e-01,  4.5737e-01,\n",
      "           6.0299e-01,  4.5312e-01, -6.8383e-01, -7.3032e-01, -1.2185e-01,\n",
      "           6.2215e-01, -4.7795e-01,  5.2144e-01, -2.1477e-01,  8.3046e-02,\n",
      "           9.7725e-01,  6.0172e-01,  2.2771e-01,  7.4083e-02,  9.3239e-02,\n",
      "           1.1763e-01,  7.4686e-01, -6.6284e-01, -8.6803e-01,  8.9760e-01,\n",
      "           1.3591e-01,  2.4926e-01, -2.6387e-01,  3.7769e-01,  1.4859e-01,\n",
      "           2.8713e-01,  4.3617e-01,  8.4012e-01, -1.1508e+00, -6.1136e-02,\n",
      "           8.3019e-01, -6.5263e-01,  2.1406e-01, -4.9968e-01,  4.3333e-01,\n",
      "          -5.8793e-01,  6.9799e-01,  6.5510e-02, -9.9591e-01,  8.7683e-01,\n",
      "          -2.2684e-01,  3.4532e-01,  4.7465e-01,  3.6212e-01,  1.1158e-01,\n",
      "          -4.4419e-01,  8.7589e-01,  3.4960e-01, -4.1789e-02, -1.3298e-01,\n",
      "          -5.2634e-01, -7.2866e-01,  2.2028e-02, -7.0320e-01,  1.9702e+00,\n",
      "          -1.0051e+00, -3.1620e-01, -5.4851e-01,  1.2384e-01,  3.2293e-02,\n",
      "          -1.5396e+00, -1.8742e+00,  6.3648e-01, -1.1181e-01,  3.1246e-01,\n",
      "          -1.1699e+00,  4.4529e-02,  4.0535e-01,  3.8770e-01,  4.4299e-01,\n",
      "          -4.0848e-01, -2.7378e-01, -1.6089e-01,  2.2328e-01, -4.1242e-01,\n",
      "          -2.9165e-01,  6.3166e-01, -4.3997e-01, -1.1804e-01,  6.2606e-01,\n",
      "          -2.3340e-01, -6.4580e-01, -2.6304e-01,  4.6845e-01, -2.1061e-03,\n",
      "           1.9136e-01,  2.7188e-01, -1.3308e+00,  6.2533e-01, -7.5089e-01,\n",
      "           7.2253e-02, -4.5393e-01,  3.0575e-01,  1.2485e-01, -3.4697e-01,\n",
      "           1.7496e+00,  1.0856e+00, -8.3562e-02,  1.0580e+00, -1.5761e-01,\n",
      "          -4.2135e-01,  9.9314e-02,  4.6868e-01, -6.8029e-01, -5.4420e-01,\n",
      "           3.3141e-01, -7.6211e-01, -1.5005e-01,  7.7077e-01, -9.3195e-01,\n",
      "           1.4476e-01,  9.8967e-01, -1.2830e-02,  8.7356e-02,  2.1988e-01,\n",
      "           6.8630e-01, -6.4412e-01, -1.7744e-01, -6.6289e-01, -1.1190e-01,\n",
      "          -6.5945e-01,  1.0519e+00,  2.1542e-01, -2.1604e-01, -1.6627e-01,\n",
      "          -5.2903e-01, -7.5958e-02,  1.5498e-01,  2.4560e-01,  4.5099e-01,\n",
      "           7.9606e-01,  1.4475e-01,  3.7593e-01,  2.0231e-01, -2.5936e-01,\n",
      "          -3.6435e-01,  3.3581e-02, -3.6120e-01,  5.0968e-01,  8.5618e-01,\n",
      "          -5.9567e-01, -3.1138e-01, -3.5644e-01,  1.2084e+00,  5.7181e-01,\n",
      "           8.5549e-01, -5.2924e-02, -3.8507e-01,  5.3147e-01, -8.2120e-01,\n",
      "          -1.4279e-01,  5.9329e-01, -4.9405e-01, -5.5226e-01, -2.9948e-01,\n",
      "           3.4599e-01,  2.9867e-01,  6.4332e-01, -9.1133e-01,  4.3203e-01,\n",
      "           2.0275e-01,  1.3367e+00, -6.3994e-01, -1.9358e-01,  5.5102e-01,\n",
      "           1.8146e-01,  3.4563e-01,  4.3801e-02,  1.7225e+00, -3.3314e-01,\n",
      "          -2.4346e-01,  4.0628e-01, -2.9157e-01,  7.5297e-01,  1.2218e+00,\n",
      "           5.9132e-02, -1.1067e+00,  7.9109e-03, -2.2451e+00,  9.7335e-02,\n",
      "          -8.3692e-02, -5.2999e-01,  2.0155e-01,  2.6365e-01,  4.7062e-01,\n",
      "          -2.7229e-01, -3.0643e-01,  4.6452e-01,  6.8615e-01,  9.3295e-03,\n",
      "          -2.2576e-02, -2.0213e-01,  7.7008e-01,  2.1839e-01, -2.0832e-01,\n",
      "          -6.0802e-01,  3.0494e-01,  1.0052e+00, -3.9436e-01, -8.6902e-01,\n",
      "           3.8778e-02,  4.0126e-02, -2.8249e-01, -2.1761e-01, -9.9112e-01,\n",
      "          -2.1027e-01, -2.1209e-01, -6.1676e-01, -9.2857e-02, -4.5985e-01,\n",
      "          -5.9157e-01,  1.8491e+00,  1.2575e-01,  3.6317e-01, -5.3234e-02,\n",
      "          -6.9767e-01, -5.4431e-02, -7.8013e-01,  3.2497e-01,  4.3629e-01,\n",
      "          -1.0949e+00,  7.9109e-01, -1.1312e-01, -1.8605e-01, -6.6376e-01,\n",
      "           5.4954e-01, -6.8405e-01,  8.2169e-01,  1.8942e-01,  5.4809e-01,\n",
      "          -2.8502e-01, -1.1950e+00, -1.4622e-01,  1.8054e-01,  4.9282e-01,\n",
      "           3.1536e-01, -1.1660e+00,  3.0327e-02, -4.8370e-01, -5.9682e-01,\n",
      "          -2.0708e-01, -6.3994e-04,  9.8362e-01, -3.6967e-01, -1.0398e+00,\n",
      "           5.8278e-01, -2.8242e-01, -1.6403e+00, -8.7048e-01, -1.5366e+00,\n",
      "           7.1178e-01, -1.0962e+00,  3.6484e-01,  1.6022e-02,  9.4459e-01,\n",
      "           5.7409e-01, -4.8368e-01, -1.6160e-01,  1.5779e-01,  5.3526e-01,\n",
      "           6.6862e-01,  1.7631e-01, -1.9308e-01, -3.9437e-02, -5.3464e-01,\n",
      "          -5.9979e-01, -8.6738e-01, -1.7309e-01, -8.3784e-02, -6.7187e-01,\n",
      "          -1.6373e-02,  4.7798e-01,  2.0001e-01,  5.8627e-01,  3.2411e-01,\n",
      "           4.2045e-01,  8.0065e-02, -2.1684e-01,  6.6537e-01,  1.1837e-01,\n",
      "           1.4422e-01,  4.5712e-01,  3.3381e-01,  2.4036e-01, -4.2424e-01,\n",
      "          -3.7299e-01,  9.7212e-02,  9.8475e-02,  2.6057e-02, -5.9843e-01,\n",
      "          -4.5019e-01,  5.9198e-02, -2.1772e-01, -6.2205e-01, -9.3259e-01,\n",
      "           3.0691e-01, -8.1015e-01,  1.0931e+00, -4.7371e-01,  1.1351e-01,\n",
      "          -2.1420e+00, -3.3134e-02,  1.0100e-01, -1.3145e-01,  2.6045e-01,\n",
      "           1.0720e-01,  5.4656e-01,  2.6178e-01, -3.9379e-01, -6.4305e-01,\n",
      "          -5.8002e-01,  1.8385e-01,  5.3409e-01,  9.4179e-01, -8.6784e-01,\n",
      "          -4.2329e-01,  6.3009e-01,  6.1893e-01, -4.8558e-01, -5.0304e-01,\n",
      "          -2.4126e-01,  1.0459e-01,  9.5171e-01,  5.4100e-01, -8.5157e-01,\n",
      "           6.8979e-01, -1.1692e-01,  2.3692e-01, -9.7703e-02, -9.0168e-02,\n",
      "           1.9866e-01, -9.7374e-01, -8.6343e-02,  4.9939e-01,  1.3093e-01,\n",
      "           2.8599e-01, -2.0384e-01, -2.3073e-01,  1.1528e-01,  1.5719e-01,\n",
      "           2.3897e-01, -3.8331e-01,  5.7537e-02,  9.8143e-01, -1.4664e-01,\n",
      "          -6.8747e-01,  2.2475e-01,  8.5290e-01, -7.5115e-01,  2.6106e-01,\n",
      "           1.9971e-01,  5.8141e-01, -2.5905e-01,  1.8836e-01,  1.0180e-01,\n",
      "          -1.8379e-01,  2.8414e-01,  1.2876e+00, -5.8491e-01, -3.5763e-01,\n",
      "           1.4586e-01,  4.4380e-01, -3.0069e-01,  7.0551e-01, -8.8334e-01,\n",
      "           1.1718e-01, -5.7833e-01,  5.6265e-01, -7.3614e-01,  7.7452e-01,\n",
      "           2.6183e-01,  5.5377e-01, -3.7961e-01, -1.1791e+00,  4.6544e-01,\n",
      "          -7.1184e-01, -1.8833e-02, -2.4415e-02, -7.6253e-02,  1.6494e-01,\n",
      "          -1.2125e-01,  2.8020e-01,  4.0901e-01, -3.4218e-01,  1.6591e-01,\n",
      "          -2.7447e-01,  4.8015e-02,  6.4576e-01, -4.8153e-01,  4.6964e-01,\n",
      "           1.4999e-01,  1.1866e+00,  8.1621e-01, -5.0547e-01,  2.8561e-01,\n",
      "          -5.5140e-01, -5.5168e-02,  8.1795e-03, -1.1561e+00, -3.1992e-01,\n",
      "           2.4635e-01, -7.0269e-02,  7.4280e-01,  8.8967e-01, -7.1847e-01,\n",
      "          -4.8297e-01,  5.6965e-01,  3.0757e-01, -2.4913e-01, -1.1492e+00,\n",
      "           5.3907e-01, -7.4077e-01, -1.5623e-01, -3.2005e-02, -3.6035e-01,\n",
      "          -1.2495e-01,  2.3705e-01,  3.9780e-01,  5.1824e-01,  9.2044e-01,\n",
      "          -6.1707e-02, -7.1605e-01, -4.1727e-01, -1.8021e-02,  1.2528e+00,\n",
      "          -5.8723e-01, -5.9966e-01,  5.2082e-01,  5.7798e-01, -7.6916e-01,\n",
      "           6.6898e-01, -6.4850e-01,  8.0359e-02, -4.6594e-01, -1.7483e-01,\n",
      "          -1.1982e+00, -5.4676e-01, -7.3047e-02,  3.6641e-01, -2.4710e-01,\n",
      "          -6.3700e-01,  1.3426e-01,  3.2797e-02,  9.8426e-01,  2.7812e-02,\n",
      "          -4.3986e-01,  2.1348e-01, -2.8459e-01, -1.1227e-01, -1.7442e+00,\n",
      "           2.1242e-01, -5.9608e-01,  1.3010e-01, -3.8769e-01, -5.4510e-01,\n",
      "          -2.4235e-01,  3.1953e-01, -8.9509e-01, -2.0072e-01,  1.1579e+00,\n",
      "          -4.6191e-01, -6.3315e-01,  5.5201e-01, -3.6803e-01, -5.8079e-02,\n",
      "           1.7682e-01,  1.8434e-01,  4.0296e-01,  2.9046e-01, -4.4318e-01,\n",
      "          -2.1424e-01,  1.4039e+00,  2.5740e-01,  1.9420e-01, -1.0427e-01,\n",
      "           2.1834e-01, -8.4066e-01,  3.4640e-01,  2.1866e-01,  3.6520e-01,\n",
      "           8.8507e-02,  3.4536e-01, -2.4403e-01,  5.8735e-01, -6.4338e-01,\n",
      "          -7.5582e-01,  5.5352e-01,  2.8277e-02,  3.0054e-01, -2.0650e-01,\n",
      "           1.0575e+00, -4.3188e-01, -9.9704e-01,  1.8655e-01, -3.0194e-01,\n",
      "          -8.4534e-01, -7.0400e-01,  9.2979e-01,  8.5990e-02,  4.5391e-01,\n",
      "          -2.4051e-01, -4.2580e-01,  2.1798e-01,  1.2633e+00,  2.4422e-01,\n",
      "           1.0130e+00,  7.1332e-01,  5.8223e-01, -3.0861e-01, -5.7962e-01,\n",
      "           4.3172e-01, -3.7529e-01,  3.9652e-01, -2.3810e-02,  3.9527e-01,\n",
      "          -6.3556e-01,  2.2139e-01, -2.2187e-01, -3.7748e-01,  2.5595e-01,\n",
      "          -1.3230e+00,  1.0224e+00,  8.4223e-01,  3.7879e-01, -4.0451e-01,\n",
      "          -2.1764e-01,  6.9905e-01,  1.8621e-01,  9.5295e-02,  7.5224e-02,\n",
      "          -3.7488e-01, -1.3536e+00, -1.6098e-01,  5.3159e-01, -9.9707e-02,\n",
      "          -5.2142e-01,  8.5154e-02,  8.7912e-01,  1.3890e+00, -8.3838e-01,\n",
      "           6.2775e-01,  8.7151e-02, -7.4821e-02, -4.8772e-01, -1.4898e+00,\n",
      "          -1.2754e+00, -3.1463e-01,  5.7709e-01,  8.6614e-01,  7.8355e-01,\n",
      "           7.3132e-01, -8.3877e-02,  6.5933e-01, -1.8124e-01,  1.0310e-01,\n",
      "          -4.7536e-01, -2.0465e-01, -2.0322e-01,  1.4876e-01, -4.8649e-01,\n",
      "          -4.8759e-01, -1.2782e+00, -5.5412e-01,  5.6173e-01, -1.1601e-01,\n",
      "          -2.4202e-01, -5.2108e-01, -3.7265e-01,  3.6801e-01, -8.1421e-02,\n",
      "          -9.9814e-01, -1.1249e+00,  3.0516e-02, -5.1770e-01, -2.7711e-01,\n",
      "           3.4017e-01,  1.2522e+00, -2.1953e-02,  6.2888e-01, -6.5339e-01,\n",
      "          -1.0445e+00,  3.6399e-01, -3.5978e-03, -1.5345e-01,  5.2512e-01,\n",
      "           3.2784e-01, -1.4262e-01, -1.7550e-01, -3.2013e-01, -3.8009e-01,\n",
      "           6.2463e-02, -6.2837e-01, -8.1689e-02, -7.6210e-02, -9.0654e-02,\n",
      "           6.8013e-01, -5.2053e-01,  1.8664e-01, -4.6574e-01, -3.1381e-01,\n",
      "           7.0864e-01, -2.6360e-01, -1.1127e+00,  1.5112e+00,  1.0296e+00,\n",
      "           5.6550e-01,  3.4786e-01, -1.0781e-01, -6.3695e-01,  1.2501e-01,\n",
      "           1.3525e-01,  5.2763e-01, -2.1399e-01, -1.2692e+00, -2.8984e-01,\n",
      "          -1.1321e-01, -6.1344e-01,  3.8581e-01, -9.5433e-02, -1.0852e+00,\n",
      "          -4.4703e-01,  4.5882e-01,  2.7845e-01,  1.5395e+00, -1.6144e-02,\n",
      "           1.8298e-01, -1.0372e+00,  3.8696e-02, -8.7337e-01,  1.6838e-01,\n",
      "          -5.2962e-01, -7.1611e-01,  8.8227e-01,  3.9336e-01, -7.2416e-01,\n",
      "          -2.6505e-01,  7.2971e-02,  7.7145e-01]]], device='cuda:0',\n",
      "       grad_fn=<GatherBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Single entity\n",
    "if second_ent[0] == second_ent[1]:\n",
    "    ent_offset = second_ent[0].view(-1, 1, 1)\n",
    "#     print(ent_offset)\n",
    "    ent_embedding = ent_offset.expand(-1,-1,768)\n",
    "    selected = torch.gather(o, 1, ent_embedding)\n",
    "    print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = o[:,42,:]\n",
    "s = o[:,43,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def sum_tensors(tensors: List[torch.Tensor]):\n",
    "    return torch.stack(tensors).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5381, 0.0461], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([ 0.8265, -0.6816], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fa = f[0][:2]\n",
    "sa = s[0][:2]\n",
    "print(fa)\n",
    "print(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([fa, sa]).sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1])\n",
    "b = torch.tensor([2])\n",
    "c = torch.tensor([3])\n",
    "@torch.jit.script\n",
    "def s(a,b):\n",
    "    return torch.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b,c]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "of = off.unsqueeze(2).expand(-1,-1,768)\n",
    "of.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.2591e-01,  7.1202e-02,  1.0741e+00, -5.9715e-01,  7.3422e-01,\n",
      "          8.5603e-01,  1.0841e+00, -2.2846e-01,  7.1181e-01, -1.3976e-01,\n",
      "          5.7261e-02, -2.8580e-01,  1.8605e-01,  2.5129e-01, -7.7597e-01,\n",
      "          5.7853e-01, -4.9510e-01,  1.2580e-01,  1.4575e-02,  8.6050e-01,\n",
      "          2.0907e-01, -2.0072e-01, -9.2557e-01,  4.1754e-01,  1.8603e-02,\n",
      "          8.5870e-01,  5.6372e-01,  5.2747e-01,  5.0323e-01,  4.8772e-01,\n",
      "          4.3810e-01,  1.6062e-01,  9.7859e-01,  7.7399e-02, -4.2207e-01,\n",
      "          3.6537e-01, -9.9465e-02,  7.3076e-01, -2.8857e-01, -5.9149e-01,\n",
      "          1.9069e-01, -1.3455e+00,  4.8455e-01, -8.1836e-02, -4.9453e-01,\n",
      "          2.5838e-01, -1.4900e-01, -4.1348e-01, -4.8747e-03, -1.7667e-01,\n",
      "         -1.3364e+00, -9.1852e-02,  2.3871e-01, -6.4757e-01,  1.0538e+00,\n",
      "          3.5170e-01, -1.1585e+00, -1.6642e+00, -2.1156e-01, -1.0385e-01,\n",
      "         -4.6887e-01,  7.8346e-01, -6.3073e-01, -8.2196e-01,  6.5401e-02,\n",
      "          9.8621e-03,  5.8867e-01,  9.9888e-01,  6.3805e-01, -9.3514e-01,\n",
      "          3.3200e-02, -1.0251e+00,  4.7446e-01, -7.6212e-01, -5.7938e-01,\n",
      "          3.6542e-01, -4.3916e-01, -6.5006e-02,  2.9967e-01,  1.0961e-01,\n",
      "          1.1369e-02,  1.2803e+00,  4.3730e-01,  8.5822e-02,  4.1250e-01,\n",
      "         -3.9359e-01,  2.8650e-02,  5.9118e-01, -2.2516e-01,  4.2512e-01,\n",
      "         -4.4763e-01, -6.6795e-01, -1.5676e-01, -1.5690e-01, -5.3498e-01,\n",
      "         -5.7457e-01,  3.3556e-01,  9.3528e-02,  6.7162e-01, -1.2842e+00,\n",
      "          9.7470e-01, -1.0724e+00, -2.0707e-01, -7.7371e-01, -2.9738e-01,\n",
      "         -1.5288e-01,  3.3048e-01, -1.3219e+00, -6.3804e-02, -3.5041e-02,\n",
      "          2.3578e-01, -1.1664e+00, -2.3239e-01, -1.2213e+00,  2.4074e-01,\n",
      "          1.2345e-01,  3.6891e-01, -1.2683e+00, -3.8013e-01,  9.5295e-02,\n",
      "          3.1748e-01,  2.7278e-01,  4.0416e-02,  9.9765e-01,  2.5599e-01,\n",
      "         -9.2193e-01,  1.0089e+00, -9.5075e-02, -2.4678e-01,  4.5737e-01,\n",
      "          6.0299e-01,  4.5312e-01, -6.8383e-01, -7.3032e-01, -1.2185e-01,\n",
      "          6.2215e-01, -4.7795e-01,  5.2144e-01, -2.1477e-01,  8.3046e-02,\n",
      "          9.7725e-01,  6.0172e-01,  2.2771e-01,  7.4083e-02,  9.3239e-02,\n",
      "          1.1763e-01,  7.4686e-01, -6.6284e-01, -8.6803e-01,  8.9760e-01,\n",
      "          1.3591e-01,  2.4926e-01, -2.6387e-01,  3.7769e-01,  1.4859e-01,\n",
      "          2.8713e-01,  4.3617e-01,  8.4012e-01, -1.1508e+00, -6.1136e-02,\n",
      "          8.3019e-01, -6.5263e-01,  2.1406e-01, -4.9968e-01,  4.3333e-01,\n",
      "         -5.8793e-01,  6.9799e-01,  6.5510e-02, -9.9591e-01,  8.7683e-01,\n",
      "         -2.2684e-01,  3.4532e-01,  4.7465e-01,  3.6212e-01,  1.1158e-01,\n",
      "         -4.4419e-01,  8.7589e-01,  3.4960e-01, -4.1789e-02, -1.3298e-01,\n",
      "         -5.2634e-01, -7.2866e-01,  2.2028e-02, -7.0320e-01,  1.9702e+00,\n",
      "         -1.0051e+00, -3.1620e-01, -5.4851e-01,  1.2384e-01,  3.2293e-02,\n",
      "         -1.5396e+00, -1.8742e+00,  6.3648e-01, -1.1181e-01,  3.1246e-01,\n",
      "         -1.1699e+00,  4.4529e-02,  4.0535e-01,  3.8770e-01,  4.4299e-01,\n",
      "         -4.0848e-01, -2.7378e-01, -1.6089e-01,  2.2328e-01, -4.1242e-01,\n",
      "         -2.9165e-01,  6.3166e-01, -4.3997e-01, -1.1804e-01,  6.2606e-01,\n",
      "         -2.3340e-01, -6.4580e-01, -2.6304e-01,  4.6845e-01, -2.1061e-03,\n",
      "          1.9136e-01,  2.7188e-01, -1.3308e+00,  6.2533e-01, -7.5089e-01,\n",
      "          7.2253e-02, -4.5393e-01,  3.0575e-01,  1.2485e-01, -3.4697e-01,\n",
      "          1.7496e+00,  1.0856e+00, -8.3562e-02,  1.0580e+00, -1.5761e-01,\n",
      "         -4.2135e-01,  9.9314e-02,  4.6868e-01, -6.8029e-01, -5.4420e-01,\n",
      "          3.3141e-01, -7.6211e-01, -1.5005e-01,  7.7077e-01, -9.3195e-01,\n",
      "          1.4476e-01,  9.8967e-01, -1.2830e-02,  8.7356e-02,  2.1988e-01,\n",
      "          6.8630e-01, -6.4412e-01, -1.7744e-01, -6.6289e-01, -1.1190e-01,\n",
      "         -6.5945e-01,  1.0519e+00,  2.1542e-01, -2.1604e-01, -1.6627e-01,\n",
      "         -5.2903e-01, -7.5958e-02,  1.5498e-01,  2.4560e-01,  4.5099e-01,\n",
      "          7.9606e-01,  1.4475e-01,  3.7593e-01,  2.0231e-01, -2.5936e-01,\n",
      "         -3.6435e-01,  3.3581e-02, -3.6120e-01,  5.0968e-01,  8.5618e-01,\n",
      "         -5.9567e-01, -3.1138e-01, -3.5644e-01,  1.2084e+00,  5.7181e-01,\n",
      "          8.5549e-01, -5.2924e-02, -3.8507e-01,  5.3147e-01, -8.2120e-01,\n",
      "         -1.4279e-01,  5.9329e-01, -4.9405e-01, -5.5226e-01, -2.9948e-01,\n",
      "          3.4599e-01,  2.9867e-01,  6.4332e-01, -9.1133e-01,  4.3203e-01,\n",
      "          2.0275e-01,  1.3367e+00, -6.3994e-01, -1.9358e-01,  5.5102e-01,\n",
      "          1.8146e-01,  3.4563e-01,  4.3801e-02,  1.7225e+00, -3.3314e-01,\n",
      "         -2.4346e-01,  4.0628e-01, -2.9157e-01,  7.5297e-01,  1.2218e+00,\n",
      "          5.9132e-02, -1.1067e+00,  7.9109e-03, -2.2451e+00,  9.7335e-02,\n",
      "         -8.3692e-02, -5.2999e-01,  2.0155e-01,  2.6365e-01,  4.7062e-01,\n",
      "         -2.7229e-01, -3.0643e-01,  4.6452e-01,  6.8615e-01,  9.3295e-03,\n",
      "         -2.2576e-02, -2.0213e-01,  7.7008e-01,  2.1839e-01, -2.0832e-01,\n",
      "         -6.0802e-01,  3.0494e-01,  1.0052e+00, -3.9436e-01, -8.6902e-01,\n",
      "          3.8778e-02,  4.0126e-02, -2.8249e-01, -2.1761e-01, -9.9112e-01,\n",
      "         -2.1027e-01, -2.1209e-01, -6.1676e-01, -9.2857e-02, -4.5985e-01,\n",
      "         -5.9157e-01,  1.8491e+00,  1.2575e-01,  3.6317e-01, -5.3234e-02,\n",
      "         -6.9767e-01, -5.4431e-02, -7.8013e-01,  3.2497e-01,  4.3629e-01,\n",
      "         -1.0949e+00,  7.9109e-01, -1.1312e-01, -1.8605e-01, -6.6376e-01,\n",
      "          5.4954e-01, -6.8405e-01,  8.2169e-01,  1.8942e-01,  5.4809e-01,\n",
      "         -2.8502e-01, -1.1950e+00, -1.4622e-01,  1.8054e-01,  4.9282e-01,\n",
      "          3.1536e-01, -1.1660e+00,  3.0327e-02, -4.8370e-01, -5.9682e-01,\n",
      "         -2.0708e-01, -6.3994e-04,  9.8362e-01, -3.6967e-01, -1.0398e+00,\n",
      "          5.8278e-01, -2.8242e-01, -1.6403e+00, -8.7048e-01, -1.5366e+00,\n",
      "          7.1178e-01, -1.0962e+00,  3.6484e-01,  1.6022e-02,  9.4459e-01,\n",
      "          5.7409e-01, -4.8368e-01, -1.6160e-01,  1.5779e-01,  5.3526e-01,\n",
      "          6.6862e-01,  1.7631e-01, -1.9308e-01, -3.9437e-02, -5.3464e-01,\n",
      "         -5.9979e-01, -8.6738e-01, -1.7309e-01, -8.3784e-02, -6.7187e-01,\n",
      "         -1.6373e-02,  4.7798e-01,  2.0001e-01,  5.8627e-01,  3.2411e-01,\n",
      "          4.2045e-01,  8.0065e-02, -2.1684e-01,  6.6537e-01,  1.1837e-01,\n",
      "          1.4422e-01,  4.5712e-01,  3.3381e-01,  2.4036e-01, -4.2424e-01,\n",
      "         -3.7299e-01,  9.7212e-02,  9.8475e-02,  2.6057e-02, -5.9843e-01,\n",
      "         -4.5019e-01,  5.9198e-02, -2.1772e-01, -6.2205e-01, -9.3259e-01,\n",
      "          3.0691e-01, -8.1015e-01,  1.0931e+00, -4.7371e-01,  1.1351e-01,\n",
      "         -2.1420e+00, -3.3134e-02,  1.0100e-01, -1.3145e-01,  2.6045e-01,\n",
      "          1.0720e-01,  5.4656e-01,  2.6178e-01, -3.9379e-01, -6.4305e-01,\n",
      "         -5.8002e-01,  1.8385e-01,  5.3409e-01,  9.4179e-01, -8.6784e-01,\n",
      "         -4.2329e-01,  6.3009e-01,  6.1893e-01, -4.8558e-01, -5.0304e-01,\n",
      "         -2.4126e-01,  1.0459e-01,  9.5171e-01,  5.4100e-01, -8.5157e-01,\n",
      "          6.8979e-01, -1.1692e-01,  2.3692e-01, -9.7703e-02, -9.0168e-02,\n",
      "          1.9866e-01, -9.7374e-01, -8.6343e-02,  4.9939e-01,  1.3093e-01,\n",
      "          2.8599e-01, -2.0384e-01, -2.3073e-01,  1.1528e-01,  1.5719e-01,\n",
      "          2.3897e-01, -3.8331e-01,  5.7537e-02,  9.8143e-01, -1.4664e-01,\n",
      "         -6.8747e-01,  2.2475e-01,  8.5290e-01, -7.5115e-01,  2.6106e-01,\n",
      "          1.9971e-01,  5.8141e-01, -2.5905e-01,  1.8836e-01,  1.0180e-01,\n",
      "         -1.8379e-01,  2.8414e-01,  1.2876e+00, -5.8491e-01, -3.5763e-01,\n",
      "          1.4586e-01,  4.4380e-01, -3.0069e-01,  7.0551e-01, -8.8334e-01,\n",
      "          1.1718e-01, -5.7833e-01,  5.6265e-01, -7.3614e-01,  7.7452e-01,\n",
      "          2.6183e-01,  5.5377e-01, -3.7961e-01, -1.1791e+00,  4.6544e-01,\n",
      "         -7.1184e-01, -1.8833e-02, -2.4415e-02, -7.6253e-02,  1.6494e-01,\n",
      "         -1.2125e-01,  2.8020e-01,  4.0901e-01, -3.4218e-01,  1.6591e-01,\n",
      "         -2.7447e-01,  4.8015e-02,  6.4576e-01, -4.8153e-01,  4.6964e-01,\n",
      "          1.4999e-01,  1.1866e+00,  8.1621e-01, -5.0547e-01,  2.8561e-01,\n",
      "         -5.5140e-01, -5.5168e-02,  8.1795e-03, -1.1561e+00, -3.1992e-01,\n",
      "          2.4635e-01, -7.0269e-02,  7.4280e-01,  8.8967e-01, -7.1847e-01,\n",
      "         -4.8297e-01,  5.6965e-01,  3.0757e-01, -2.4913e-01, -1.1492e+00,\n",
      "          5.3907e-01, -7.4077e-01, -1.5623e-01, -3.2005e-02, -3.6035e-01,\n",
      "         -1.2495e-01,  2.3705e-01,  3.9780e-01,  5.1824e-01,  9.2044e-01,\n",
      "         -6.1707e-02, -7.1605e-01, -4.1727e-01, -1.8021e-02,  1.2528e+00,\n",
      "         -5.8723e-01, -5.9966e-01,  5.2082e-01,  5.7798e-01, -7.6916e-01,\n",
      "          6.6898e-01, -6.4850e-01,  8.0359e-02, -4.6594e-01, -1.7483e-01,\n",
      "         -1.1982e+00, -5.4676e-01, -7.3047e-02,  3.6641e-01, -2.4710e-01,\n",
      "         -6.3700e-01,  1.3426e-01,  3.2797e-02,  9.8426e-01,  2.7812e-02,\n",
      "         -4.3986e-01,  2.1348e-01, -2.8459e-01, -1.1227e-01, -1.7442e+00,\n",
      "          2.1242e-01, -5.9608e-01,  1.3010e-01, -3.8769e-01, -5.4510e-01,\n",
      "         -2.4235e-01,  3.1953e-01, -8.9509e-01, -2.0072e-01,  1.1579e+00,\n",
      "         -4.6191e-01, -6.3315e-01,  5.5201e-01, -3.6803e-01, -5.8079e-02,\n",
      "          1.7682e-01,  1.8434e-01,  4.0296e-01,  2.9046e-01, -4.4318e-01,\n",
      "         -2.1424e-01,  1.4039e+00,  2.5740e-01,  1.9420e-01, -1.0427e-01,\n",
      "          2.1834e-01, -8.4066e-01,  3.4640e-01,  2.1866e-01,  3.6520e-01,\n",
      "          8.8507e-02,  3.4536e-01, -2.4403e-01,  5.8735e-01, -6.4338e-01,\n",
      "         -7.5582e-01,  5.5352e-01,  2.8277e-02,  3.0054e-01, -2.0650e-01,\n",
      "          1.0575e+00, -4.3188e-01, -9.9704e-01,  1.8655e-01, -3.0194e-01,\n",
      "         -8.4534e-01, -7.0400e-01,  9.2979e-01,  8.5990e-02,  4.5391e-01,\n",
      "         -2.4051e-01, -4.2580e-01,  2.1798e-01,  1.2633e+00,  2.4422e-01,\n",
      "          1.0130e+00,  7.1332e-01,  5.8223e-01, -3.0861e-01, -5.7962e-01,\n",
      "          4.3172e-01, -3.7529e-01,  3.9652e-01, -2.3810e-02,  3.9527e-01,\n",
      "         -6.3556e-01,  2.2139e-01, -2.2187e-01, -3.7748e-01,  2.5595e-01,\n",
      "         -1.3230e+00,  1.0224e+00,  8.4223e-01,  3.7879e-01, -4.0451e-01,\n",
      "         -2.1764e-01,  6.9905e-01,  1.8621e-01,  9.5295e-02,  7.5224e-02,\n",
      "         -3.7488e-01, -1.3536e+00, -1.6098e-01,  5.3159e-01, -9.9707e-02,\n",
      "         -5.2142e-01,  8.5154e-02,  8.7912e-01,  1.3890e+00, -8.3838e-01,\n",
      "          6.2775e-01,  8.7151e-02, -7.4821e-02, -4.8772e-01, -1.4898e+00,\n",
      "         -1.2754e+00, -3.1463e-01,  5.7709e-01,  8.6614e-01,  7.8355e-01,\n",
      "          7.3132e-01, -8.3877e-02,  6.5933e-01, -1.8124e-01,  1.0310e-01,\n",
      "         -4.7536e-01, -2.0465e-01, -2.0322e-01,  1.4876e-01, -4.8649e-01,\n",
      "         -4.8759e-01, -1.2782e+00, -5.5412e-01,  5.6173e-01, -1.1601e-01,\n",
      "         -2.4202e-01, -5.2108e-01, -3.7265e-01,  3.6801e-01, -8.1421e-02,\n",
      "         -9.9814e-01, -1.1249e+00,  3.0516e-02, -5.1770e-01, -2.7711e-01,\n",
      "          3.4017e-01,  1.2522e+00, -2.1953e-02,  6.2888e-01, -6.5339e-01,\n",
      "         -1.0445e+00,  3.6399e-01, -3.5978e-03, -1.5345e-01,  5.2512e-01,\n",
      "          3.2784e-01, -1.4262e-01, -1.7550e-01, -3.2013e-01, -3.8009e-01,\n",
      "          6.2463e-02, -6.2837e-01, -8.1689e-02, -7.6210e-02, -9.0654e-02,\n",
      "          6.8013e-01, -5.2053e-01,  1.8664e-01, -4.6574e-01, -3.1381e-01,\n",
      "          7.0864e-01, -2.6360e-01, -1.1127e+00,  1.5112e+00,  1.0296e+00,\n",
      "          5.6550e-01,  3.4786e-01, -1.0781e-01, -6.3695e-01,  1.2501e-01,\n",
      "          1.3525e-01,  5.2763e-01, -2.1399e-01, -1.2692e+00, -2.8984e-01,\n",
      "         -1.1321e-01, -6.1344e-01,  3.8581e-01, -9.5433e-02, -1.0852e+00,\n",
      "         -4.4703e-01,  4.5882e-01,  2.7845e-01,  1.5395e+00, -1.6144e-02,\n",
      "          1.8298e-01, -1.0372e+00,  3.8696e-02, -8.7337e-01,  1.6838e-01,\n",
      "         -5.2962e-01, -7.1611e-01,  8.8227e-01,  3.9336e-01, -7.2416e-01,\n",
      "         -2.6505e-01,  7.2971e-02,  7.7145e-01]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(o[:,42,:])\n",
    "print(o[:,45,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5381,  0.0461,  0.5133,  ..., -0.4763,  0.2441,  0.4812],\n",
       "         [ 0.8265, -0.6816,  0.1423,  ..., -0.3302,  0.3041, -0.3024],\n",
       "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
       "         [ 0.4259,  0.0712,  1.0741,  ..., -0.2650,  0.0730,  0.7715],\n",
       "         [-0.6122, -1.0878, -0.1084,  ...,  0.2920,  0.7835,  0.1617]]],\n",
       "       device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select from output only the embeddings associated with the positions of the offsets\n",
    "# When the starting and ending offsets are the same we will have a duplicate embedding\n",
    "torch.gather(o, 1, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7],\n",
      "        [5, 7]])\n"
     ]
    }
   ],
   "source": [
    "ten = torch.tensor([[5, 7], [1, 3]])\n",
    "a = torch.gather(ten, 0, torch.tensor([[0, 0], [0, 0]]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5872\\922657592.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconcat_bert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mconcat_bert\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "concat_bert = torch.cat((output[-1],output[-2]) ,dim=-1)\n",
    "concat_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = []\n",
    "# for features, offsets, labels in train_dataloader:\n",
    "#     embeddings.append(model(features, offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 11199, 10093,  ...,     0,     0,     0],\n",
       "        [  101,  2002,  3473,  ...,     0,     0,     0],\n",
       "        [  101,  2002,  2018,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2002,  2001,  ...,     0,     0,     0],\n",
       "        [  101,  2798,  8480,  ...,     0,     0,     0],\n",
       "        [  101,  2009, 14964,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = collate_batch(train_ds)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model(train[0], train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.bert.encoder.layer[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bert.trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# gc.get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11199, 10093,  3877,  1011,  1011,  2209,  1996,  2610,  2961,\n",
       "          6513,  1997,  4079,  1010,  8538,  1012, 14019,  2011,  4079,  1999,\n",
       "          1996,  2345,  2792,  1997,  2186,  1015,  1010,  2044,  2002,  7771,\n",
       "          2007,  8437,  1010,  1998,  2003,  2025,  2464,  2153,  1012, 18188,\n",
       "          2726,  2209, 19431, 13737,  1010, 15595,  1005,  1055,  2767,  1998,\n",
       "          2036,  1037,  2095,  2340, 11136,  1999,  4079,  1005,  1055,  2465,\n",
       "          1012, 14019,  2014,  6898,  2206,  4079,  1005,  1055,  6040,  2044,\n",
       "          2002,  2876,  1005,  1056,  2031,  3348,  2007,  2014,  2021,  2101,\n",
       "         11323,  2023,  2001,  2349,  2000,  2032,  9105, 26076,  2125,  2014,\n",
       "          2767, 15595,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(s, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf37a2529baf03c803266b8d55d553bda8f92a60c47b8d9f7bdcfc02cbd55fef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
