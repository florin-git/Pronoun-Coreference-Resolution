{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wIOz40JYJddq",
   "metadata": {
    "id": "wIOz40JYJddq"
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3244dc15",
   "metadata": {
    "executionInfo": {
     "elapsed": 5489,
     "status": "ok",
     "timestamp": 1660751770217,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "3244dc15"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    BertModel\n",
    ")\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from typing import *\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Display the entire text\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58276aa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1660751781007,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "58276aa2",
    "outputId": "150bbcf1-dbcd-480d-9e85-82e77346e83f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "model_name_or_path = \"bert-base-cased\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kYGIp3MoJug2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15143,
     "status": "ok",
     "timestamp": 1660751806072,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "kYGIp3MoJug2",
    "outputId": "aad7d923-b163-4609-93a9-a07790cbfba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "QFqvmoaqhbyn",
   "metadata": {
    "executionInfo": {
     "elapsed": 1526,
     "status": "ok",
     "timestamp": 1660751810222,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "QFqvmoaqhbyn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_location = \"/content/drive/MyDrive/Colab Notebooks/Deep Learning/NLP/nlp2022-hw3/hw3/stud/\"\n",
    "os.chdir(curr_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fM1EDyRLifxE",
   "metadata": {
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1660751818200,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "fM1EDyRLifxE"
   },
   "outputs": [],
   "source": [
    "from arguments import CustomTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3633ef51",
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1660751819788,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "3633ef51"
   },
   "outputs": [],
   "source": [
    "train_clean_path = \"../../model/data/train_clean.tsv\"\n",
    "valid_clean_path = \"../../model/data/valid_clean.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "B1CvSmUrKc1y",
   "metadata": {
    "executionInfo": {
     "elapsed": 1177,
     "status": "ok",
     "timestamp": 1660751822725,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "B1CvSmUrKc1y"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(filepath_or_buffer=train_clean_path, sep=\"\\t\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer=valid_clean_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677693f1",
   "metadata": {},
   "source": [
    "Is a token classification: for each token I need to say if it is ambiguous or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ccf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I should introduce a mention tag for the pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5ec9c1",
   "metadata": {
    "id": "1f5ec9c1"
   },
   "outputs": [],
   "source": [
    "FEMININE = 0\n",
    "MASCULINE = 1\n",
    "UNKNOWN = 2\n",
    "\n",
    "def get_gender(pronoun: str):\n",
    "    gender_mapping = {\n",
    "        'she': FEMININE,\n",
    "        'her': FEMININE,\n",
    "        'he': MASCULINE,\n",
    "        'his': MASCULINE,\n",
    "        'him': MASCULINE,\n",
    "    }\n",
    "\n",
    "    return gender_mapping.get(pronoun.lower(), UNKNOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df00da9",
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1660751829631,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "4df00da9"
   },
   "outputs": [],
   "source": [
    "# class GAPAmbiguousDataset(Dataset):\n",
    "#     \"\"\"Custom GAP Dataset class\"\"\"\n",
    "\n",
    "#     def __init__(self, df, tokenizer, labeled=True):\n",
    "#         self.df = df\n",
    "\n",
    "#         self.labeled = labeled\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.tokens = []\n",
    "#         self.pron_offsets = []\n",
    "# #         self.original_offsets = []\n",
    "\n",
    "#         self._convert_tokens_to_ids()\n",
    "\n",
    "#         if labeled:\n",
    "#             self.labels = []\n",
    "#             self._assign_class_to_tokens()\n",
    "        \n",
    "#         assert len(self.tokens) == len(self.labels)\n",
    "\n",
    "        \n",
    "#     def _assign_class_to_tokens(self):        \n",
    "#         labels = []\n",
    "#         for idx, sentence in enumerate(self.tokens):\n",
    "#             for token_idx in range(len(sentence)):\n",
    "# #                 if token_id == sentence[self.pron_offsets[idx]]:\n",
    "#                 if token_idx == self.pron_offsets[idx]:\n",
    "#                     labels.append(2)\n",
    "                    \n",
    "#                 else:\n",
    "#                     labels.append(1)\n",
    "                    \n",
    "#             self.labels.append(labels)\n",
    "#             labels = []    \n",
    "        \n",
    "#     def _convert_tokens_to_ids(self):\n",
    "#         CLS = [self.tokenizer.cls_token]\n",
    "#         SEP = [self.tokenizer.sep_token]\n",
    "\n",
    "#         for _, row in self.df.iterrows():\n",
    "#             tokens, ambiguous_pron_offset, pronouns_offset = self._tokenize(row)\n",
    "#             self.tokens.append(self.tokenizer.convert_tokens_to_ids(\n",
    "#                 CLS + tokens + SEP))\n",
    "            \n",
    "#             # Because of the introduction of CLS we have to add 1\n",
    "#             self.ambiguous_pron_offsets.append(pron_offset+1)\n",
    "            \n",
    "   \n",
    "\n",
    "#     def _insert_tag(self, text, pronoun_offset):\n",
    "#         \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n",
    "#         to_be_inserted = sorted([\n",
    "#         #         (a_offset, \" [A] \"),\n",
    "#         #         (b_offset, \" [B] \"),\n",
    "#             (pronoun_offset, \" <P> \")\n",
    "#         ], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "#         for offset, tag in to_be_inserted:\n",
    "#             text = text[:offset] + tag + text[offset:]\n",
    "#         return text\n",
    "\n",
    "#     def _tokenize(self, row):\n",
    "#         \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n",
    "#         entries = {}\n",
    "#         final_tokens = []\n",
    "\n",
    "#         text = self._insert_tag(row['text'], row['p_offset'])\n",
    "#         for token in tokenizer.tokenize(text):\n",
    "#             if token == (\"<P>\"):\n",
    "#                 entries[token] = len(final_tokens)\n",
    "#                 continue\n",
    "            \n",
    "#             final_tokens.append(token)\n",
    "#         return final_tokens, entries[\"<P>\"]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokens)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.labeled:\n",
    "#             assert len(self.tokens[idx]) == len(self.labels[idx])\n",
    "#             return self.tokens[idx], self.labels[idx] \n",
    "#         return self.tokens[idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175a7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAP_AmbiguousDetection_Dataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.df = df\n",
    "\n",
    "        self.labeled = labeled\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = []\n",
    "        self.ambiguous_pron_offsets = []\n",
    "        self.pronouns_offsets = []\n",
    "#         self.original_offsets = []\n",
    "\n",
    "        self._convert_tokens_to_ids()\n",
    "\n",
    "        if labeled:\n",
    "            self.labels = []\n",
    "            self._assign_class_to_tokens()\n",
    "        \n",
    "        assert len(self.tokens) == len(self.labels)\n",
    "\n",
    "        \n",
    "    def _assign_class_to_tokens(self):        \n",
    "        \n",
    "        for sentence_idx, offsets_list in enumerate(self.pronouns_offsets):\n",
    "            labels = []\n",
    "            for offset in offsets_list:\n",
    "                if offset == self.ambiguous_pron_offsets[sentence_idx]:\n",
    "                    labels.append(2)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "                    \n",
    "            self.labels.append(labels)   \n",
    "        \n",
    "    def _convert_tokens_to_ids(self):\n",
    "        CLS = [self.tokenizer.cls_token]\n",
    "        SEP = [self.tokenizer.sep_token]\n",
    "\n",
    "        for _, row in self.df.iterrows():\n",
    "            tokens, ambiguous_pron_offset, pronouns_offset = self._tokenize(row)\n",
    "            self.tokens.append(self.tokenizer.convert_tokens_to_ids(\n",
    "                CLS + tokens + SEP))\n",
    "            \n",
    "            # Because of the introduction of CLS we have to add 1\n",
    "            self.ambiguous_pron_offsets.append(ambiguous_pron_offset+1)\n",
    "            \n",
    "            self.pronouns_offsets.append(pronouns_offset)\n",
    "            \n",
    "   \n",
    "\n",
    "    def _insert_tag(self, text, pronoun_offset):\n",
    "        \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n",
    "        to_be_inserted = sorted([\n",
    "        #         (a_offset, \" [A] \"),\n",
    "        #         (b_offset, \" [B] \"),\n",
    "            (pronoun_offset, \" <P> \")\n",
    "        ], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for offset, tag in to_be_inserted:\n",
    "            text = text[:offset] + tag + text[offset:]\n",
    "        return text\n",
    "\n",
    "    def _tokenize(self, row):\n",
    "        \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n",
    "        entries = {}\n",
    "        final_tokens = []\n",
    "        pronouns_offsets = []\n",
    "        pronoun_list = ['he','she','him','her','his','hers']\n",
    "        \n",
    "        text = self._insert_tag(row['text'], row['p_offset'])\n",
    "        for token in self.tokenizer.tokenize(text):\n",
    "            if token == (\"<P>\"):\n",
    "                entries[token] = len(final_tokens)\n",
    "                continue\n",
    "                \n",
    "            if token.lower() in pronoun_list:\n",
    "                pronouns_offsets.append(len(final_tokens)+1)\n",
    "            \n",
    "            final_tokens.append(token)\n",
    "        return final_tokens, entries[\"<P>\"], pronouns_offsets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            assert len(self.pronouns_offsets[idx]) == len(self.labels[idx])\n",
    "            return self.tokens[idx], self.pronouns_offsets[idx], self.labels[idx]\n",
    "        return self.tokens[idx], self.pronouns_offsets[idx], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4ed717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_len(sentences, truncate_len) -> int:\n",
    "    # calculate the max sentence length in the dataset\n",
    "    max_len = min(\n",
    "        max((len(x) for x in sentences)),\n",
    "        truncate_len\n",
    "    )\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff66fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(list_sequences: List[List[Any]], max_len: int, pad: int) -> List[Any]:\n",
    "    \n",
    "    features = np.full((len(list_sequences), max_len), pad, dtype=np.int64)\n",
    "\n",
    "    # Padding\n",
    "    for i, row in enumerate(list_sequences):\n",
    "        features[i, :len(row)] = row\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72067558",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1660751846339,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "72067558"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch, truncate_len=400):\n",
    "    \"\"\"Batch preparation.\n",
    "\n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"\n",
    "    batch_features, batch_pronouns_offsets, batch_labels = zip(*batch)\n",
    "\n",
    "    max_len_features_in_batch = compute_max_len(batch_features, truncate_len)\n",
    "    max_len_offsets_in_batch = compute_max_len(batch_pronouns_offsets, truncate_len)\n",
    "    \n",
    "    # Features        \n",
    "    padded_features = pad_sequence(batch_features, max_len_features_in_batch, 0)\n",
    "    features_tensor = torch.tensor(padded_features, device=device)\n",
    "    \n",
    "    # Offsets\n",
    "    padded_pronouns_offsets = pad_sequence(batch_pronouns_offsets, max_len_offsets_in_batch, 0)\n",
    "    pronouns_offsets_tensor = torch.tensor(padded_pronouns_offsets, device=device)\n",
    "    \n",
    "    # Labels\n",
    "    if batch_labels[0] is None:\n",
    "        return features_tensor, pronouns_offsets_tensor, None\n",
    "    \n",
    "    padded_labels = pad_sequence(batch_labels, max_len_offsets_in_batch, 0)\n",
    "    labels_tensor = torch.tensor(padded_labels, dtype=torch.uint8, device=device)\n",
    "    \n",
    "    return features_tensor, pronouns_offsets_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a3c2791",
   "metadata": {
    "id": "9a3c2791"
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, batch, truncate_len=400):\n",
    "        \"\"\"Batch preparation.\n",
    "\n",
    "        1. Pad the sequences\n",
    "        2. Transform the target.\n",
    "        \"\"\"\n",
    "        batch_features, batch_pronouns_offsets, batch_labels = zip(*batch)\n",
    "\n",
    "        max_len_features_in_batch = self.compute_max_len(batch_features, truncate_len)\n",
    "        max_len_offsets_in_batch = self.compute_max_len(batch_pronouns_offsets, truncate_len)\n",
    "\n",
    "        # Features        \n",
    "        padded_features = self.pad_sequence(batch_features, max_len_features_in_batch, 0)\n",
    "        features_tensor = torch.tensor(padded_features, device=device)\n",
    "\n",
    "        # Offsets\n",
    "        padded_pronouns_offsets = self.pad_sequence(batch_pronouns_offsets, max_len_offsets_in_batch, 0)\n",
    "        pronouns_offsets_tensor = torch.tensor(padded_pronouns_offsets, device=device)\n",
    "\n",
    "        # Labels\n",
    "        if batch_labels[0] is None:\n",
    "            return features_tensor, pronouns_offsets_tensor, None\n",
    "\n",
    "        padded_labels = self.pad_sequence(batch_labels, max_len_offsets_in_batch, 0)\n",
    "        labels_tensor = torch.tensor(padded_labels, dtype=torch.uint8, device=device)\n",
    "\n",
    "        return features_tensor, pronouns_offsets_tensor, labels_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_max_len(sentences, truncate_len) -> int:\n",
    "        # calculate the max sentence length in the dataset\n",
    "        max_len = min(\n",
    "            max((len(x) for x in sentences)),\n",
    "            truncate_len\n",
    "        )\n",
    "        return max_len\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_sequence(list_sequences: List[List[Any]], max_len: int, pad: int) -> List[Any]:\n",
    "    \n",
    "        features = np.full((len(list_sequences), max_len), pad, dtype=np.int64)\n",
    "\n",
    "        # Padding\n",
    "        for i, row in enumerate(list_sequences):\n",
    "            features[i, :len(row)] = row\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6844631",
   "metadata": {
    "id": "e6844631"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"../../model/tokenizer/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7997cbe1",
   "metadata": {
    "executionInfo": {
     "elapsed": 5692,
     "status": "ok",
     "timestamp": 1660751846338,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "7997cbe1"
   },
   "outputs": [],
   "source": [
    "train_ds = GAP_AmbiguousDetection_Dataset(df_train[:100], tokenizer)\n",
    "valid_ds = GAP_AmbiguousDetection_Dataset(df_valid[:50], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea4a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73b4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886b42fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "collator = Collator(device)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                              collate_fn=collator, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7f92b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "\n",
    "bert = BertModel.from_pretrained(bert_model_name).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ade4af05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = GAPModel(\"bert-base-cased\").to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yfvwpncpkmtU",
   "metadata": {
    "id": "yfvwpncpkmtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 11929, 11341,  2821,   118,   118,  1307,  1103,  2021,  2575,\n",
      "          6124,  1104,  3274,   117,  8153,   119, 12786,  8223,  1174,  1118,\n",
      "          3274,  1107,  1103,  1509,  2004,  1104,  1326,   122,   117,  1170,\n",
      "          1119,  7362,  1114,  8067,   117,  1105,  1110,  1136,  1562,  1254,\n",
      "           119, 19704,  1819,  1307, 21173, 14190,   117, 16473,   112,   188,\n",
      "          1910,  1105,  1145,   170,  1214,  1429, 11602,  1107,  3274,   112,\n",
      "           188,  1705,   119, 12786,  8223,  1174,  1123,  6508,  1378,  3274,\n",
      "           112,   188,  5566,  1170,  1119,  2010,   112,   189,  1138,  2673,\n",
      "          1114,  1123,  1133,  1224, 11326,  1142,  1108,  1496,  1106,  1140,\n",
      "          9256, 24121,  1116,  1228,  1123,  1910, 16473,   119,   102],\n",
      "        [  101,  1124,  2580,  1146,  1107,  5826,  1633,   117,  3461,  1103,\n",
      "          1248,  3778,  1104,  1421,  1482,  1259,  1117,  3330,   117,  5291,\n",
      "          1105,  4345,  1105,  5919,   117,  9751,  2176,   113,   153,  8043,\n",
      "          5005,   114,  1105, 15407,   119,  1230,  1344,  1278,  1552,  1127,\n",
      "          2097,  1120,  1203, 18491,  1200,  1693,  1323,  1107, 16387,  6097,\n",
      "          1968,   117,  3461,   119, 28116,  2376,  1114,  6190, 12958,  1732,\n",
      "          1121,  3224,  1106,  3130,   119,  1230,  3014,   117,  4829,   118,\n",
      "          6358,  8458, 12993,  1110,  3777,  4401,  1118,  1103,  1137,  9080,\n",
      "          1348, 13492,  1104,   156,  5114,  3454, 13030,  7971,  1105, 14812,\n",
      "         21440,  2180, 14812, 20264,   119,   102,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "for features, pronouns_offsets, labels in train_dataloader:\n",
    "    print(features)\n",
    "    output = bert(features, attention_mask=(features > 0).long(), \n",
    "                  token_type_ids=None, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "#     res = head(output.hidden_states[-1], offsets)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1059710",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, pronouns_offsets, labels in train_dataloader:\n",
    "    output_att = bert(features, output_attentions=True)\n",
    "    \n",
    "#     res = head(output.hidden_states[-1], offsets)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29d144c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 99, 3072])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([output.hidden_states[x] for x in [-1, -2, -3, -4]], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0b774fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 99, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_sum = torch.stack([output.hidden_states[x] for x in [-1, -2, -3, -4]], dim=0)\n",
    "to_sum.shape\n",
    "\n",
    "torch.sum(to_sum, dim =0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e90baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"The cat sat on the mat\", return_tensors='pt')\n",
    "# inputs = torch.tensor([train_ds[0][0]], device=device, dtype=torch.int64)\n",
    "outputs = bert(inputs, output_attentions=True)\n",
    "attention = outputs[-1]  # Output includes attention weights when output_attentions=True\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "097d7a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a818999748>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(attention[-1][0][-1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8256b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import model_view\n",
    "# model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f1ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bertviz import head_view\n",
    "# head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8c7c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1da63ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 99, 99])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_att[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a2d1e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[4.2342e-01, 1.0329e-02, 2.1425e-02,  ..., 2.6812e-03,\n",
       "            9.6221e-03, 9.5543e-02],\n",
       "           [1.5462e-03, 2.1176e-03, 5.7858e-03,  ..., 4.3158e-03,\n",
       "            3.9451e-03, 6.8317e-04],\n",
       "           [3.6258e-03, 1.5284e-02, 1.5809e-02,  ..., 1.4516e-02,\n",
       "            6.4294e-03, 1.0809e-03],\n",
       "           ...,\n",
       "           [1.0016e-02, 3.9249e-03, 1.4834e-02,  ..., 5.2647e-03,\n",
       "            3.4583e-03, 1.1564e-03],\n",
       "           [1.7533e-02, 3.1164e-03, 5.0359e-03,  ..., 5.9237e-03,\n",
       "            5.3873e-03, 3.1608e-02],\n",
       "           [3.1473e-01, 8.9481e-04, 6.2489e-03,  ..., 1.3714e-04,\n",
       "            4.2550e-04, 5.8916e-01]],\n",
       " \n",
       "          [[2.1291e-01, 1.4604e-02, 4.2316e-02,  ..., 5.4483e-03,\n",
       "            8.7653e-03, 6.9382e-03],\n",
       "           [6.3862e-01, 2.5806e-02, 8.0980e-02,  ..., 1.9607e-04,\n",
       "            5.1085e-05, 2.0034e-05],\n",
       "           [1.8794e-01, 6.9394e-02, 1.2233e-01,  ..., 2.3485e-04,\n",
       "            9.0569e-05, 4.2227e-05],\n",
       "           ...,\n",
       "           [7.0465e-03, 3.1036e-04, 7.7365e-04,  ..., 1.8401e-02,\n",
       "            3.6160e-02, 4.3924e-02],\n",
       "           [1.0477e-04, 7.3438e-05, 5.4740e-04,  ..., 9.5891e-02,\n",
       "            1.4777e-01, 1.2898e-01],\n",
       "           [1.9837e-03, 1.7544e-04, 1.3408e-03,  ..., 8.2889e-02,\n",
       "            2.5903e-01, 7.4874e-02]],\n",
       " \n",
       "          [[5.6987e-02, 1.4942e-02, 2.2877e-02,  ..., 4.1344e-03,\n",
       "            1.0106e-02, 7.8444e-03],\n",
       "           [8.1779e-03, 6.3672e-03, 4.3761e-03,  ..., 2.0070e-03,\n",
       "            3.4011e-03, 6.5128e-04],\n",
       "           [6.8799e-02, 5.5790e-03, 1.6064e-02,  ..., 2.0707e-03,\n",
       "            8.5502e-03, 9.4555e-03],\n",
       "           ...,\n",
       "           [3.3656e-02, 1.5107e-02, 3.8226e-03,  ..., 5.5688e-03,\n",
       "            3.7067e-03, 3.3087e-03],\n",
       "           [7.3056e-04, 3.9298e-03, 3.4627e-03,  ..., 9.2497e-03,\n",
       "            1.1704e-02, 8.0794e-03],\n",
       "           [5.6228e-03, 7.6340e-03, 5.9718e-03,  ..., 6.0844e-03,\n",
       "            2.1961e-02, 8.5943e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.6431e-02, 2.4178e-02, 3.3328e-02,  ..., 1.1138e-02,\n",
       "            3.4082e-03, 6.3698e-03],\n",
       "           [5.0120e-03, 2.3812e-03, 7.7749e-03,  ..., 6.1886e-03,\n",
       "            1.7819e-03, 2.9042e-04],\n",
       "           [9.5403e-03, 6.3154e-03, 5.9676e-04,  ..., 1.6959e-02,\n",
       "            2.6409e-03, 1.1253e-03],\n",
       "           ...,\n",
       "           [1.5447e-02, 3.5778e-03, 7.1891e-03,  ..., 7.8868e-04,\n",
       "            1.3167e-02, 1.5297e-02],\n",
       "           [1.6200e-03, 6.8072e-03, 4.0818e-03,  ..., 1.0280e-02,\n",
       "            1.4050e-02, 5.1528e-03],\n",
       "           [1.8910e-03, 4.3083e-03, 1.4290e-02,  ..., 4.3854e-03,\n",
       "            1.7415e-02, 1.4882e-03]],\n",
       " \n",
       "          [[4.1854e-01, 3.2857e-03, 7.9372e-03,  ..., 2.3913e-03,\n",
       "            3.3418e-03, 2.7166e-02],\n",
       "           [1.2324e-01, 1.5330e-01, 1.9767e-03,  ..., 8.7628e-03,\n",
       "            2.1620e-03, 1.0164e-03],\n",
       "           [1.8972e-01, 7.5016e-03, 2.1379e-01,  ..., 1.4973e-03,\n",
       "            2.4333e-03, 1.3838e-03],\n",
       "           ...,\n",
       "           [1.4508e-02, 1.5158e-02, 1.3994e-03,  ..., 1.3545e-01,\n",
       "            1.9747e-03, 6.4363e-03],\n",
       "           [7.2265e-02, 2.5954e-03, 5.7251e-03,  ..., 6.0973e-03,\n",
       "            2.9216e-03, 9.1902e-03],\n",
       "           [2.0787e-01, 7.1710e-03, 5.3650e-02,  ..., 2.4276e-03,\n",
       "            4.5644e-03, 1.2273e-01]],\n",
       " \n",
       "          [[2.1613e-06, 1.8255e-07, 6.3971e-07,  ..., 6.4301e-08,\n",
       "            4.1856e-07, 9.9535e-01],\n",
       "           [2.2776e-03, 2.2297e-02, 6.2621e-02,  ..., 3.7517e-02,\n",
       "            1.2150e-03, 2.5558e-03],\n",
       "           [1.0493e-02, 3.4188e-02, 5.7054e-03,  ..., 7.2006e-03,\n",
       "            7.6002e-04, 1.4649e-03],\n",
       "           ...,\n",
       "           [1.8080e-03, 5.8831e-02, 2.6745e-02,  ..., 3.6623e-02,\n",
       "            1.4710e-03, 8.8129e-04],\n",
       "           [1.2850e-03, 7.5932e-04, 9.0554e-04,  ..., 3.1860e-03,\n",
       "            2.7735e-01, 1.7062e-04],\n",
       "           [1.6706e-02, 6.0578e-04, 6.1475e-04,  ..., 1.4377e-03,\n",
       "            1.9746e-02, 2.4079e-02]]],\n",
       " \n",
       " \n",
       "         [[[4.1711e-01, 6.4296e-03, 4.7214e-03,  ..., 1.4935e-03,\n",
       "            1.5308e-03, 1.3315e-03],\n",
       "           [8.3467e-03, 3.4780e-03, 1.3254e-02,  ..., 2.9228e-02,\n",
       "            2.5571e-02, 2.6422e-02],\n",
       "           [1.0356e-01, 5.1933e-03, 6.4789e-03,  ..., 1.2312e-02,\n",
       "            1.1637e-02, 1.0999e-02],\n",
       "           ...,\n",
       "           [4.8334e-02, 9.0512e-03, 6.8471e-03,  ..., 8.0468e-03,\n",
       "            8.2223e-03, 8.7166e-03],\n",
       "           [4.4825e-02, 8.5047e-03, 7.4203e-03,  ..., 8.7209e-03,\n",
       "            8.3869e-03, 9.0782e-03],\n",
       "           [4.9247e-02, 9.2827e-03, 7.3188e-03,  ..., 8.9205e-03,\n",
       "            8.7822e-03, 8.6543e-03]],\n",
       " \n",
       "          [[2.1081e-01, 6.9778e-03, 1.5520e-02,  ..., 7.3508e-03,\n",
       "            8.2536e-03, 8.7291e-03],\n",
       "           [6.6747e-02, 1.6802e-01, 7.4599e-02,  ..., 8.1947e-04,\n",
       "            6.3889e-04, 3.9060e-04],\n",
       "           [1.6043e-02, 3.0536e-01, 4.7756e-02,  ..., 6.7043e-04,\n",
       "            9.9206e-04, 8.2977e-04],\n",
       "           ...,\n",
       "           [6.4090e-03, 2.5743e-05, 2.7143e-04,  ..., 3.1329e-01,\n",
       "            1.7093e-01, 1.4336e-01],\n",
       "           [5.6438e-03, 1.0301e-05, 1.8789e-04,  ..., 4.9584e-01,\n",
       "            1.5909e-01, 1.1675e-01],\n",
       "           [3.8371e-03, 6.1998e-06, 9.7617e-05,  ..., 5.0884e-01,\n",
       "            2.7800e-01, 1.0568e-01]],\n",
       " \n",
       "          [[5.3621e-02, 1.0525e-02, 1.7693e-02,  ..., 7.2244e-03,\n",
       "            7.7707e-03, 7.2491e-03],\n",
       "           [3.5209e-03, 3.2121e-03, 1.2036e-02,  ..., 2.0736e-02,\n",
       "            2.0139e-02, 2.0565e-02],\n",
       "           [2.0403e-03, 9.2686e-03, 7.1836e-03,  ..., 4.8473e-02,\n",
       "            5.1224e-02, 4.9482e-02],\n",
       "           ...,\n",
       "           [2.4353e-02, 8.9340e-03, 1.4551e-02,  ..., 2.7636e-02,\n",
       "            2.8038e-02, 2.7966e-02],\n",
       "           [1.7172e-02, 9.1242e-03, 1.4482e-02,  ..., 2.8531e-02,\n",
       "            2.8930e-02, 2.9317e-02],\n",
       "           [1.6868e-02, 8.6308e-03, 1.5680e-02,  ..., 2.8365e-02,\n",
       "            2.9496e-02, 2.9791e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2825e-02, 1.3818e-02, 1.8428e-02,  ..., 1.8830e-02,\n",
       "            1.8738e-02, 1.8362e-02],\n",
       "           [3.6631e-03, 4.3625e-03, 3.7679e-02,  ..., 1.9515e-02,\n",
       "            1.7568e-02, 1.8553e-02],\n",
       "           [1.7654e-02, 1.8223e-02, 1.5177e-02,  ..., 1.7591e-02,\n",
       "            1.7203e-02, 1.9027e-02],\n",
       "           ...,\n",
       "           [2.9901e-02, 9.6670e-03, 8.8992e-03,  ..., 3.2810e-02,\n",
       "            3.1913e-02, 3.3307e-02],\n",
       "           [2.8924e-02, 9.0285e-03, 8.4131e-03,  ..., 3.6318e-02,\n",
       "            3.4973e-02, 3.6856e-02],\n",
       "           [3.2802e-02, 8.3528e-03, 8.4571e-03,  ..., 3.6656e-02,\n",
       "            3.6198e-02, 3.7602e-02]],\n",
       " \n",
       "          [[3.9662e-01, 6.9332e-03, 7.0806e-03,  ..., 4.1132e-03,\n",
       "            4.4952e-03, 4.0052e-03],\n",
       "           [1.8054e-01, 5.5765e-02, 5.7248e-03,  ..., 1.2979e-03,\n",
       "            1.4418e-03, 1.4405e-03],\n",
       "           [3.3747e-01, 1.7116e-03, 6.5026e-02,  ..., 4.0791e-03,\n",
       "            4.6161e-03, 4.4073e-03],\n",
       "           ...,\n",
       "           [8.0294e-02, 5.2789e-03, 1.7530e-02,  ..., 2.1901e-02,\n",
       "            5.0424e-02, 7.4969e-02],\n",
       "           [8.2540e-02, 4.7942e-03, 2.2025e-02,  ..., 4.2351e-02,\n",
       "            2.2041e-02, 5.5169e-02],\n",
       "           [9.1004e-02, 4.4432e-03, 1.4352e-02,  ..., 6.6478e-02,\n",
       "            5.4132e-02, 3.0372e-02]],\n",
       " \n",
       "          [[2.2523e-06, 3.7384e-06, 1.3091e-06,  ..., 1.4247e-08,\n",
       "            1.1888e-08, 1.2922e-08],\n",
       "           [7.7381e-03, 2.6211e-03, 1.9858e-03,  ..., 8.9728e-03,\n",
       "            9.5530e-03, 9.6246e-03],\n",
       "           [7.0532e-03, 4.8143e-03, 1.6168e-02,  ..., 1.1747e-02,\n",
       "            1.2039e-02, 1.1814e-02],\n",
       "           ...,\n",
       "           [2.1775e-04, 1.4835e-03, 1.3341e-02,  ..., 1.3483e-01,\n",
       "            1.3535e-01, 1.3425e-01],\n",
       "           [2.0335e-04, 1.5918e-03, 1.3969e-02,  ..., 1.3079e-01,\n",
       "            1.2379e-01, 1.2741e-01],\n",
       "           [1.8250e-04, 1.5325e-03, 1.6170e-02,  ..., 1.3776e-01,\n",
       "            1.3289e-01, 1.2422e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.2301e-01, 1.4900e-03, 1.5696e-03,  ..., 8.1778e-04,\n",
       "            8.4365e-03, 1.5166e-02],\n",
       "           [1.2864e-01, 5.7028e-03, 6.7928e-03,  ..., 1.8462e-03,\n",
       "            8.0679e-03, 3.1552e-03],\n",
       "           [2.1109e-01, 3.2258e-03, 1.8859e-03,  ..., 3.1574e-04,\n",
       "            7.0050e-03, 2.6070e-03],\n",
       "           ...,\n",
       "           [4.0066e-02, 5.4236e-03, 2.5699e-03,  ..., 7.8027e-03,\n",
       "            3.2056e-02, 1.0274e-01],\n",
       "           [4.8282e-02, 9.6208e-04, 2.1859e-03,  ..., 5.8397e-03,\n",
       "            3.6335e-02, 1.0301e-01],\n",
       "           [6.8313e-01, 4.8623e-05, 5.7974e-05,  ..., 1.2232e-04,\n",
       "            1.4884e-02, 1.7532e-01]],\n",
       " \n",
       "          [[4.3347e-01, 5.7920e-03, 7.7769e-03,  ..., 3.7439e-03,\n",
       "            1.3640e-02, 1.5482e-02],\n",
       "           [6.4006e-01, 4.9109e-03, 4.7606e-03,  ..., 6.0927e-06,\n",
       "            4.9304e-05, 1.5083e-04],\n",
       "           [7.0823e-01, 1.0100e-02, 1.3827e-03,  ..., 2.6926e-05,\n",
       "            6.0889e-04, 2.8995e-03],\n",
       "           ...,\n",
       "           [7.5243e-01, 1.5625e-05, 7.6098e-06,  ..., 3.2057e-04,\n",
       "            5.8575e-02, 1.7819e-01],\n",
       "           [5.0383e-01, 3.7992e-05, 2.2599e-04,  ..., 1.0330e-02,\n",
       "            1.0122e-01, 3.3729e-01],\n",
       "           [9.9134e-01, 6.7377e-07, 2.2348e-06,  ..., 8.9919e-05,\n",
       "            2.4370e-03, 5.6739e-03]],\n",
       " \n",
       "          [[4.5846e-01, 1.7047e-03, 3.6263e-03,  ..., 9.8865e-04,\n",
       "            1.9038e-02, 1.8658e-02],\n",
       "           [9.8431e-01, 3.8548e-03, 1.3316e-04,  ..., 6.4746e-07,\n",
       "            6.3610e-06, 6.6354e-06],\n",
       "           [9.6574e-01, 2.9576e-02, 2.2109e-04,  ..., 9.2036e-07,\n",
       "            6.2464e-05, 2.0905e-04],\n",
       "           ...,\n",
       "           [8.9532e-01, 2.0690e-05, 3.2758e-05,  ..., 2.2920e-03,\n",
       "            1.2285e-02, 4.1850e-02],\n",
       "           [8.0120e-01, 1.5216e-06, 1.1324e-05,  ..., 2.3585e-03,\n",
       "            1.3003e-01, 4.2221e-02],\n",
       "           [9.1446e-01, 4.4722e-07, 7.8103e-06,  ..., 6.9688e-05,\n",
       "            5.7632e-02, 2.6281e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2105e-01, 3.5627e-03, 6.0753e-03,  ..., 2.9745e-03,\n",
       "            1.2006e-02, 3.0725e-02],\n",
       "           [1.8170e-01, 7.9235e-04, 3.5296e-03,  ..., 6.8500e-05,\n",
       "            3.0244e-03, 1.6268e-03],\n",
       "           [3.4128e-01, 1.2776e-03, 7.2690e-04,  ..., 9.9026e-04,\n",
       "            6.1647e-03, 3.2313e-03],\n",
       "           ...,\n",
       "           [3.1155e-01, 7.6206e-04, 5.2356e-04,  ..., 4.9515e-04,\n",
       "            1.7656e-01, 1.2277e-01],\n",
       "           [1.9636e-01, 4.4282e-04, 5.7413e-04,  ..., 1.8264e-02,\n",
       "            1.2014e-01, 9.3289e-02],\n",
       "           [7.9510e-01, 2.4908e-04, 1.6144e-04,  ..., 5.6568e-03,\n",
       "            3.2575e-02, 5.1531e-02]],\n",
       " \n",
       "          [[4.5969e-01, 1.4092e-03, 4.9882e-04,  ..., 5.2496e-04,\n",
       "            4.8406e-02, 5.5372e-02],\n",
       "           [4.0626e-01, 1.6886e-02, 3.3271e-03,  ..., 3.2405e-03,\n",
       "            1.2696e-02, 7.7619e-03],\n",
       "           [2.5005e-01, 2.0235e-03, 2.8363e-04,  ..., 1.6873e-04,\n",
       "            3.7890e-02, 5.0930e-03],\n",
       "           ...,\n",
       "           [7.3470e-02, 7.1167e-02, 1.6582e-03,  ..., 1.3552e-02,\n",
       "            2.9999e-02, 1.2933e-02],\n",
       "           [2.8988e-01, 1.7133e-03, 4.2432e-04,  ..., 1.1782e-03,\n",
       "            6.7429e-02, 1.1370e-02],\n",
       "           [1.9307e-03, 4.9340e-06, 7.5325e-06,  ..., 4.4650e-05,\n",
       "            4.4190e-03, 9.2198e-05]],\n",
       " \n",
       "          [[5.3719e-01, 1.2564e-02, 1.6548e-02,  ..., 8.4286e-04,\n",
       "            2.8674e-03, 4.0832e-03],\n",
       "           [1.3022e-01, 5.0101e-03, 8.5815e-01,  ..., 8.6529e-11,\n",
       "            1.0877e-09, 5.0444e-07],\n",
       "           [1.6909e-03, 2.1666e-05, 1.4461e-04,  ..., 1.2644e-06,\n",
       "            1.3667e-08, 1.2229e-09],\n",
       "           ...,\n",
       "           [1.4662e-02, 2.4083e-08, 2.6775e-09,  ..., 2.5389e-04,\n",
       "            9.8264e-01, 7.8180e-04],\n",
       "           [1.1407e-02, 4.6406e-08, 5.0626e-08,  ..., 6.1965e-07,\n",
       "            9.0777e-05, 9.8841e-01],\n",
       "           [9.9999e-01, 2.5289e-10, 2.4575e-08,  ..., 4.1538e-08,\n",
       "            1.1523e-07, 1.3840e-05]]],\n",
       " \n",
       " \n",
       "         [[[5.7791e-01, 8.0728e-04, 1.2696e-03,  ..., 2.5194e-03,\n",
       "            2.5996e-03, 2.5195e-03],\n",
       "           [1.9163e-02, 1.8669e-02, 2.7926e-02,  ..., 7.2186e-04,\n",
       "            5.6663e-04, 5.7514e-04],\n",
       "           [1.5664e-02, 5.1462e-02, 4.3262e-02,  ..., 4.6548e-04,\n",
       "            3.3843e-04, 3.4563e-04],\n",
       "           ...,\n",
       "           [4.5614e-02, 7.5288e-03, 9.7373e-03,  ..., 1.4575e-03,\n",
       "            1.2215e-03, 1.2003e-03],\n",
       "           [4.8839e-02, 5.8886e-03, 7.5207e-03,  ..., 1.6501e-03,\n",
       "            1.3771e-03, 1.3869e-03],\n",
       "           [6.3814e-02, 4.9157e-03, 6.3914e-03,  ..., 1.4237e-03,\n",
       "            1.1909e-03, 1.1968e-03]],\n",
       " \n",
       "          [[4.3482e-01, 8.4241e-03, 8.0555e-03,  ..., 4.8078e-03,\n",
       "            4.0286e-03, 3.8228e-03],\n",
       "           [8.7017e-02, 3.6877e-02, 8.7920e-02,  ..., 5.3466e-05,\n",
       "            1.1290e-05, 2.4114e-06],\n",
       "           [2.7108e-02, 4.9324e-03, 9.8917e-03,  ..., 2.0414e-06,\n",
       "            7.8194e-06, 6.4033e-06],\n",
       "           ...,\n",
       "           [4.0254e-03, 4.9278e-05, 7.7905e-05,  ..., 7.5454e-03,\n",
       "            3.8383e-02, 7.4927e-01],\n",
       "           [1.1734e-02, 6.3738e-05, 4.3039e-04,  ..., 1.2973e-01,\n",
       "            2.9148e-02, 1.4516e-01],\n",
       "           [3.5544e-02, 4.0267e-05, 3.0444e-04,  ..., 4.9584e-01,\n",
       "            1.4625e-01, 6.1023e-02]],\n",
       " \n",
       "          [[4.5606e-01, 7.8220e-03, 8.2618e-03,  ..., 3.4893e-03,\n",
       "            3.2330e-03, 3.0181e-03],\n",
       "           [9.7388e-01, 4.5439e-03, 2.8231e-03,  ..., 9.4622e-07,\n",
       "            5.1685e-07, 5.8159e-08],\n",
       "           [8.7945e-01, 7.9749e-02, 4.6457e-03,  ..., 6.6875e-07,\n",
       "            1.6619e-06, 7.0466e-07],\n",
       "           ...,\n",
       "           [2.6787e-01, 1.9428e-05, 5.1553e-05,  ..., 2.8812e-03,\n",
       "            1.3664e-03, 1.7649e-02],\n",
       "           [4.2004e-01, 4.4629e-06, 3.3213e-05,  ..., 4.4170e-02,\n",
       "            2.1513e-03, 2.6595e-03],\n",
       "           [5.2545e-01, 3.4602e-06, 1.9236e-05,  ..., 2.3469e-01,\n",
       "            5.4680e-02, 4.3045e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.1689e-01, 6.4126e-03, 7.6173e-03,  ..., 4.2962e-03,\n",
       "            4.5664e-03, 4.9018e-03],\n",
       "           [3.0165e-02, 1.2881e-02, 5.2036e-02,  ..., 1.8957e-04,\n",
       "            2.1865e-04, 2.4801e-04],\n",
       "           [8.0592e-02, 2.5829e-02, 1.5193e-02,  ..., 1.1732e-04,\n",
       "            1.2561e-04, 1.4770e-04],\n",
       "           ...,\n",
       "           [3.4764e-02, 1.5931e-03, 3.7597e-04,  ..., 2.2988e-02,\n",
       "            4.6281e-02, 7.6701e-02],\n",
       "           [4.8879e-02, 1.8185e-03, 3.4540e-04,  ..., 1.7429e-02,\n",
       "            3.5881e-02, 6.0708e-02],\n",
       "           [6.3139e-02, 1.9247e-03, 3.6399e-04,  ..., 1.2908e-02,\n",
       "            2.7439e-02, 4.6269e-02]],\n",
       " \n",
       "          [[4.4992e-01, 6.0016e-04, 3.7077e-04,  ..., 8.8570e-04,\n",
       "            8.3473e-04, 8.7286e-04],\n",
       "           [3.8574e-01, 2.0027e-03, 2.6455e-02,  ..., 4.4410e-04,\n",
       "            4.0969e-04, 3.8096e-04],\n",
       "           [3.3201e-01, 1.2404e-02, 1.3577e-02,  ..., 1.2763e-03,\n",
       "            9.2178e-04, 9.1115e-04],\n",
       "           ...,\n",
       "           [1.6381e-01, 7.9341e-04, 9.2535e-04,  ..., 3.0654e-03,\n",
       "            4.1108e-03, 3.8062e-03],\n",
       "           [1.3892e-01, 8.2456e-04, 8.3440e-04,  ..., 4.4868e-03,\n",
       "            5.2817e-03, 5.2593e-03],\n",
       "           [2.0514e-01, 8.0713e-04, 6.9120e-04,  ..., 3.5379e-03,\n",
       "            4.5908e-03, 4.1757e-03]],\n",
       " \n",
       "          [[5.4790e-01, 1.2434e-02, 3.0388e-02,  ..., 3.1380e-04,\n",
       "            2.2803e-04, 6.8556e-04],\n",
       "           [6.8168e-03, 1.8018e-04, 9.9275e-01,  ..., 3.0986e-12,\n",
       "            1.5599e-13, 5.7248e-11],\n",
       "           [4.2510e-03, 7.6307e-06, 3.8759e-04,  ..., 4.2501e-08,\n",
       "            5.5967e-13, 2.6488e-14],\n",
       "           ...,\n",
       "           [1.1986e-09, 9.5751e-14, 5.2409e-14,  ..., 1.9061e-05,\n",
       "            9.9997e-01, 7.4892e-06],\n",
       "           [6.5601e-09, 8.9857e-12, 4.3622e-12,  ..., 1.4993e-09,\n",
       "            5.0663e-06, 9.9999e-01],\n",
       "           [1.2608e-03, 2.9263e-07, 3.4945e-05,  ..., 1.0049e-02,\n",
       "            1.2870e-04, 7.7227e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.7372e-01, 1.3117e-02, 2.7810e-03,  ..., 1.6241e-02,\n",
       "            2.6664e-02, 8.4264e-02],\n",
       "           [2.5007e-01, 1.4223e-02, 2.1273e-03,  ..., 1.8355e-03,\n",
       "            4.5957e-02, 2.4977e-01],\n",
       "           [1.5833e-01, 1.7085e-02, 1.5215e-03,  ..., 2.8356e-03,\n",
       "            3.9802e-02, 1.7995e-01],\n",
       "           ...,\n",
       "           [1.4133e-01, 2.7779e-02, 3.7638e-04,  ..., 3.9815e-02,\n",
       "            2.4076e-02, 2.5245e-01],\n",
       "           [4.6742e-01, 5.7135e-03, 1.3994e-03,  ..., 1.0668e-02,\n",
       "            1.7732e-02, 7.1980e-02],\n",
       "           [6.0169e-01, 1.0407e-02, 2.0987e-03,  ..., 1.1860e-02,\n",
       "            2.3700e-02, 9.6628e-02]],\n",
       " \n",
       "          [[3.0446e-01, 1.5978e-02, 8.1813e-03,  ..., 3.7890e-03,\n",
       "            1.4881e-02, 9.8076e-02],\n",
       "           [9.0753e-01, 1.8515e-03, 6.3385e-05,  ..., 2.0881e-05,\n",
       "            4.9675e-04, 6.7254e-02],\n",
       "           [6.6031e-01, 1.6332e-01, 2.9552e-04,  ..., 3.2644e-05,\n",
       "            1.2086e-03, 9.4264e-02],\n",
       "           ...,\n",
       "           [6.7414e-01, 1.7206e-05, 2.6954e-05,  ..., 2.9866e-02,\n",
       "            5.1099e-02, 1.9903e-01],\n",
       "           [1.7655e-01, 8.2697e-05, 3.1818e-04,  ..., 4.1525e-01,\n",
       "            1.2460e-01, 8.7942e-02],\n",
       "           [6.6859e-01, 6.0839e-03, 2.0303e-03,  ..., 1.5405e-03,\n",
       "            2.5529e-02, 7.5974e-02]],\n",
       " \n",
       "          [[6.3102e-02, 1.0718e-02, 6.4619e-03,  ..., 7.7497e-03,\n",
       "            1.4372e-02, 5.2844e-02],\n",
       "           [6.2334e-02, 1.1818e-01, 3.4261e-03,  ..., 4.2632e-03,\n",
       "            2.0755e-03, 2.8895e-02],\n",
       "           [1.1559e-01, 6.9228e-03, 4.8004e-02,  ..., 1.4797e-03,\n",
       "            5.5623e-03, 6.9953e-02],\n",
       "           ...,\n",
       "           [1.3368e-01, 1.1444e-03, 3.2948e-04,  ..., 1.1492e-01,\n",
       "            9.9381e-02, 1.0013e-01],\n",
       "           [1.0402e-01, 9.0906e-04, 1.4448e-03,  ..., 1.0920e-02,\n",
       "            7.3158e-02, 6.1454e-02],\n",
       "           [2.5896e-01, 4.3963e-03, 3.6475e-03,  ..., 8.2088e-03,\n",
       "            2.3519e-02, 1.4650e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.8214e-01, 4.6482e-03, 4.1057e-03,  ..., 2.1066e-03,\n",
       "            1.2125e-02, 2.7523e-01],\n",
       "           [2.4541e-01, 7.2567e-03, 9.5166e-03,  ..., 2.0253e-03,\n",
       "            2.3878e-03, 2.9735e-01],\n",
       "           [3.1328e-01, 6.1061e-03, 6.4925e-05,  ..., 8.8194e-04,\n",
       "            2.5105e-03, 3.8682e-01],\n",
       "           ...,\n",
       "           [2.4413e-01, 6.1213e-03, 2.3400e-03,  ..., 3.7873e-02,\n",
       "            2.6700e-02, 2.1619e-01],\n",
       "           [1.3580e-01, 7.8234e-04, 1.6783e-03,  ..., 6.2983e-03,\n",
       "            7.4377e-02, 1.1197e-01],\n",
       "           [2.7701e-01, 2.2312e-03, 4.4056e-03,  ..., 2.2107e-03,\n",
       "            2.0675e-02, 3.4755e-01]],\n",
       " \n",
       "          [[3.5147e-01, 2.4455e-03, 3.2603e-03,  ..., 3.3441e-03,\n",
       "            1.6365e-02, 3.2934e-01],\n",
       "           [3.4981e-01, 3.6349e-03, 7.7727e-03,  ..., 5.4879e-04,\n",
       "            4.8445e-03, 2.1011e-01],\n",
       "           [5.3431e-01, 1.8146e-04, 4.0922e-05,  ..., 2.6782e-05,\n",
       "            4.1100e-03, 2.5462e-01],\n",
       "           ...,\n",
       "           [3.3919e-01, 3.4880e-04, 7.0239e-04,  ..., 2.0674e-02,\n",
       "            9.0669e-02, 4.5190e-01],\n",
       "           [1.8975e-01, 3.9797e-04, 1.4028e-03,  ..., 1.6513e-02,\n",
       "            1.3008e-01, 3.4538e-01],\n",
       "           [5.3056e-01, 1.3728e-04, 3.9962e-04,  ..., 2.1968e-04,\n",
       "            5.4298e-02, 1.6458e-01]],\n",
       " \n",
       "          [[4.1615e-02, 2.0381e-02, 3.3649e-03,  ..., 9.6477e-03,\n",
       "            5.8917e-03, 5.2276e-03],\n",
       "           [5.6209e-01, 7.5183e-04, 2.4571e-03,  ..., 3.1049e-03,\n",
       "            2.9109e-02, 4.5225e-02],\n",
       "           [4.8976e-01, 4.8122e-03, 4.0354e-04,  ..., 3.8721e-03,\n",
       "            4.1343e-02, 6.8496e-02],\n",
       "           ...,\n",
       "           [2.5454e-01, 8.1450e-03, 8.2398e-03,  ..., 4.4128e-02,\n",
       "            8.4091e-02, 3.9162e-02],\n",
       "           [5.5732e-02, 8.7963e-03, 6.0752e-03,  ..., 2.3200e-02,\n",
       "            3.0585e-02, 1.7943e-02],\n",
       "           [1.7231e-01, 1.4193e-03, 1.8034e-03,  ..., 4.2277e-03,\n",
       "            1.0564e-02, 6.4586e-01]]],\n",
       " \n",
       " \n",
       "         [[[2.3370e-01, 1.4180e-02, 5.1492e-03,  ..., 2.8524e-02,\n",
       "            2.8100e-02, 2.6101e-02],\n",
       "           [8.4099e-02, 2.5199e-02, 6.8845e-02,  ..., 4.5919e-03,\n",
       "            5.5455e-03, 6.1929e-03],\n",
       "           [4.9566e-02, 4.3740e-02, 8.1586e-02,  ..., 1.5428e-03,\n",
       "            1.9644e-03, 2.7373e-03],\n",
       "           ...,\n",
       "           [2.6791e-01, 1.0215e-02, 4.2762e-03,  ..., 1.1695e-02,\n",
       "            1.3818e-02, 1.2104e-02],\n",
       "           [1.8911e-01, 1.1703e-02, 4.7224e-03,  ..., 1.2184e-02,\n",
       "            1.3715e-02, 1.2218e-02],\n",
       "           [2.1244e-01, 1.0800e-02, 4.4448e-03,  ..., 1.0943e-02,\n",
       "            1.2158e-02, 1.0393e-02]],\n",
       " \n",
       "          [[3.4078e-01, 1.7019e-02, 7.7303e-03,  ..., 8.1871e-03,\n",
       "            7.0973e-03, 5.4178e-03],\n",
       "           [9.1052e-01, 5.3356e-03, 3.8426e-03,  ..., 4.1737e-05,\n",
       "            4.3287e-05, 4.3477e-05],\n",
       "           [7.3848e-01, 3.6149e-02, 9.3644e-04,  ..., 6.7407e-06,\n",
       "            1.4796e-05, 9.6258e-05],\n",
       "           ...,\n",
       "           [6.4122e-01, 1.8248e-04, 4.6878e-04,  ..., 6.2446e-02,\n",
       "            5.9133e-03, 1.1173e-02],\n",
       "           [1.6318e-01, 1.4016e-05, 1.4239e-05,  ..., 7.2589e-01,\n",
       "            2.6702e-02, 2.3692e-03],\n",
       "           [1.4375e-01, 8.0749e-05, 1.1051e-05,  ..., 6.5194e-02,\n",
       "            7.0816e-01, 1.8786e-02]],\n",
       " \n",
       "          [[6.5917e-02, 8.4691e-03, 7.2761e-03,  ..., 9.9598e-03,\n",
       "            7.6331e-03, 8.5218e-03],\n",
       "           [3.7589e-02, 8.2765e-02, 2.6582e-02,  ..., 2.3512e-03,\n",
       "            8.9830e-04, 1.4044e-03],\n",
       "           [5.2903e-02, 2.0577e-02, 1.0964e-01,  ..., 1.7346e-03,\n",
       "            6.7112e-04, 1.0522e-03],\n",
       "           ...,\n",
       "           [5.2307e-02, 4.5096e-03, 2.4860e-03,  ..., 4.6067e-03,\n",
       "            4.4401e-03, 4.3875e-03],\n",
       "           [6.2769e-02, 2.0527e-03, 1.4084e-03,  ..., 3.5729e-03,\n",
       "            4.2045e-03, 4.0892e-03],\n",
       "           [4.0101e-02, 2.8190e-03, 1.7685e-03,  ..., 1.9234e-03,\n",
       "            2.1378e-03, 2.0825e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.8181e-01, 6.7197e-03, 4.7333e-03,  ..., 9.5864e-03,\n",
       "            9.5492e-03, 1.0032e-02],\n",
       "           [1.1724e-01, 7.3678e-02, 1.3697e-02,  ..., 1.1854e-03,\n",
       "            9.1374e-04, 1.2307e-03],\n",
       "           [2.4443e-01, 3.0975e-02, 2.0873e-03,  ..., 1.3866e-03,\n",
       "            1.3012e-03, 1.6328e-03],\n",
       "           ...,\n",
       "           [2.7539e-01, 5.1374e-03, 2.4265e-03,  ..., 3.8767e-02,\n",
       "            4.3270e-02, 4.7033e-02],\n",
       "           [2.3808e-01, 2.8569e-03, 1.5175e-03,  ..., 4.6489e-02,\n",
       "            5.3463e-02, 5.4390e-02],\n",
       "           [2.2351e-01, 3.8159e-03, 2.3767e-03,  ..., 4.4415e-02,\n",
       "            4.9066e-02, 5.2808e-02]],\n",
       " \n",
       "          [[3.6007e-01, 2.4719e-03, 1.9327e-03,  ..., 1.6782e-02,\n",
       "            1.9094e-02, 1.6634e-02],\n",
       "           [2.1354e-02, 1.4828e-02, 1.7632e-01,  ..., 7.3349e-04,\n",
       "            7.4176e-04, 1.2030e-03],\n",
       "           [1.4634e-02, 2.1738e-02, 1.4369e-02,  ..., 1.2483e-03,\n",
       "            1.2487e-03, 2.1876e-03],\n",
       "           ...,\n",
       "           [2.9366e-01, 4.8278e-04, 1.4332e-04,  ..., 2.8298e-02,\n",
       "            3.8583e-02, 2.9099e-02],\n",
       "           [2.5912e-01, 1.9981e-04, 5.9529e-05,  ..., 4.2768e-02,\n",
       "            6.5516e-02, 4.5660e-02],\n",
       "           [2.6486e-01, 4.0632e-04, 1.3080e-04,  ..., 3.6756e-02,\n",
       "            4.9088e-02, 3.6620e-02]],\n",
       " \n",
       "          [[6.2103e-02, 9.0228e-03, 3.3516e-02,  ..., 3.0660e-02,\n",
       "            4.1921e-02, 4.3001e-02],\n",
       "           [2.1623e-02, 2.8833e-02, 7.0533e-02,  ..., 1.0314e-02,\n",
       "            7.0021e-03, 9.3931e-03],\n",
       "           [5.0016e-02, 4.1359e-02, 1.6880e-02,  ..., 7.3343e-03,\n",
       "            3.3684e-03, 4.9915e-03],\n",
       "           ...,\n",
       "           [2.3385e-01, 4.0951e-03, 1.8167e-03,  ..., 4.8075e-02,\n",
       "            5.5964e-02, 4.7868e-02],\n",
       "           [3.0390e-01, 2.8067e-03, 1.1791e-03,  ..., 4.6443e-02,\n",
       "            5.0299e-02, 4.2166e-02],\n",
       "           [3.3163e-01, 3.5131e-03, 1.5269e-03,  ..., 3.7213e-02,\n",
       "            4.1291e-02, 3.6850e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[9.4068e-02, 1.7352e-02, 4.1670e-03,  ..., 9.8703e-03,\n",
       "            1.3922e-02, 6.3811e-01],\n",
       "           [1.2483e-02, 3.4259e-02, 8.1961e-03,  ..., 5.7044e-02,\n",
       "            7.9875e-03, 3.9850e-02],\n",
       "           [4.4291e-02, 2.9871e-02, 9.9617e-04,  ..., 2.6489e-02,\n",
       "            1.9718e-02, 3.8184e-01],\n",
       "           ...,\n",
       "           [6.4397e-02, 1.1522e-01, 5.5044e-04,  ..., 2.3838e-02,\n",
       "            3.3557e-02, 2.1946e-01],\n",
       "           [1.1880e-01, 7.1362e-03, 9.3044e-04,  ..., 3.2006e-03,\n",
       "            2.6534e-02, 4.7746e-01],\n",
       "           [1.2289e-01, 1.3221e-03, 1.4125e-03,  ..., 1.1271e-03,\n",
       "            6.9686e-02, 6.0665e-01]],\n",
       " \n",
       "          [[3.1972e-02, 3.7772e-02, 1.1040e-02,  ..., 2.3144e-02,\n",
       "            1.6670e-02, 2.9391e-02],\n",
       "           [1.3967e-01, 7.5443e-04, 3.9576e-03,  ..., 1.3264e-03,\n",
       "            2.7893e-02, 1.8391e-01],\n",
       "           [1.0195e-01, 7.0077e-03, 8.7691e-04,  ..., 1.2942e-03,\n",
       "            1.8091e-02, 4.8721e-01],\n",
       "           ...,\n",
       "           [9.0612e-02, 1.3860e-02, 7.2821e-03,  ..., 1.6784e-03,\n",
       "            8.5584e-02, 2.0794e-01],\n",
       "           [5.2594e-02, 4.1767e-03, 2.2463e-03,  ..., 7.3143e-03,\n",
       "            1.3017e-01, 1.0798e-01],\n",
       "           [3.3526e-02, 2.9672e-03, 2.3710e-03,  ..., 4.0926e-03,\n",
       "            8.2586e-03, 6.6748e-01]],\n",
       " \n",
       "          [[1.0372e-01, 2.5323e-02, 4.6278e-03,  ..., 1.9414e-02,\n",
       "            2.2046e-03, 4.2478e-01],\n",
       "           [2.6329e-02, 2.4792e-02, 3.9249e-02,  ..., 1.5288e-02,\n",
       "            7.6479e-04, 1.5128e-01],\n",
       "           [4.1445e-02, 2.1530e-02, 3.6278e-03,  ..., 1.2414e-02,\n",
       "            1.3370e-02, 5.6664e-01],\n",
       "           ...,\n",
       "           [3.1160e-02, 2.3873e-02, 1.4680e-02,  ..., 6.9388e-03,\n",
       "            2.3431e-03, 4.3195e-01],\n",
       "           [4.4784e-02, 1.8550e-02, 6.2340e-03,  ..., 2.2224e-02,\n",
       "            2.9536e-03, 1.1150e-01],\n",
       "           [5.5497e-03, 1.5102e-03, 8.3529e-04,  ..., 4.4419e-04,\n",
       "            2.2422e-02, 8.5402e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.0323e-02, 8.9455e-03, 4.7587e-03,  ..., 5.2420e-03,\n",
       "            1.0630e-02, 4.7647e-01],\n",
       "           [4.5954e-02, 2.0464e-02, 9.0983e-03,  ..., 3.6960e-04,\n",
       "            5.0095e-03, 6.7879e-01],\n",
       "           [6.2590e-02, 1.1997e-01, 1.3760e-05,  ..., 7.6490e-05,\n",
       "            7.7827e-03, 4.4641e-01],\n",
       "           ...,\n",
       "           [1.7972e-02, 5.0213e-04, 1.5985e-05,  ..., 2.4720e-02,\n",
       "            2.3956e-02, 8.5245e-01],\n",
       "           [2.1182e-02, 4.7289e-04, 2.1178e-04,  ..., 7.3243e-03,\n",
       "            2.4887e-02, 4.7507e-01],\n",
       "           [3.5502e-02, 9.7983e-03, 2.0167e-03,  ..., 1.4072e-02,\n",
       "            1.3850e-02, 5.7611e-01]],\n",
       " \n",
       "          [[1.8777e-01, 1.0125e-02, 3.1812e-03,  ..., 3.7963e-03,\n",
       "            6.6648e-03, 7.1662e-01],\n",
       "           [1.4863e-01, 5.4058e-02, 2.1567e-02,  ..., 6.6937e-04,\n",
       "            1.0428e-03, 6.0411e-01],\n",
       "           [1.5268e-01, 1.0349e-01, 3.3720e-03,  ..., 2.3321e-04,\n",
       "            5.9958e-04, 6.2173e-01],\n",
       "           ...,\n",
       "           [7.6858e-02, 2.6183e-04, 6.9517e-05,  ..., 1.1178e-02,\n",
       "            6.1296e-02, 2.0927e-01],\n",
       "           [6.7149e-02, 1.0483e-04, 3.2565e-05,  ..., 2.3646e-02,\n",
       "            6.9690e-02, 3.5905e-01],\n",
       "           [8.7248e-02, 1.4774e-03, 1.2504e-03,  ..., 5.7242e-04,\n",
       "            5.5523e-03, 8.7121e-01]],\n",
       " \n",
       "          [[3.4068e-01, 5.4357e-03, 1.0101e-03,  ..., 2.6972e-03,\n",
       "            1.4698e-01, 8.4357e-02],\n",
       "           [9.1013e-01, 5.7347e-05, 9.5335e-09,  ..., 1.0686e-09,\n",
       "            3.0257e-07, 8.9685e-02],\n",
       "           [8.4267e-04, 9.9688e-01, 4.3217e-06,  ..., 4.0042e-11,\n",
       "            3.0645e-08, 1.1428e-03],\n",
       "           ...,\n",
       "           [3.3254e-03, 1.8500e-10, 9.1484e-09,  ..., 6.1426e-07,\n",
       "            2.5355e-05, 1.1100e-05],\n",
       "           [3.6950e-03, 4.4735e-09, 1.9111e-08,  ..., 9.4643e-01,\n",
       "            3.4043e-02, 1.4343e-04],\n",
       "           [4.0189e-01, 4.8501e-03, 2.2119e-03,  ..., 2.9520e-03,\n",
       "            1.4898e-02, 2.5972e-02]]],\n",
       " \n",
       " \n",
       "         [[[6.2637e-02, 8.5512e-03, 1.6160e-03,  ..., 5.6110e-02,\n",
       "            5.9299e-02, 6.6621e-02],\n",
       "           [9.5259e-03, 3.9201e-03, 7.5055e-03,  ..., 1.0121e-02,\n",
       "            1.7523e-02, 2.2417e-02],\n",
       "           [3.5354e-02, 1.5629e-02, 1.2234e-02,  ..., 1.2002e-02,\n",
       "            1.8343e-02, 2.8740e-02],\n",
       "           ...,\n",
       "           [5.5480e-02, 1.3180e-02, 5.9094e-03,  ..., 3.2773e-02,\n",
       "            3.8869e-02, 5.3224e-02],\n",
       "           [5.1174e-02, 1.0711e-02, 1.0019e-02,  ..., 3.3272e-02,\n",
       "            3.7588e-02, 5.0081e-02],\n",
       "           [6.3383e-02, 9.2751e-03, 7.6396e-03,  ..., 3.4476e-02,\n",
       "            3.8292e-02, 4.9357e-02]],\n",
       " \n",
       "          [[2.1703e-02, 9.3970e-03, 1.7754e-02,  ..., 2.2319e-02,\n",
       "            3.0568e-02, 3.7532e-02],\n",
       "           [6.0345e-02, 3.0947e-02, 2.5005e-02,  ..., 7.4097e-03,\n",
       "            4.9794e-03, 5.7200e-03],\n",
       "           [4.9575e-02, 2.4613e-02, 1.2342e-01,  ..., 6.2223e-03,\n",
       "            2.8012e-03, 2.8957e-03],\n",
       "           ...,\n",
       "           [1.2204e-01, 3.3974e-03, 2.1426e-03,  ..., 4.4945e-02,\n",
       "            3.7492e-02, 3.6910e-02],\n",
       "           [1.1911e-01, 2.3907e-03, 1.2687e-03,  ..., 4.1743e-02,\n",
       "            3.4492e-02, 3.5219e-02],\n",
       "           [1.0918e-01, 2.2234e-03, 1.0846e-03,  ..., 4.0242e-02,\n",
       "            3.6937e-02, 4.0374e-02]],\n",
       " \n",
       "          [[1.0963e-01, 1.0480e-02, 7.9887e-03,  ..., 4.4154e-02,\n",
       "            6.7734e-02, 5.6358e-02],\n",
       "           [1.1936e-02, 6.1660e-03, 1.1816e-02,  ..., 5.5447e-03,\n",
       "            9.9034e-03, 8.3373e-03],\n",
       "           [3.6238e-02, 6.4499e-03, 2.9338e-02,  ..., 5.6886e-03,\n",
       "            1.0373e-02, 8.0743e-03],\n",
       "           ...,\n",
       "           [8.7894e-02, 1.5775e-02, 5.2469e-03,  ..., 3.2430e-02,\n",
       "            4.8042e-02, 3.8823e-02],\n",
       "           [8.4498e-02, 2.4863e-02, 6.2573e-03,  ..., 3.8134e-02,\n",
       "            4.9819e-02, 3.9786e-02],\n",
       "           [8.0071e-02, 2.5319e-02, 4.7576e-03,  ..., 3.6987e-02,\n",
       "            4.6989e-02, 3.8375e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.3966e-02, 7.5983e-03, 3.7854e-03,  ..., 1.2852e-02,\n",
       "            1.5179e-02, 2.1802e-02],\n",
       "           [2.0703e-02, 6.7271e-03, 1.3679e-02,  ..., 8.8963e-04,\n",
       "            9.2684e-04, 1.8518e-03],\n",
       "           [1.6896e-02, 3.6618e-02, 3.4250e-02,  ..., 5.8082e-04,\n",
       "            3.7345e-04, 9.1640e-04],\n",
       "           ...,\n",
       "           [2.5687e-02, 7.8098e-04, 7.0850e-04,  ..., 2.9030e-02,\n",
       "            5.4891e-02, 9.1145e-02],\n",
       "           [1.7035e-02, 3.6505e-04, 2.4091e-04,  ..., 2.8831e-02,\n",
       "            4.7493e-02, 7.8794e-02],\n",
       "           [2.0059e-02, 5.3229e-04, 3.1977e-04,  ..., 2.9860e-02,\n",
       "            4.5325e-02, 7.2518e-02]],\n",
       " \n",
       "          [[2.1001e-01, 4.0519e-03, 1.0720e-03,  ..., 1.1421e-02,\n",
       "            1.2649e-02, 2.0213e-02],\n",
       "           [3.9773e-02, 7.0290e-03, 2.2679e-03,  ..., 1.8104e-04,\n",
       "            1.6917e-04, 3.5161e-04],\n",
       "           [5.5406e-02, 5.5304e-02, 2.3856e-02,  ..., 4.9438e-04,\n",
       "            4.4336e-04, 8.7914e-04],\n",
       "           ...,\n",
       "           [4.0992e-02, 1.8444e-04, 6.9141e-05,  ..., 5.1720e-02,\n",
       "            4.5469e-02, 4.3983e-02],\n",
       "           [4.7481e-02, 1.0301e-04, 6.6025e-05,  ..., 1.0260e-01,\n",
       "            6.1561e-02, 5.4495e-02],\n",
       "           [5.1427e-02, 1.4461e-04, 1.1687e-04,  ..., 1.1838e-01,\n",
       "            6.9846e-02, 6.1209e-02]],\n",
       " \n",
       "          [[2.8042e-01, 4.3107e-03, 1.3490e-03,  ..., 1.3321e-02,\n",
       "            3.2145e-02, 8.9056e-02],\n",
       "           [8.4809e-01, 3.4557e-04, 1.0298e-04,  ..., 6.4255e-07,\n",
       "            3.5358e-07, 1.1501e-05],\n",
       "           [5.7427e-03, 9.6890e-01, 2.4085e-03,  ..., 1.6127e-08,\n",
       "            2.2564e-07, 7.1493e-06],\n",
       "           ...,\n",
       "           [9.5378e-01, 2.0426e-08, 1.5966e-08,  ..., 2.2732e-02,\n",
       "            1.9061e-05, 2.5016e-05],\n",
       "           [7.7260e-05, 1.1272e-09, 2.3474e-10,  ..., 9.9938e-01,\n",
       "            3.9334e-04, 3.0932e-06],\n",
       "           [2.5861e-05, 1.5423e-09, 1.1923e-10,  ..., 7.9470e-05,\n",
       "            9.9896e-01, 6.2554e-04]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[9.1038e-02, 3.4963e-03, 1.8293e-03,  ..., 1.3059e-02,\n",
       "            4.3170e-02, 2.6138e-01],\n",
       "           [1.3282e-01, 9.0776e-02, 5.8537e-06,  ..., 1.0128e-02,\n",
       "            3.9190e-02, 5.0791e-01],\n",
       "           [1.2000e-01, 1.1652e-05, 2.2083e-02,  ..., 1.9349e-03,\n",
       "            3.4573e-02, 7.5060e-01],\n",
       "           ...,\n",
       "           [8.8646e-04, 1.8021e-05, 1.6779e-05,  ..., 2.7353e-04,\n",
       "            3.3846e-04, 1.1060e-03],\n",
       "           [8.5261e-02, 1.3160e-03, 7.0602e-04,  ..., 5.6008e-03,\n",
       "            3.8193e-02, 3.5025e-01],\n",
       "           [5.6532e-02, 1.2068e-03, 1.3761e-03,  ..., 9.6351e-04,\n",
       "            2.3174e-02, 7.3304e-01]],\n",
       " \n",
       "          [[1.1383e-01, 2.7238e-02, 8.4678e-03,  ..., 1.6533e-02,\n",
       "            2.4238e-03, 2.1270e-01],\n",
       "           [6.5304e-04, 3.7086e-02, 1.1632e-02,  ..., 3.3373e-02,\n",
       "            6.3474e-04, 8.8748e-03],\n",
       "           [1.7100e-02, 8.4733e-02, 1.5368e-02,  ..., 3.0411e-02,\n",
       "            6.8346e-03, 1.6608e-01],\n",
       "           ...,\n",
       "           [3.0796e-03, 4.9256e-02, 1.0673e-02,  ..., 3.3424e-02,\n",
       "            1.7502e-03, 4.3715e-02],\n",
       "           [1.3303e-03, 1.9560e-02, 1.9431e-03,  ..., 1.1446e-02,\n",
       "            2.7981e-03, 9.3693e-02],\n",
       "           [7.9473e-03, 7.9377e-04, 7.3358e-04,  ..., 6.0398e-04,\n",
       "            1.1917e-02, 8.7495e-01]],\n",
       " \n",
       "          [[2.4582e-02, 8.5601e-03, 1.5342e-03,  ..., 3.3554e-03,\n",
       "            2.7540e-02, 7.5226e-01],\n",
       "           [1.8558e-01, 6.1660e-02, 4.5639e-03,  ..., 3.5116e-06,\n",
       "            3.2952e-05, 5.8402e-01],\n",
       "           [3.8730e-02, 1.2714e-01, 5.2610e-03,  ..., 9.1434e-07,\n",
       "            1.0981e-05, 7.8097e-01],\n",
       "           ...,\n",
       "           [3.2976e-04, 2.1118e-06, 4.6823e-07,  ..., 3.8760e-03,\n",
       "            6.3146e-03, 3.8625e-01],\n",
       "           [6.7278e-04, 7.5355e-06, 2.6410e-06,  ..., 5.2190e-02,\n",
       "            3.4656e-02, 4.0999e-01],\n",
       "           [2.6382e-02, 3.9710e-03, 1.8799e-03,  ..., 1.6420e-03,\n",
       "            1.6144e-02, 7.6712e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4692e-02, 6.9817e-03, 1.2318e-03,  ..., 7.9098e-03,\n",
       "            5.3638e-03, 8.2694e-01],\n",
       "           [9.1540e-03, 7.4043e-02, 4.8171e-02,  ..., 6.1670e-04,\n",
       "            6.7984e-04, 6.9580e-01],\n",
       "           [1.1790e-02, 1.5084e-02, 6.5171e-03,  ..., 9.1729e-05,\n",
       "            1.6523e-03, 8.5060e-01],\n",
       "           ...,\n",
       "           [8.3479e-03, 4.5164e-04, 1.6429e-04,  ..., 8.8739e-03,\n",
       "            1.1670e-02, 9.3416e-01],\n",
       "           [1.5373e-02, 7.7571e-04, 1.3738e-04,  ..., 7.6139e-02,\n",
       "            1.6647e-02, 5.5401e-01],\n",
       "           [5.6227e-03, 1.0406e-02, 2.3877e-03,  ..., 6.2330e-03,\n",
       "            2.0092e-03, 8.4355e-01]],\n",
       " \n",
       "          [[1.6317e-01, 3.1194e-03, 2.5026e-04,  ..., 2.6979e-03,\n",
       "            6.3290e-03, 3.5884e-01],\n",
       "           [3.3309e-02, 6.6945e-03, 2.0617e-03,  ..., 2.4887e-03,\n",
       "            2.4268e-02, 9.3689e-02],\n",
       "           [7.8023e-02, 1.3498e-02, 1.4719e-03,  ..., 1.9732e-03,\n",
       "            5.5037e-03, 7.1821e-01],\n",
       "           ...,\n",
       "           [5.5416e-02, 3.5892e-03, 5.6217e-04,  ..., 5.5093e-03,\n",
       "            2.8803e-02, 3.2305e-01],\n",
       "           [7.9633e-02, 2.8015e-03, 4.0835e-04,  ..., 3.3253e-03,\n",
       "            2.0959e-02, 1.9083e-01],\n",
       "           [2.0060e-03, 4.7312e-04, 1.8826e-04,  ..., 2.8212e-04,\n",
       "            6.4964e-04, 9.8212e-01]],\n",
       " \n",
       "          [[3.9016e-03, 7.8944e-03, 1.5240e-03,  ..., 3.3391e-03,\n",
       "            6.4029e-02, 2.9999e-01],\n",
       "           [2.5789e-02, 1.0807e-03, 3.1078e-03,  ..., 3.5807e-02,\n",
       "            1.5830e-02, 1.0094e-01],\n",
       "           [1.4539e-02, 6.8513e-03, 3.1091e-04,  ..., 3.7749e-03,\n",
       "            1.0116e-02, 7.6647e-01],\n",
       "           ...,\n",
       "           [1.6073e-02, 1.8400e-01, 4.7536e-03,  ..., 2.8635e-03,\n",
       "            1.6103e-02, 9.9600e-02],\n",
       "           [2.2812e-03, 1.0425e-02, 3.8887e-04,  ..., 1.9400e-03,\n",
       "            9.9068e-02, 1.8089e-01],\n",
       "           [1.2810e-02, 1.0895e-03, 3.2364e-04,  ..., 1.1589e-03,\n",
       "            1.9518e-02, 8.8103e-01]]],\n",
       " \n",
       " \n",
       "         [[[9.4098e-02, 9.6619e-03, 2.1950e-03,  ..., 1.4156e-02,\n",
       "            1.8200e-02, 1.9369e-02],\n",
       "           [1.0811e-01, 9.7341e-03, 1.7111e-04,  ..., 3.1246e-02,\n",
       "            1.8818e-02, 1.2256e-02],\n",
       "           [8.8384e-02, 1.3152e-04, 2.2437e-02,  ..., 3.6415e-02,\n",
       "            2.3322e-02, 1.2081e-02],\n",
       "           ...,\n",
       "           [7.9590e-02, 8.8305e-03, 4.7793e-03,  ..., 1.1722e-02,\n",
       "            1.3199e-02, 2.3734e-02],\n",
       "           [9.0682e-02, 5.5462e-03, 2.6676e-03,  ..., 1.1613e-02,\n",
       "            7.2180e-03, 1.3595e-02],\n",
       "           [9.4513e-02, 2.7854e-03, 1.0752e-03,  ..., 1.3009e-02,\n",
       "            7.2001e-03, 1.0506e-02]],\n",
       " \n",
       "          [[8.7029e-02, 1.1732e-03, 3.4213e-03,  ..., 5.9028e-02,\n",
       "            9.1869e-02, 1.2802e-01],\n",
       "           [7.7959e-04, 6.0716e-04, 1.7273e-02,  ..., 1.8722e-02,\n",
       "            3.8684e-02, 3.2103e-02],\n",
       "           [1.0412e-03, 1.0903e-03, 5.3673e-03,  ..., 2.3675e-02,\n",
       "            3.2635e-02, 2.8830e-02],\n",
       "           ...,\n",
       "           [9.4147e-03, 3.3120e-03, 1.3716e-03,  ..., 3.1465e-02,\n",
       "            4.2705e-02, 5.2300e-02],\n",
       "           [7.1177e-03, 2.8147e-03, 7.7338e-04,  ..., 3.4194e-02,\n",
       "            3.7377e-02, 4.5558e-02],\n",
       "           [7.1978e-03, 2.4768e-03, 8.5269e-04,  ..., 3.7972e-02,\n",
       "            3.9883e-02, 4.6999e-02]],\n",
       " \n",
       "          [[2.4624e-02, 2.7944e-03, 9.1128e-04,  ..., 1.0033e-02,\n",
       "            7.0954e-03, 8.0759e-03],\n",
       "           [1.6967e-01, 4.5840e-02, 5.9792e-03,  ..., 8.6817e-05,\n",
       "            2.0327e-05, 1.3971e-05],\n",
       "           [1.1003e-01, 2.8118e-01, 3.7647e-02,  ..., 1.9628e-05,\n",
       "            1.3489e-05, 1.6146e-05],\n",
       "           ...,\n",
       "           [6.4441e-03, 3.5865e-05, 4.7641e-07,  ..., 4.8523e-02,\n",
       "            1.0729e-02, 5.7359e-03],\n",
       "           [1.6464e-03, 8.2143e-06, 5.7781e-07,  ..., 3.5250e-01,\n",
       "            3.2930e-02, 5.3276e-03],\n",
       "           [1.1357e-03, 4.8305e-06, 5.6057e-07,  ..., 4.5104e-01,\n",
       "            1.5590e-01, 9.3003e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4865e-02, 5.3198e-04, 2.7236e-04,  ..., 2.1197e-02,\n",
       "            2.0263e-02, 3.1381e-02],\n",
       "           [8.2193e-03, 1.9936e-02, 1.0549e-02,  ..., 1.1907e-02,\n",
       "            8.6444e-03, 1.0090e-02],\n",
       "           [1.6401e-02, 2.9127e-02, 5.8997e-02,  ..., 1.3836e-03,\n",
       "            7.3601e-04, 7.9890e-04],\n",
       "           ...,\n",
       "           [2.4266e-02, 9.6685e-04, 2.8684e-04,  ..., 1.0721e-01,\n",
       "            1.1506e-01, 1.3258e-01],\n",
       "           [2.5386e-02, 4.0496e-04, 1.6494e-04,  ..., 1.1216e-01,\n",
       "            9.3665e-02, 1.0084e-01],\n",
       "           [2.8006e-02, 2.4697e-04, 1.4626e-04,  ..., 1.1355e-01,\n",
       "            1.0357e-01, 1.1798e-01]],\n",
       " \n",
       "          [[1.4901e-01, 1.4792e-02, 3.2884e-03,  ..., 1.1924e-01,\n",
       "            1.2325e-01, 1.2788e-01],\n",
       "           [5.4485e-02, 2.2492e-02, 3.6802e-02,  ..., 1.9056e-02,\n",
       "            1.9221e-02, 2.5103e-02],\n",
       "           [2.1051e-02, 3.5092e-02, 4.7707e-02,  ..., 3.8491e-03,\n",
       "            2.7563e-03, 4.3026e-03],\n",
       "           ...,\n",
       "           [9.6353e-03, 1.5156e-02, 3.2689e-03,  ..., 1.2661e-01,\n",
       "            1.2740e-01, 1.5452e-01],\n",
       "           [1.1356e-02, 9.2637e-03, 2.3362e-03,  ..., 1.8588e-01,\n",
       "            1.4088e-01, 1.6467e-01],\n",
       "           [1.3307e-02, 7.1138e-03, 1.3629e-03,  ..., 2.0585e-01,\n",
       "            1.4791e-01, 1.5505e-01]],\n",
       " \n",
       "          [[5.9741e-03, 6.1543e-03, 9.7923e-03,  ..., 2.9107e-02,\n",
       "            3.6304e-02, 5.2493e-02],\n",
       "           [1.0193e-02, 2.5030e-02, 1.2165e-02,  ..., 2.1526e-02,\n",
       "            1.6436e-02, 1.9413e-02],\n",
       "           [8.2073e-03, 1.4488e-02, 4.9443e-04,  ..., 2.7076e-02,\n",
       "            3.3853e-02, 4.9162e-02],\n",
       "           ...,\n",
       "           [1.4248e-02, 9.2980e-03, 4.1094e-03,  ..., 2.8934e-02,\n",
       "            2.9999e-02, 4.3310e-02],\n",
       "           [1.7543e-02, 6.6888e-03, 3.7730e-03,  ..., 2.3646e-02,\n",
       "            2.8882e-02, 4.1385e-02],\n",
       "           [1.6392e-02, 5.4800e-03, 4.4956e-03,  ..., 2.2923e-02,\n",
       "            2.7344e-02, 3.9828e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.4199e-01, 5.5256e-03, 1.0483e-03,  ..., 9.3995e-03,\n",
       "            9.5605e-03, 8.8387e-02],\n",
       "           [9.4982e-04, 7.1428e-03, 2.3166e-03,  ..., 5.5189e-03,\n",
       "            1.7105e-02, 5.6024e-02],\n",
       "           [8.9192e-04, 2.3597e-02, 1.5028e-03,  ..., 1.2386e-02,\n",
       "            5.4667e-03, 5.0838e-01],\n",
       "           ...,\n",
       "           [1.4852e-03, 8.0049e-03, 3.2262e-03,  ..., 3.4657e-03,\n",
       "            4.3462e-02, 1.8667e-01],\n",
       "           [4.1714e-03, 5.3553e-03, 2.0588e-03,  ..., 2.3584e-02,\n",
       "            2.4255e-02, 2.4506e-01],\n",
       "           [3.7810e-04, 9.8949e-05, 7.7552e-05,  ..., 2.5972e-04,\n",
       "            1.1988e-03, 9.8155e-01]],\n",
       " \n",
       "          [[1.5895e-01, 3.0841e-04, 1.0705e-05,  ..., 2.2028e-05,\n",
       "            2.2179e-02, 7.9556e-01],\n",
       "           [3.6024e-03, 7.6488e-04, 3.6543e-08,  ..., 2.8614e-10,\n",
       "            5.9938e-06, 9.9296e-01],\n",
       "           [1.1635e-06, 9.9984e-01, 6.8619e-07,  ..., 4.4441e-12,\n",
       "            3.9574e-10, 1.0608e-04],\n",
       "           ...,\n",
       "           [5.6723e-03, 3.2069e-10, 5.9005e-12,  ..., 3.6374e-05,\n",
       "            5.8558e-04, 1.5232e-01],\n",
       "           [6.6515e-03, 2.4019e-08, 1.8972e-07,  ..., 7.7528e-01,\n",
       "            2.6489e-02, 1.7752e-01],\n",
       "           [3.0278e-01, 6.8516e-03, 3.3033e-03,  ..., 4.3361e-03,\n",
       "            2.3070e-02, 3.1439e-01]],\n",
       " \n",
       "          [[1.1778e-02, 8.7250e-04, 2.2813e-04,  ..., 8.7709e-04,\n",
       "            4.8439e-03, 6.8484e-01],\n",
       "           [3.4115e-03, 1.4667e-02, 4.4327e-04,  ..., 3.5442e-06,\n",
       "            6.2476e-05, 9.0994e-01],\n",
       "           [1.3323e-03, 4.1481e-03, 6.8652e-04,  ..., 8.4258e-06,\n",
       "            6.0885e-05, 9.8143e-01],\n",
       "           ...,\n",
       "           [1.5607e-02, 1.1825e-05, 1.3132e-07,  ..., 2.5006e-03,\n",
       "            6.2175e-03, 7.7241e-01],\n",
       "           [1.4727e-02, 8.6670e-06, 2.1493e-06,  ..., 1.0228e-03,\n",
       "            7.4769e-03, 4.8410e-01],\n",
       "           [2.9844e-03, 1.5092e-03, 8.5224e-04,  ..., 1.1939e-03,\n",
       "            9.0208e-04, 9.4300e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.1667e-02, 1.5550e-03, 7.9283e-04,  ..., 3.9297e-03,\n",
       "            7.0886e-04, 8.3138e-01],\n",
       "           [2.7755e-03, 1.3230e-03, 1.1574e-03,  ..., 1.2585e-06,\n",
       "            1.3487e-05, 9.4919e-01],\n",
       "           [1.5016e-02, 2.8642e-01, 4.8844e-02,  ..., 4.0177e-06,\n",
       "            6.4969e-05, 3.4318e-01],\n",
       "           ...,\n",
       "           [1.8149e-03, 6.0051e-07, 1.2981e-07,  ..., 1.8091e-03,\n",
       "            6.5489e-03, 9.8410e-01],\n",
       "           [2.7863e-02, 1.8655e-05, 4.0898e-05,  ..., 5.2558e-02,\n",
       "            4.1193e-02, 7.8192e-01],\n",
       "           [1.5816e-02, 7.1748e-04, 1.0487e-03,  ..., 7.3869e-04,\n",
       "            3.6476e-03, 8.7800e-01]],\n",
       " \n",
       "          [[1.9243e-02, 2.5982e-03, 8.2491e-04,  ..., 8.5516e-03,\n",
       "            1.9233e-02, 6.9971e-01],\n",
       "           [3.0521e-02, 1.9360e-03, 5.9779e-04,  ..., 4.2837e-07,\n",
       "            4.6920e-04, 9.5824e-01],\n",
       "           [4.1946e-02, 1.7097e-01, 2.2803e-02,  ..., 3.3770e-06,\n",
       "            1.0197e-03, 7.3446e-01],\n",
       "           ...,\n",
       "           [1.4442e-02, 1.5543e-07, 3.2509e-06,  ..., 1.0123e-03,\n",
       "            1.1686e-02, 9.6096e-01],\n",
       "           [1.4415e-02, 5.4494e-06, 1.6282e-05,  ..., 8.8373e-02,\n",
       "            1.1316e-01, 6.8058e-01],\n",
       "           [2.0060e-02, 1.1818e-03, 1.2808e-03,  ..., 5.2649e-04,\n",
       "            1.6553e-02, 8.8490e-01]],\n",
       " \n",
       "          [[3.2357e-02, 1.1735e-02, 4.4178e-03,  ..., 3.0947e-04,\n",
       "            2.2536e-03, 6.0719e-01],\n",
       "           [1.2529e-02, 7.2324e-03, 3.7662e-03,  ..., 6.3643e-06,\n",
       "            9.3826e-05, 7.0845e-01],\n",
       "           [8.2742e-04, 3.6909e-04, 2.8422e-05,  ..., 1.4553e-06,\n",
       "            1.9447e-05, 9.9280e-01],\n",
       "           ...,\n",
       "           [3.1916e-03, 3.7632e-06, 2.4067e-06,  ..., 3.9171e-04,\n",
       "            2.0829e-03, 9.8892e-01],\n",
       "           [1.4084e-02, 1.1160e-04, 1.0893e-04,  ..., 2.6911e-03,\n",
       "            1.6069e-02, 9.0770e-01],\n",
       "           [1.9637e-02, 1.4899e-04, 1.2754e-04,  ..., 4.5730e-04,\n",
       "            2.0841e-03, 9.5946e-01]]],\n",
       " \n",
       " \n",
       "         [[[6.3631e-01, 4.8817e-03, 6.1501e-04,  ..., 2.9834e-02,\n",
       "            2.8943e-02, 5.2957e-02],\n",
       "           [1.1892e-03, 3.8991e-03, 6.1380e-03,  ..., 5.7753e-03,\n",
       "            4.7813e-03, 7.9346e-03],\n",
       "           [1.6799e-03, 1.8136e-02, 4.2729e-03,  ..., 1.0870e-02,\n",
       "            8.0459e-03, 1.4337e-02],\n",
       "           ...,\n",
       "           [9.9536e-04, 2.3765e-03, 5.7101e-04,  ..., 2.4209e-02,\n",
       "            2.8767e-02, 4.2570e-02],\n",
       "           [2.9816e-04, 1.1092e-03, 2.3326e-04,  ..., 2.0524e-02,\n",
       "            2.3332e-02, 3.5700e-02],\n",
       "           [4.1311e-04, 1.6979e-03, 3.6395e-04,  ..., 2.4269e-02,\n",
       "            2.7137e-02, 4.0041e-02]],\n",
       " \n",
       "          [[1.7009e-01, 5.3314e-03, 2.2461e-05,  ..., 3.5393e-03,\n",
       "            1.8940e-03, 1.4682e-02],\n",
       "           [1.5604e-02, 2.3231e-03, 2.2536e-05,  ..., 4.0608e-06,\n",
       "            3.6790e-06, 5.9734e-05],\n",
       "           [2.8384e-03, 7.8694e-01, 1.6722e-02,  ..., 4.7318e-08,\n",
       "            2.6614e-08, 9.3630e-07],\n",
       "           ...,\n",
       "           [1.5625e-02, 2.1072e-07, 3.6773e-11,  ..., 9.7585e-03,\n",
       "            1.1018e-05, 5.7282e-06],\n",
       "           [1.1064e-02, 3.3457e-07, 2.3651e-09,  ..., 7.0296e-01,\n",
       "            1.1029e-01, 1.1472e-03],\n",
       "           [1.5279e-03, 1.7039e-07, 7.0981e-10,  ..., 1.7069e-03,\n",
       "            8.7075e-01, 9.6876e-02]],\n",
       " \n",
       "          [[1.0155e-02, 1.4479e-03, 8.8044e-04,  ..., 2.0900e-02,\n",
       "            2.7972e-02, 3.4602e-02],\n",
       "           [4.8038e-03, 2.5699e-02, 1.2166e-02,  ..., 4.1213e-04,\n",
       "            8.2903e-05, 7.2820e-05],\n",
       "           [1.2325e-02, 2.8358e-02, 1.8990e-02,  ..., 1.6108e-04,\n",
       "            1.7706e-05, 1.7983e-05],\n",
       "           ...,\n",
       "           [8.5396e-03, 9.9440e-05, 4.9734e-05,  ..., 3.1357e-02,\n",
       "            2.7967e-02, 2.9073e-02],\n",
       "           [1.2093e-02, 4.9166e-05, 2.1343e-05,  ..., 9.6345e-02,\n",
       "            5.5335e-02, 4.2946e-02],\n",
       "           [1.1224e-02, 5.7207e-05, 2.5702e-05,  ..., 1.4497e-01,\n",
       "            8.8823e-02, 6.2009e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.5191e-02, 1.2466e-03, 1.3085e-04,  ..., 7.3637e-03,\n",
       "            8.3149e-03, 1.2395e-02],\n",
       "           [4.4843e-03, 9.5289e-03, 3.0140e-02,  ..., 2.0602e-04,\n",
       "            1.3381e-04, 1.8606e-04],\n",
       "           [7.0859e-03, 1.7399e-02, 1.6687e-01,  ..., 7.7359e-05,\n",
       "            7.0286e-05, 1.1892e-04],\n",
       "           ...,\n",
       "           [1.7245e-02, 3.1973e-05, 1.5955e-06,  ..., 2.5496e-01,\n",
       "            2.1388e-01, 7.9846e-02],\n",
       "           [3.6568e-03, 3.5515e-06, 1.9239e-07,  ..., 2.3381e-01,\n",
       "            3.9918e-01, 1.5240e-01],\n",
       "           [3.4651e-03, 3.0538e-06, 2.5637e-07,  ..., 9.7453e-02,\n",
       "            4.2313e-01, 2.9176e-01]],\n",
       " \n",
       "          [[1.5781e-02, 3.0591e-03, 8.3912e-04,  ..., 1.5180e-02,\n",
       "            1.4927e-02, 2.3250e-02],\n",
       "           [2.1838e-02, 1.1096e-02, 5.9103e-03,  ..., 7.1089e-04,\n",
       "            2.4681e-04, 5.4705e-04],\n",
       "           [1.8248e-02, 2.2788e-02, 1.0253e-02,  ..., 3.6099e-05,\n",
       "            1.6908e-05, 4.1046e-05],\n",
       "           ...,\n",
       "           [3.1091e-02, 4.0302e-05, 5.8277e-06,  ..., 1.0728e-01,\n",
       "            2.6027e-02, 9.5009e-03],\n",
       "           [1.4309e-02, 1.4364e-05, 1.5364e-06,  ..., 2.0959e-01,\n",
       "            1.0233e-01, 2.4232e-02],\n",
       "           [8.8455e-03, 1.2824e-05, 1.3537e-06,  ..., 8.3143e-02,\n",
       "            1.2555e-01, 5.0464e-02]],\n",
       " \n",
       "          [[2.8292e-02, 2.0154e-02, 8.2885e-03,  ..., 1.0632e-02,\n",
       "            8.3499e-03, 1.3199e-02],\n",
       "           [3.6784e-03, 1.4447e-02, 6.7856e-02,  ..., 1.8885e-04,\n",
       "            2.0392e-04, 3.0572e-04],\n",
       "           [4.7402e-03, 1.1559e-02, 3.3408e-02,  ..., 1.1527e-04,\n",
       "            1.1852e-04, 1.7447e-04],\n",
       "           ...,\n",
       "           [8.9452e-03, 3.0757e-05, 6.1329e-06,  ..., 2.4885e-02,\n",
       "            1.3647e-01, 3.0951e-01],\n",
       "           [8.8811e-03, 7.3028e-06, 1.1086e-06,  ..., 1.5263e-02,\n",
       "            5.5791e-02, 1.3228e-01],\n",
       "           [1.2799e-02, 1.0712e-05, 1.7988e-06,  ..., 1.3889e-02,\n",
       "            3.7804e-02, 8.1627e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.3829e-03, 3.1136e-03, 5.0756e-04,  ..., 4.7941e-02,\n",
       "            8.5587e-02, 1.0262e-01],\n",
       "           [1.3036e-02, 5.8154e-03, 3.3897e-04,  ..., 9.9701e-02,\n",
       "            1.9183e-02, 3.6789e-01],\n",
       "           [1.3926e-02, 1.4564e-03, 2.4694e-03,  ..., 2.8023e-03,\n",
       "            1.5717e-02, 8.0678e-01],\n",
       "           ...,\n",
       "           [1.7097e-02, 1.2939e-03, 4.3715e-05,  ..., 3.4333e-02,\n",
       "            3.6685e-02, 6.1947e-01],\n",
       "           [2.4250e-02, 1.2412e-03, 7.7435e-05,  ..., 1.7668e-02,\n",
       "            8.9245e-02, 3.3530e-01],\n",
       "           [1.3083e-02, 1.0006e-03, 1.0116e-03,  ..., 8.3214e-04,\n",
       "            1.3748e-03, 8.2061e-01]],\n",
       " \n",
       "          [[2.0891e-02, 6.4903e-05, 1.0041e-05,  ..., 5.4092e-03,\n",
       "            4.5412e-01, 4.3891e-01],\n",
       "           [5.7725e-02, 1.6354e-03, 5.7281e-06,  ..., 5.4952e-08,\n",
       "            1.4493e-05, 9.2943e-01],\n",
       "           [1.3320e-04, 9.0128e-01, 7.0369e-04,  ..., 3.9745e-07,\n",
       "            5.7454e-07, 9.5173e-02],\n",
       "           ...,\n",
       "           [9.2836e-05, 4.6520e-07, 1.1373e-07,  ..., 6.7796e-04,\n",
       "            3.9040e-02, 4.1704e-01],\n",
       "           [1.1760e-04, 2.1477e-07, 9.1107e-08,  ..., 4.9360e-02,\n",
       "            1.2479e-01, 8.1632e-01],\n",
       "           [3.3812e-03, 2.3053e-05, 1.2927e-05,  ..., 1.0520e-04,\n",
       "            1.2155e-02, 9.7643e-01]],\n",
       " \n",
       "          [[8.7185e-02, 2.4723e-04, 2.3770e-04,  ..., 2.7683e-04,\n",
       "            3.7101e-03, 8.7956e-01],\n",
       "           [2.9879e-03, 1.3797e-03, 8.1939e-05,  ..., 3.1443e-07,\n",
       "            7.0794e-06, 9.9239e-01],\n",
       "           [2.9877e-03, 2.4254e-02, 4.0877e-03,  ..., 5.5563e-06,\n",
       "            1.0307e-05, 9.6510e-01],\n",
       "           ...,\n",
       "           [4.0991e-03, 8.0554e-07, 1.6649e-07,  ..., 2.8561e-04,\n",
       "            1.5041e-03, 9.8874e-01],\n",
       "           [8.7485e-03, 7.8183e-07, 2.1395e-06,  ..., 6.1395e-03,\n",
       "            8.8436e-02, 8.7902e-01],\n",
       "           [6.7218e-02, 1.7322e-03, 1.0895e-03,  ..., 7.3078e-04,\n",
       "            5.2105e-03, 8.2225e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.5977e-02, 2.5444e-04, 1.7201e-05,  ..., 3.7765e-02,\n",
       "            1.6232e-01, 5.8877e-01],\n",
       "           [2.5609e-06, 2.5205e-04, 9.9946e-01,  ..., 3.7423e-09,\n",
       "            3.0514e-08, 5.8367e-05],\n",
       "           [2.6785e-10, 3.7806e-09, 6.9019e-07,  ..., 8.0961e-13,\n",
       "            5.7116e-13, 3.0192e-09],\n",
       "           ...,\n",
       "           [8.8076e-04, 1.0449e-09, 7.6437e-09,  ..., 2.2749e-04,\n",
       "            8.4336e-01, 1.5331e-01],\n",
       "           [1.6149e-04, 3.6978e-09, 6.1887e-11,  ..., 8.3497e-05,\n",
       "            4.1176e-02, 9.5826e-01],\n",
       "           [6.1867e-02, 2.9458e-03, 7.4153e-03,  ..., 7.4995e-03,\n",
       "            1.8519e-02, 7.1888e-01]],\n",
       " \n",
       "          [[1.4068e-02, 9.7056e-03, 1.7503e-03,  ..., 6.3729e-03,\n",
       "            4.7148e-03, 6.9092e-01],\n",
       "           [6.8979e-04, 4.0037e-03, 1.4236e-01,  ..., 1.0248e-05,\n",
       "            3.8695e-05, 8.1632e-01],\n",
       "           [7.2144e-04, 4.2094e-04, 1.9586e-03,  ..., 4.4123e-06,\n",
       "            1.3189e-05, 8.3225e-01],\n",
       "           ...,\n",
       "           [1.6718e-03, 1.1456e-06, 2.5913e-06,  ..., 2.0972e-02,\n",
       "            6.7122e-02, 8.9033e-01],\n",
       "           [1.3756e-03, 1.9503e-05, 1.8371e-06,  ..., 2.7903e-03,\n",
       "            1.4717e-02, 9.5007e-01],\n",
       "           [1.3969e-02, 5.5703e-04, 1.9197e-03,  ..., 1.2600e-03,\n",
       "            5.1953e-03, 8.5577e-01]],\n",
       " \n",
       "          [[5.9169e-01, 1.5948e-03, 1.2196e-03,  ..., 6.6840e-04,\n",
       "            1.5574e-03, 2.9346e-01],\n",
       "           [2.6332e-03, 1.4363e-01, 3.8601e-03,  ..., 2.1722e-03,\n",
       "            1.2307e-03, 1.9857e-01],\n",
       "           [5.6281e-03, 1.3031e-01, 2.2666e-02,  ..., 8.7579e-04,\n",
       "            5.7123e-04, 5.5269e-01],\n",
       "           ...,\n",
       "           [3.9028e-03, 3.0554e-03, 8.9566e-04,  ..., 1.6317e-01,\n",
       "            7.3951e-02, 2.1365e-01],\n",
       "           [6.7448e-03, 1.7035e-03, 6.2503e-04,  ..., 2.1738e-02,\n",
       "            6.1356e-02, 6.0622e-01],\n",
       "           [1.6303e-02, 9.1941e-04, 2.8809e-04,  ..., 3.1423e-03,\n",
       "            8.6451e-03, 8.8220e-01]]],\n",
       " \n",
       " \n",
       "         [[[5.6187e-03, 1.6870e-02, 4.7645e-03,  ..., 1.5545e-01,\n",
       "            9.5927e-02, 1.5245e-01],\n",
       "           [7.3312e-02, 3.4784e-02, 1.5142e-02,  ..., 9.9475e-03,\n",
       "            2.2831e-02, 8.9233e-02],\n",
       "           [2.5637e-02, 4.6225e-02, 6.0918e-02,  ..., 3.2178e-03,\n",
       "            7.8345e-03, 1.9774e-02],\n",
       "           ...,\n",
       "           [4.5604e-03, 1.9995e-03, 1.6958e-04,  ..., 3.2837e-01,\n",
       "            1.4157e-01, 2.2223e-01],\n",
       "           [4.1282e-03, 1.5316e-03, 1.2161e-04,  ..., 2.4307e-01,\n",
       "            1.8316e-01, 2.1897e-01],\n",
       "           [2.9458e-03, 1.0443e-03, 5.7981e-05,  ..., 2.1573e-01,\n",
       "            1.9256e-01, 2.7161e-01]],\n",
       " \n",
       "          [[3.4486e-02, 2.5840e-04, 6.6367e-06,  ..., 4.6515e-03,\n",
       "            1.0819e-02, 9.1267e-02],\n",
       "           [9.8822e-02, 1.8067e-02, 1.0768e-04,  ..., 4.5352e-05,\n",
       "            3.1407e-05, 3.6121e-04],\n",
       "           [9.8312e-03, 2.9041e-01, 1.2378e-02,  ..., 8.5349e-06,\n",
       "            9.1048e-06, 6.7393e-05],\n",
       "           ...,\n",
       "           [2.8158e-03, 2.9846e-06, 3.8024e-09,  ..., 8.3547e-03,\n",
       "            2.0911e-04, 1.1527e-04],\n",
       "           [7.1372e-03, 2.8061e-05, 3.1216e-07,  ..., 3.4278e-01,\n",
       "            3.9929e-01, 6.9360e-02],\n",
       "           [2.2559e-03, 1.1164e-05, 1.8500e-07,  ..., 3.0004e-02,\n",
       "            3.9644e-01, 3.1542e-01]],\n",
       " \n",
       "          [[6.4378e-02, 4.6450e-04, 2.9709e-04,  ..., 4.4089e-03,\n",
       "            2.9942e-03, 7.1718e-03],\n",
       "           [2.6203e-03, 1.6771e-02, 9.0345e-03,  ..., 3.5801e-04,\n",
       "            3.5779e-04, 1.0483e-03],\n",
       "           [2.4098e-03, 2.0796e-02, 1.7329e-02,  ..., 2.5651e-05,\n",
       "            2.3308e-05, 8.8651e-05],\n",
       "           ...,\n",
       "           [6.9225e-03, 3.1455e-05, 2.4614e-06,  ..., 1.0092e-01,\n",
       "            8.2977e-03, 4.0188e-03],\n",
       "           [6.1790e-04, 4.6086e-05, 3.0032e-06,  ..., 3.9043e-01,\n",
       "            3.6699e-01, 7.9691e-02],\n",
       "           [8.4801e-04, 4.0498e-05, 2.1629e-06,  ..., 1.0224e-01,\n",
       "            4.2630e-01, 2.5048e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.9865e-02, 4.4865e-05, 7.3321e-06,  ..., 9.7534e-02,\n",
       "            2.3887e-01, 2.5744e-01],\n",
       "           [2.4279e-03, 3.2886e-02, 7.4686e-01,  ..., 1.5229e-05,\n",
       "            1.1833e-05, 4.8702e-05],\n",
       "           [1.7324e-05, 1.4349e-04, 3.7617e-03,  ..., 3.5489e-09,\n",
       "            6.0439e-09, 2.5500e-08],\n",
       "           ...,\n",
       "           [1.2906e-05, 7.6095e-08, 7.0011e-08,  ..., 3.2213e-02,\n",
       "            8.8773e-01, 7.5644e-02],\n",
       "           [4.8585e-06, 3.0973e-08, 5.6310e-09,  ..., 1.0936e-04,\n",
       "            1.1736e-01, 8.3235e-01],\n",
       "           [1.8968e-05, 5.3649e-08, 2.1955e-09,  ..., 4.2430e-06,\n",
       "            2.5563e-03, 2.5283e-01]],\n",
       " \n",
       "          [[8.4779e-03, 4.1308e-03, 1.9238e-03,  ..., 2.7084e-01,\n",
       "            2.4295e-02, 3.3018e-02],\n",
       "           [1.0730e-03, 8.6776e-03, 2.4213e-02,  ..., 1.1104e-03,\n",
       "            6.9750e-04, 2.7861e-03],\n",
       "           [7.3192e-04, 5.0097e-03, 4.9094e-02,  ..., 4.9976e-05,\n",
       "            5.7154e-05, 4.2917e-04],\n",
       "           ...,\n",
       "           [1.9035e-04, 5.3550e-05, 1.4230e-05,  ..., 3.4532e-02,\n",
       "            5.5527e-01, 3.0077e-01],\n",
       "           [3.5415e-04, 1.3890e-05, 8.1482e-06,  ..., 2.2123e-03,\n",
       "            1.9961e-01, 7.0799e-01],\n",
       "           [1.8352e-03, 2.3346e-05, 1.6914e-05,  ..., 8.7039e-04,\n",
       "            3.8941e-02, 4.2922e-01]],\n",
       " \n",
       "          [[5.6197e-01, 2.7068e-03, 4.9360e-03,  ..., 7.9977e-03,\n",
       "            9.6519e-03, 1.9016e-02],\n",
       "           [4.2461e-03, 6.6581e-02, 2.5174e-01,  ..., 8.1988e-03,\n",
       "            6.0066e-03, 2.9325e-02],\n",
       "           [5.6688e-03, 5.5293e-02, 1.2947e-01,  ..., 6.8932e-03,\n",
       "            4.1371e-03, 1.1998e-02],\n",
       "           ...,\n",
       "           [3.9046e-03, 3.3447e-03, 1.7303e-03,  ..., 7.7906e-02,\n",
       "            1.8300e-01, 2.9950e-01],\n",
       "           [2.8726e-03, 1.9702e-03, 1.4470e-03,  ..., 5.8310e-02,\n",
       "            1.0662e-01, 1.9232e-01],\n",
       "           [3.2295e-03, 2.0355e-03, 1.0537e-03,  ..., 5.5632e-02,\n",
       "            8.3926e-02, 1.6994e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.7307e-03, 4.1686e-05, 2.1869e-05,  ..., 3.0684e-03,\n",
       "            5.9373e-01, 3.2806e-01],\n",
       "           [2.4411e-03, 6.0921e-03, 1.2518e-05,  ..., 1.6045e-07,\n",
       "            2.7530e-05, 9.8940e-01],\n",
       "           [4.5089e-04, 4.1598e-01, 1.2282e-02,  ..., 5.1237e-06,\n",
       "            1.7964e-04, 5.5106e-01],\n",
       "           ...,\n",
       "           [1.7904e-04, 1.3392e-06, 6.3478e-08,  ..., 1.1744e-03,\n",
       "            1.0278e-03, 9.0607e-01],\n",
       "           [2.1273e-04, 1.9708e-06, 2.4382e-06,  ..., 2.1057e-02,\n",
       "            9.1413e-02, 8.5374e-01],\n",
       "           [2.0608e-03, 1.0463e-03, 1.7745e-03,  ..., 1.2267e-03,\n",
       "            1.1304e-02, 9.0768e-01]],\n",
       " \n",
       "          [[2.5808e-02, 3.0933e-04, 5.8430e-05,  ..., 2.0823e-04,\n",
       "            7.0637e-03, 9.5276e-01],\n",
       "           [2.6302e-03, 2.1299e-03, 3.6648e-06,  ..., 4.1712e-06,\n",
       "            1.0577e-03, 9.6712e-01],\n",
       "           [2.1458e-03, 4.8567e-01, 8.2208e-04,  ..., 4.0173e-05,\n",
       "            8.3291e-05, 4.2867e-01],\n",
       "           ...,\n",
       "           [8.8021e-04, 4.6151e-06, 2.0858e-07,  ..., 1.2319e-03,\n",
       "            2.1404e-04, 8.6444e-02],\n",
       "           [6.8477e-03, 5.5565e-05, 4.7093e-05,  ..., 2.1996e-02,\n",
       "            2.1113e-02, 8.9220e-01],\n",
       "           [8.7102e-03, 8.1499e-03, 1.5414e-03,  ..., 1.7389e-03,\n",
       "            2.0811e-03, 7.6532e-01]],\n",
       " \n",
       "          [[6.2301e-03, 3.4790e-06, 1.3691e-05,  ..., 4.5256e-06,\n",
       "            4.1781e-03, 9.7764e-01],\n",
       "           [3.4159e-04, 1.9139e-03, 2.2999e-03,  ..., 2.0939e-06,\n",
       "            9.1893e-05, 9.5887e-01],\n",
       "           [1.1513e-03, 4.1996e-03, 1.8571e-03,  ..., 2.5692e-05,\n",
       "            1.6251e-04, 8.3621e-01],\n",
       "           ...,\n",
       "           [2.3824e-03, 5.1343e-06, 6.2918e-07,  ..., 1.7992e-03,\n",
       "            9.9065e-03, 9.3819e-01],\n",
       "           [7.7764e-03, 1.2623e-05, 1.5521e-05,  ..., 8.2317e-04,\n",
       "            4.5879e-02, 8.6364e-01],\n",
       "           [1.9394e-02, 2.6806e-03, 2.3924e-03,  ..., 4.3645e-03,\n",
       "            1.7369e-02, 7.1328e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.1653e-02, 4.1066e-05, 1.0302e-05,  ..., 6.3661e-05,\n",
       "            7.0796e-03, 9.3259e-01],\n",
       "           [1.5522e-03, 1.9568e-04, 8.1395e-06,  ..., 2.7384e-08,\n",
       "            4.0881e-04, 9.9576e-01],\n",
       "           [9.3020e-04, 6.2853e-03, 2.1596e-04,  ..., 1.5336e-07,\n",
       "            4.2888e-04, 9.8920e-01],\n",
       "           ...,\n",
       "           [1.9053e-03, 4.9862e-07, 3.2484e-07,  ..., 8.5162e-05,\n",
       "            3.0866e-03, 9.8983e-01],\n",
       "           [2.8052e-02, 3.2609e-06, 5.4894e-06,  ..., 1.2697e-02,\n",
       "            4.4022e-01, 4.3855e-01],\n",
       "           [1.5781e-02, 2.0380e-03, 1.9160e-03,  ..., 1.7911e-03,\n",
       "            2.3970e-02, 7.7508e-01]],\n",
       " \n",
       "          [[2.1788e-02, 4.2165e-04, 2.4852e-04,  ..., 3.0086e-03,\n",
       "            2.6397e-03, 9.3098e-01],\n",
       "           [2.8772e-04, 7.3268e-03, 2.8098e-03,  ..., 2.3343e-05,\n",
       "            4.2279e-05, 9.4390e-01],\n",
       "           [4.0036e-04, 2.1780e-02, 1.0682e-02,  ..., 1.3330e-05,\n",
       "            7.6556e-05, 9.2891e-01],\n",
       "           ...,\n",
       "           [4.4129e-03, 1.9682e-05, 3.3152e-05,  ..., 5.1552e-02,\n",
       "            6.0665e-03, 4.9087e-01],\n",
       "           [1.3139e-03, 5.0542e-05, 2.3653e-05,  ..., 3.8124e-02,\n",
       "            1.0892e-02, 3.3821e-01],\n",
       "           [6.3655e-03, 9.7392e-04, 3.2701e-04,  ..., 1.7436e-03,\n",
       "            2.0615e-03, 9.3169e-01]],\n",
       " \n",
       "          [[2.2297e-02, 3.0735e-05, 1.2557e-05,  ..., 3.3256e-03,\n",
       "            2.2539e-02, 5.5161e-01],\n",
       "           [6.1339e-05, 8.4810e-05, 5.7171e-06,  ..., 2.3356e-07,\n",
       "            4.1106e-06, 9.9901e-01],\n",
       "           [1.0117e-04, 8.2643e-03, 9.5074e-04,  ..., 8.0896e-06,\n",
       "            1.1347e-05, 9.8363e-01],\n",
       "           ...,\n",
       "           [5.3720e-04, 1.9581e-07, 1.1967e-07,  ..., 1.4680e-03,\n",
       "            7.0808e-04, 9.8157e-01],\n",
       "           [6.0522e-03, 3.1298e-06, 4.1756e-06,  ..., 1.1535e-02,\n",
       "            1.0271e-02, 8.4037e-01],\n",
       "           [3.3135e-03, 7.4139e-04, 4.2878e-04,  ..., 1.8122e-03,\n",
       "            1.4781e-03, 9.0672e-01]]],\n",
       " \n",
       " \n",
       "         [[[9.6894e-03, 4.3538e-04, 5.3440e-05,  ..., 7.0273e-03,\n",
       "            4.5140e-03, 2.4737e-02],\n",
       "           [3.0039e-02, 1.4357e-02, 9.2244e-04,  ..., 3.7792e-04,\n",
       "            2.1222e-04, 3.1948e-03],\n",
       "           [2.0084e-02, 3.6489e-01, 2.3220e-02,  ..., 1.6402e-04,\n",
       "            1.5097e-04, 3.5185e-03],\n",
       "           ...,\n",
       "           [2.3540e-03, 9.0012e-05, 1.4708e-06,  ..., 8.0963e-03,\n",
       "            1.5859e-03, 4.4930e-03],\n",
       "           [7.0050e-03, 6.7973e-04, 1.0936e-05,  ..., 2.1090e-02,\n",
       "            2.8466e-02, 8.0428e-02],\n",
       "           [7.4817e-03, 5.9546e-04, 3.2526e-05,  ..., 1.4788e-02,\n",
       "            4.1239e-02, 1.8017e-01]],\n",
       " \n",
       "          [[5.6769e-02, 5.5541e-03, 7.3756e-05,  ..., 1.0694e-02,\n",
       "            7.0895e-03, 5.2827e-02],\n",
       "           [2.5738e-03, 8.5586e-03, 1.5006e-03,  ..., 3.0713e-03,\n",
       "            3.3138e-03, 2.9356e-02],\n",
       "           [2.3945e-03, 6.7364e-02, 1.4468e-02,  ..., 3.7864e-04,\n",
       "            8.0472e-04, 1.4887e-02],\n",
       "           ...,\n",
       "           [8.1974e-03, 4.4400e-04, 1.3386e-05,  ..., 1.1367e-01,\n",
       "            4.9084e-02, 1.0203e-01],\n",
       "           [2.9425e-03, 7.6817e-04, 2.1309e-05,  ..., 6.3191e-02,\n",
       "            1.4473e-01, 2.2821e-01],\n",
       "           [3.6627e-03, 8.7515e-04, 5.6115e-05,  ..., 3.4686e-02,\n",
       "            1.5433e-01, 3.1653e-01]],\n",
       " \n",
       "          [[1.2390e-02, 6.1306e-05, 4.1067e-04,  ..., 1.5317e-01,\n",
       "            5.1607e-02, 4.9690e-02],\n",
       "           [1.5139e-03, 4.4896e-02, 1.4643e-02,  ..., 6.4953e-03,\n",
       "            8.8097e-03, 2.4148e-02],\n",
       "           [1.8692e-04, 2.8948e-02, 1.6910e-02,  ..., 2.5252e-04,\n",
       "            2.6392e-04, 1.8240e-03],\n",
       "           ...,\n",
       "           [3.0637e-03, 3.1281e-04, 1.2113e-04,  ..., 3.8315e-02,\n",
       "            2.0941e-01, 2.8682e-01],\n",
       "           [2.1971e-03, 4.6270e-04, 6.8324e-05,  ..., 5.4161e-02,\n",
       "            1.6544e-01, 3.2935e-01],\n",
       "           [6.8392e-03, 1.3903e-03, 3.6413e-04,  ..., 5.8820e-02,\n",
       "            1.3155e-01, 3.0331e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.6591e-02, 2.9403e-04, 1.6680e-05,  ..., 1.3211e-03,\n",
       "            2.1109e-03, 5.2672e-03],\n",
       "           [8.5687e-04, 1.8888e-02, 4.7353e-03,  ..., 4.9895e-04,\n",
       "            4.4888e-04, 3.3177e-03],\n",
       "           [3.5511e-04, 1.0029e-02, 1.0309e-02,  ..., 1.6007e-04,\n",
       "            1.1650e-04, 1.1210e-03],\n",
       "           ...,\n",
       "           [6.9674e-03, 9.2212e-04, 2.6183e-05,  ..., 2.6953e-02,\n",
       "            1.0029e-02, 2.6343e-02],\n",
       "           [3.2631e-03, 3.7733e-04, 7.1167e-06,  ..., 2.9553e-02,\n",
       "            2.3703e-02, 3.5465e-02],\n",
       "           [2.7673e-03, 2.6209e-04, 2.7515e-05,  ..., 1.0346e-02,\n",
       "            1.6506e-02, 3.4821e-02]],\n",
       " \n",
       "          [[1.3350e-02, 1.3409e-04, 5.4246e-05,  ..., 3.3545e-02,\n",
       "            6.7508e-02, 1.9395e-01],\n",
       "           [7.9782e-04, 1.9910e-02, 5.1062e-03,  ..., 1.3965e-03,\n",
       "            1.6749e-03, 1.6524e-02],\n",
       "           [6.7254e-04, 7.2763e-02, 7.2712e-02,  ..., 3.6102e-04,\n",
       "            5.5866e-04, 6.2514e-03],\n",
       "           ...,\n",
       "           [4.1709e-03, 2.3529e-04, 3.0313e-05,  ..., 1.2217e-02,\n",
       "            1.5517e-02, 1.0330e-01],\n",
       "           [2.0877e-03, 6.0550e-05, 4.9080e-06,  ..., 3.5036e-02,\n",
       "            4.2498e-02, 1.0834e-01],\n",
       "           [4.3866e-03, 1.4709e-04, 1.9500e-05,  ..., 1.0728e-01,\n",
       "            1.1068e-01, 1.4384e-01]],\n",
       " \n",
       "          [[1.6586e-02, 2.5565e-05, 9.0662e-05,  ..., 2.0465e-01,\n",
       "            6.8759e-02, 1.0398e-01],\n",
       "           [3.4304e-04, 2.1203e-03, 4.8329e-03,  ..., 4.2428e-04,\n",
       "            4.8891e-04, 5.3419e-03],\n",
       "           [3.9762e-04, 4.5582e-03, 7.6000e-03,  ..., 3.4754e-04,\n",
       "            6.5306e-04, 8.7860e-03],\n",
       "           ...,\n",
       "           [1.2345e-03, 5.1041e-05, 1.3617e-05,  ..., 1.3827e-02,\n",
       "            1.2667e-02, 3.5704e-02],\n",
       "           [4.9506e-04, 2.2963e-05, 2.7647e-06,  ..., 3.0241e-02,\n",
       "            4.4020e-02, 6.5634e-02],\n",
       "           [7.7494e-04, 5.1068e-05, 1.9580e-05,  ..., 2.6818e-02,\n",
       "            6.0301e-02, 1.2843e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.1404e-02, 3.4726e-05, 1.9888e-06,  ..., 3.5679e-04,\n",
       "            2.7905e-02, 8.8168e-01],\n",
       "           [7.7438e-03, 1.1699e-02, 9.9525e-05,  ..., 1.1190e-06,\n",
       "            5.3916e-03, 9.3412e-01],\n",
       "           [1.4031e-02, 2.7691e-01, 2.7229e-02,  ..., 5.8555e-06,\n",
       "            3.6497e-03, 5.7743e-01],\n",
       "           ...,\n",
       "           [3.2179e-04, 1.0167e-06, 1.5140e-06,  ..., 9.8500e-03,\n",
       "            1.6597e-03, 6.2087e-02],\n",
       "           [3.6783e-03, 9.4107e-06, 3.0429e-06,  ..., 3.1497e-02,\n",
       "            1.2226e-01, 5.4244e-01],\n",
       "           [1.5586e-02, 6.8864e-03, 2.8412e-03,  ..., 4.5946e-03,\n",
       "            1.9029e-02, 7.5475e-01]],\n",
       " \n",
       "          [[4.0428e-01, 1.0531e-03, 8.8368e-05,  ..., 9.9328e-05,\n",
       "            6.5414e-04, 5.7923e-01],\n",
       "           [6.4250e-04, 3.6757e-03, 1.0079e-02,  ..., 1.4492e-06,\n",
       "            1.8521e-04, 9.1845e-01],\n",
       "           [7.4242e-05, 2.2846e-03, 7.6800e-03,  ..., 3.1062e-06,\n",
       "            5.9707e-05, 9.4467e-01],\n",
       "           ...,\n",
       "           [9.8209e-04, 5.3355e-06, 2.8255e-06,  ..., 1.0776e-02,\n",
       "            2.7364e-03, 9.1154e-01],\n",
       "           [1.3175e-01, 1.3511e-03, 1.0505e-04,  ..., 2.4365e-03,\n",
       "            3.5884e-03, 6.9149e-01],\n",
       "           [6.5630e-03, 9.3316e-04, 7.8010e-04,  ..., 1.4219e-03,\n",
       "            2.2345e-03, 9.5275e-01]],\n",
       " \n",
       "          [[9.2282e-03, 2.0700e-03, 5.2320e-04,  ..., 9.6363e-03,\n",
       "            1.6026e-02, 4.1238e-02],\n",
       "           [1.8273e-02, 1.3341e-02, 3.9366e-04,  ..., 1.2230e-01,\n",
       "            3.1993e-03, 1.5816e-01],\n",
       "           [1.0172e-02, 5.5243e-03, 6.7747e-04,  ..., 1.1256e-02,\n",
       "            6.8229e-03, 7.8236e-01],\n",
       "           ...,\n",
       "           [6.3614e-03, 7.2081e-02, 1.0951e-03,  ..., 1.0387e-02,\n",
       "            1.5328e-03, 4.2696e-02],\n",
       "           [6.9185e-02, 6.0295e-03, 7.2050e-04,  ..., 5.0735e-03,\n",
       "            4.4732e-02, 2.6445e-01],\n",
       "           [8.0605e-03, 5.2741e-04, 4.3753e-04,  ..., 2.0473e-03,\n",
       "            4.5044e-03, 9.4019e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1207e-01, 7.8714e-04, 1.4658e-04,  ..., 4.2063e-05,\n",
       "            5.9108e-04, 8.5843e-01],\n",
       "           [1.7206e-03, 3.5425e-02, 2.4237e-02,  ..., 9.8070e-06,\n",
       "            7.3179e-05, 6.3739e-01],\n",
       "           [1.5059e-03, 6.6454e-03, 2.6976e-02,  ..., 1.9209e-05,\n",
       "            1.3056e-04, 5.4589e-01],\n",
       "           ...,\n",
       "           [3.5684e-03, 2.5989e-06, 1.7080e-06,  ..., 2.0470e-02,\n",
       "            1.8121e-02, 9.2524e-01],\n",
       "           [3.6369e-02, 3.0912e-03, 4.0265e-04,  ..., 2.0945e-04,\n",
       "            5.1494e-03, 7.9345e-01],\n",
       "           [4.9936e-02, 4.2959e-03, 3.0234e-03,  ..., 5.2604e-03,\n",
       "            2.4672e-03, 7.5444e-01]],\n",
       " \n",
       "          [[1.7062e-01, 1.3742e-03, 2.0154e-04,  ..., 3.7366e-04,\n",
       "            1.9035e-03, 7.8920e-01],\n",
       "           [4.4391e-04, 1.5056e-02, 3.3799e-03,  ..., 8.6852e-06,\n",
       "            9.9725e-04, 6.5018e-01],\n",
       "           [3.1139e-04, 2.8190e-04, 6.0020e-03,  ..., 5.2484e-06,\n",
       "            3.0504e-04, 9.3571e-01],\n",
       "           ...,\n",
       "           [4.3190e-03, 1.4566e-05, 1.1965e-05,  ..., 3.4230e-03,\n",
       "            6.6140e-02, 6.9236e-01],\n",
       "           [4.0701e-03, 2.1071e-04, 1.3432e-04,  ..., 3.1889e-02,\n",
       "            2.1194e-02, 7.1576e-01],\n",
       "           [6.8634e-03, 2.8369e-03, 3.3934e-03,  ..., 1.5539e-03,\n",
       "            2.4814e-03, 7.6294e-01]],\n",
       " \n",
       "          [[4.8252e-03, 1.1602e-03, 4.3498e-04,  ..., 1.0727e-03,\n",
       "            2.8001e-02, 1.9183e-01],\n",
       "           [4.1201e-02, 7.7212e-02, 7.7603e-04,  ..., 3.0257e-03,\n",
       "            2.5070e-03, 7.4742e-01],\n",
       "           [8.9436e-03, 1.0497e-02, 2.8481e-02,  ..., 1.4880e-03,\n",
       "            7.2083e-04, 8.9755e-01],\n",
       "           ...,\n",
       "           [7.7429e-04, 1.6148e-03, 3.7761e-04,  ..., 5.8736e-03,\n",
       "            1.9017e-04, 3.0443e-02],\n",
       "           [3.2511e-02, 1.4887e-02, 9.6620e-04,  ..., 1.8834e-03,\n",
       "            1.8751e-02, 9.6100e-02],\n",
       "           [7.0014e-03, 1.3494e-03, 1.2427e-03,  ..., 3.2709e-03,\n",
       "            5.7502e-03, 8.7549e-01]]],\n",
       " \n",
       " \n",
       "         [[[1.3214e-01, 7.7077e-05, 1.7136e-05,  ..., 1.0134e-02,\n",
       "            1.2874e-02, 5.9402e-02],\n",
       "           [1.7303e-04, 3.3442e-03, 3.8870e-04,  ..., 3.3397e-04,\n",
       "            6.1273e-04, 6.1501e-03],\n",
       "           [1.5145e-04, 1.6036e-02, 7.2949e-03,  ..., 2.4780e-04,\n",
       "            6.6335e-04, 1.1759e-02],\n",
       "           ...,\n",
       "           [2.3875e-03, 4.1053e-04, 2.4152e-05,  ..., 1.7995e-01,\n",
       "            8.6019e-02, 7.7655e-02],\n",
       "           [1.5956e-03, 5.5297e-04, 2.1564e-05,  ..., 1.8233e-01,\n",
       "            1.9320e-01, 9.5386e-02],\n",
       "           [3.2548e-03, 1.1779e-03, 1.0586e-04,  ..., 1.2115e-01,\n",
       "            2.0008e-01, 1.7719e-01]],\n",
       " \n",
       "          [[5.3913e-01, 1.1067e-04, 2.6309e-04,  ..., 2.0449e-02,\n",
       "            1.1414e-02, 3.0990e-02],\n",
       "           [2.9867e-04, 7.7530e-03, 1.9083e-02,  ..., 1.6559e-03,\n",
       "            1.5567e-03, 5.8972e-03],\n",
       "           [1.9868e-04, 1.6598e-02, 6.4046e-02,  ..., 1.6680e-03,\n",
       "            5.6666e-04, 3.7575e-03],\n",
       "           ...,\n",
       "           [4.1364e-03, 1.4395e-04, 1.6870e-05,  ..., 1.5343e-02,\n",
       "            1.1098e-01, 3.3467e-01],\n",
       "           [3.0137e-03, 1.2512e-04, 2.2711e-05,  ..., 5.4495e-03,\n",
       "            4.0956e-02, 1.8106e-01],\n",
       "           [6.6735e-03, 2.9367e-04, 7.6343e-05,  ..., 9.0592e-03,\n",
       "            2.1782e-02, 7.4917e-02]],\n",
       " \n",
       "          [[1.4593e-02, 3.3174e-02, 2.4922e-03,  ..., 9.4033e-02,\n",
       "            1.2557e-01, 2.1925e-01],\n",
       "           [3.1182e-01, 9.1505e-03, 6.7431e-04,  ..., 6.6219e-02,\n",
       "            8.2675e-02, 1.8030e-01],\n",
       "           [1.0197e-01, 5.6088e-03, 1.1934e-03,  ..., 2.8905e-02,\n",
       "            3.5979e-02, 1.5407e-01],\n",
       "           ...,\n",
       "           [2.2676e-02, 4.1891e-03, 1.0415e-03,  ..., 1.1984e-01,\n",
       "            9.5956e-02, 2.1177e-01],\n",
       "           [3.2598e-02, 4.9716e-03, 1.3661e-03,  ..., 5.2454e-02,\n",
       "            3.4596e-02, 1.1103e-01],\n",
       "           [3.4478e-02, 5.4778e-03, 1.5442e-03,  ..., 3.3335e-02,\n",
       "            2.5078e-02, 6.8239e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.4171e-01, 1.0393e-02, 4.9389e-03,  ..., 1.1347e-02,\n",
       "            1.1605e-02, 7.6812e-02],\n",
       "           [5.0725e-04, 7.9264e-02, 1.1142e-01,  ..., 1.4507e-02,\n",
       "            1.3849e-02, 3.5895e-02],\n",
       "           [1.8214e-04, 1.8220e-02, 5.5940e-02,  ..., 1.9375e-03,\n",
       "            1.4910e-03, 9.3391e-03],\n",
       "           ...,\n",
       "           [8.6176e-03, 1.0986e-03, 9.8073e-05,  ..., 6.4027e-02,\n",
       "            2.0281e-01, 2.6121e-01],\n",
       "           [7.4096e-03, 8.2704e-04, 1.0644e-04,  ..., 1.7968e-02,\n",
       "            1.6082e-01, 4.9899e-01],\n",
       "           [1.9178e-02, 5.0389e-03, 1.4233e-03,  ..., 1.8625e-02,\n",
       "            9.1091e-02, 4.2649e-01]],\n",
       " \n",
       "          [[1.2531e-01, 1.2129e-04, 8.9496e-05,  ..., 6.2279e-04,\n",
       "            9.3949e-04, 8.6371e-03],\n",
       "           [5.5645e-04, 8.1187e-03, 4.8207e-02,  ..., 5.1054e-04,\n",
       "            5.4537e-04, 5.5355e-03],\n",
       "           [1.1938e-04, 1.7875e-02, 4.7337e-02,  ..., 7.8667e-05,\n",
       "            1.5037e-04, 1.7091e-03],\n",
       "           ...,\n",
       "           [2.2908e-03, 1.4170e-04, 3.2508e-05,  ..., 2.5430e-02,\n",
       "            3.5186e-02, 1.0955e-01],\n",
       "           [4.0079e-03, 1.3345e-04, 1.1460e-05,  ..., 2.9153e-02,\n",
       "            4.3522e-02, 1.7112e-01],\n",
       "           [5.6468e-03, 3.5526e-04, 5.2193e-05,  ..., 2.1620e-02,\n",
       "            3.2526e-02, 1.5284e-01]],\n",
       " \n",
       "          [[1.4519e-02, 4.8534e-03, 2.7463e-03,  ..., 4.6676e-02,\n",
       "            3.7260e-02, 1.4851e-01],\n",
       "           [1.7039e-02, 3.3233e-02, 8.5945e-03,  ..., 2.7128e-03,\n",
       "            2.7691e-03, 1.7117e-02],\n",
       "           [1.2919e-02, 1.2311e-02, 2.0657e-02,  ..., 3.0439e-03,\n",
       "            2.2303e-03, 9.6237e-03],\n",
       "           ...,\n",
       "           [5.3231e-02, 4.9033e-03, 5.8100e-04,  ..., 1.3288e-01,\n",
       "            7.7213e-02, 9.1439e-02],\n",
       "           [2.9026e-02, 3.3525e-03, 2.2556e-04,  ..., 3.0660e-02,\n",
       "            4.3769e-02, 7.0109e-02],\n",
       "           [2.2238e-02, 6.0315e-03, 6.2618e-04,  ..., 2.3641e-02,\n",
       "            4.9784e-02, 1.0423e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.2096e-02, 4.1139e-03, 2.2623e-04,  ..., 1.0342e-02,\n",
       "            1.0205e-01, 3.1153e-01],\n",
       "           [3.3079e-03, 3.8837e-02, 5.1844e-02,  ..., 1.9634e-04,\n",
       "            8.1777e-04, 2.3874e-01],\n",
       "           [3.0202e-03, 2.1108e-02, 2.4358e-02,  ..., 1.2206e-04,\n",
       "            8.4020e-04, 4.4082e-01],\n",
       "           ...,\n",
       "           [1.7799e-03, 1.5355e-05, 2.4783e-05,  ..., 2.2812e-01,\n",
       "            8.1640e-04, 6.9466e-01],\n",
       "           [2.1869e-02, 5.6191e-03, 6.9059e-04,  ..., 6.5643e-03,\n",
       "            3.5886e-03, 6.0330e-01],\n",
       "           [2.7545e-02, 2.4139e-03, 1.3138e-03,  ..., 3.8925e-03,\n",
       "            1.3400e-03, 6.7434e-01]],\n",
       " \n",
       "          [[4.5451e-02, 7.8349e-03, 9.3008e-04,  ..., 3.0262e-03,\n",
       "            8.0219e-03, 3.8779e-03],\n",
       "           [3.6930e-04, 2.8890e-01, 2.6563e-04,  ..., 4.9249e-04,\n",
       "            2.7134e-04, 6.4665e-01],\n",
       "           [1.1839e-04, 1.1489e-03, 1.1315e-01,  ..., 2.2172e-05,\n",
       "            4.8428e-05, 8.7365e-01],\n",
       "           ...,\n",
       "           [1.5396e-03, 1.2230e-03, 1.0356e-04,  ..., 5.5410e-02,\n",
       "            1.6727e-03, 8.5755e-01],\n",
       "           [6.2925e-03, 1.2033e-03, 5.3691e-04,  ..., 1.4360e-04,\n",
       "            1.2877e-01, 6.5379e-01],\n",
       "           [8.3475e-03, 1.1470e-03, 7.2934e-04,  ..., 4.5812e-04,\n",
       "            6.3050e-03, 8.7362e-01]],\n",
       " \n",
       "          [[1.2882e-01, 9.9749e-04, 8.2079e-05,  ..., 2.2089e-03,\n",
       "            1.3265e-01, 4.2555e-02],\n",
       "           [4.1369e-03, 1.2203e-03, 3.0824e-04,  ..., 1.8668e-04,\n",
       "            9.1362e-04, 9.6472e-01],\n",
       "           [2.4580e-03, 4.0944e-02, 6.1259e-03,  ..., 4.4575e-04,\n",
       "            8.1544e-04, 8.8325e-01],\n",
       "           ...,\n",
       "           [1.4661e-02, 1.0836e-04, 2.5869e-04,  ..., 1.0174e-02,\n",
       "            5.9508e-04, 9.2270e-01],\n",
       "           [5.2034e-01, 5.0104e-04, 2.6936e-04,  ..., 4.0141e-03,\n",
       "            3.8111e-03, 1.1527e-01],\n",
       "           [9.1461e-03, 4.8035e-03, 5.1443e-03,  ..., 8.9492e-03,\n",
       "            6.7469e-03, 4.5784e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.8935e-01, 2.0252e-04, 8.2586e-06,  ..., 2.4377e-04,\n",
       "            1.0743e-03, 7.7110e-01],\n",
       "           [2.5018e-04, 8.2406e-03, 3.4351e-03,  ..., 2.8551e-04,\n",
       "            8.2253e-05, 8.7326e-01],\n",
       "           [6.5003e-05, 2.2008e-03, 8.4612e-04,  ..., 4.9952e-05,\n",
       "            7.8805e-06, 9.8525e-01],\n",
       "           ...,\n",
       "           [2.4810e-04, 4.7949e-05, 2.9503e-05,  ..., 4.4966e-02,\n",
       "            4.1883e-05, 9.0319e-01],\n",
       "           [9.3197e-03, 4.6193e-04, 1.6423e-04,  ..., 7.7301e-03,\n",
       "            7.0255e-03, 4.0752e-01],\n",
       "           [6.8393e-03, 2.4798e-03, 7.7129e-04,  ..., 1.7540e-03,\n",
       "            3.1365e-03, 9.0091e-01]],\n",
       " \n",
       "          [[4.3269e-02, 1.1505e-02, 2.2318e-03,  ..., 2.1851e-03,\n",
       "            8.7892e-03, 1.0406e-02],\n",
       "           [4.4717e-03, 2.4924e-02, 1.3104e-02,  ..., 2.9779e-03,\n",
       "            8.5592e-03, 2.8941e-01],\n",
       "           [4.3028e-03, 4.8292e-02, 6.1569e-03,  ..., 1.4345e-02,\n",
       "            2.6618e-03, 2.6883e-01],\n",
       "           ...,\n",
       "           [1.0286e-02, 8.7715e-03, 2.8669e-03,  ..., 1.1634e-02,\n",
       "            3.3157e-03, 5.7100e-01],\n",
       "           [1.8377e-02, 2.5327e-03, 6.8123e-04,  ..., 3.7179e-03,\n",
       "            2.8256e-03, 2.4900e-02],\n",
       "           [3.3034e-03, 4.8530e-03, 6.3959e-03,  ..., 5.4632e-03,\n",
       "            1.0340e-02, 4.8956e-01]],\n",
       " \n",
       "          [[6.8898e-03, 1.4896e-03, 3.7395e-05,  ..., 5.2205e-03,\n",
       "            6.7133e-01, 9.6517e-02],\n",
       "           [4.4406e-04, 1.8869e-03, 1.7429e-02,  ..., 5.6263e-06,\n",
       "            2.7569e-05, 9.7031e-01],\n",
       "           [9.4180e-05, 4.1543e-04, 8.9094e-03,  ..., 4.8931e-06,\n",
       "            7.5381e-05, 6.4623e-01],\n",
       "           ...,\n",
       "           [2.0007e-03, 9.3415e-07, 2.1143e-06,  ..., 9.0072e-04,\n",
       "            1.2326e-03, 9.9400e-01],\n",
       "           [1.2513e-02, 9.3693e-02, 1.4797e-03,  ..., 4.8426e-03,\n",
       "            6.3720e-03, 7.9034e-02],\n",
       "           [3.6212e-02, 1.3439e-02, 2.3232e-03,  ..., 1.0880e-02,\n",
       "            3.0786e-02, 4.1718e-01]]],\n",
       " \n",
       " \n",
       "         [[[3.5220e-02, 2.8268e-03, 1.4614e-03,  ..., 8.4578e-02,\n",
       "            5.0889e-02, 1.4624e-01],\n",
       "           [2.8310e-03, 3.5147e-02, 1.2085e-01,  ..., 3.3048e-03,\n",
       "            2.2404e-03, 9.6197e-03],\n",
       "           [8.2021e-04, 8.3911e-03, 8.6832e-02,  ..., 7.2165e-04,\n",
       "            3.4933e-04, 3.7078e-03],\n",
       "           ...,\n",
       "           [7.4923e-03, 7.5284e-04, 3.0250e-05,  ..., 4.3019e-02,\n",
       "            1.9203e-01, 4.6328e-01],\n",
       "           [9.0894e-03, 7.5782e-04, 2.5987e-05,  ..., 1.1544e-02,\n",
       "            5.8123e-02, 3.5170e-01],\n",
       "           [1.5660e-02, 1.1138e-03, 6.9253e-05,  ..., 2.1596e-02,\n",
       "            6.7182e-02, 3.0518e-01]],\n",
       " \n",
       "          [[6.5548e-02, 2.0215e-02, 2.6116e-03,  ..., 1.6361e-02,\n",
       "            1.4829e-02, 4.3405e-02],\n",
       "           [2.9904e-03, 1.2877e-01, 1.3582e-03,  ..., 3.0719e-03,\n",
       "            3.2097e-03, 8.6301e-03],\n",
       "           [1.0194e-03, 4.1410e-03, 1.5766e-01,  ..., 8.9070e-04,\n",
       "            6.7567e-04, 1.1795e-03],\n",
       "           ...,\n",
       "           [5.5540e-03, 7.9854e-04, 1.5501e-04,  ..., 4.6591e-01,\n",
       "            9.0978e-02, 1.5929e-01],\n",
       "           [2.4389e-03, 1.3503e-04, 3.1142e-05,  ..., 8.8459e-02,\n",
       "            1.6394e-01, 4.5988e-01],\n",
       "           [1.5241e-03, 1.7942e-04, 2.9274e-05,  ..., 4.7792e-02,\n",
       "            9.4020e-02, 4.1168e-01]],\n",
       " \n",
       "          [[1.0408e-01, 1.1328e-02, 4.9914e-04,  ..., 4.0302e-02,\n",
       "            4.0915e-02, 1.9937e-01],\n",
       "           [8.2869e-04, 4.1940e-02, 4.1074e-04,  ..., 6.1120e-04,\n",
       "            3.3821e-04, 2.2537e-03],\n",
       "           [2.9186e-04, 5.6438e-01, 1.9533e-02,  ..., 3.6982e-04,\n",
       "            2.1472e-04, 1.0429e-03],\n",
       "           ...,\n",
       "           [3.8774e-02, 2.6560e-03, 1.4051e-03,  ..., 3.5044e-02,\n",
       "            3.3784e-02, 1.2628e-01],\n",
       "           [3.9089e-02, 8.7710e-04, 2.7642e-04,  ..., 2.8777e-02,\n",
       "            3.5519e-02, 1.2704e-01],\n",
       "           [6.9549e-02, 8.7116e-04, 4.2010e-04,  ..., 3.0907e-02,\n",
       "            3.2254e-02, 1.0447e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2073e-01, 3.1268e-04, 5.0755e-05,  ..., 1.3849e-02,\n",
       "            1.6685e-02, 8.2064e-02],\n",
       "           [2.1824e-04, 1.4379e-02, 1.5757e-03,  ..., 5.2735e-03,\n",
       "            7.2113e-03, 3.1767e-02],\n",
       "           [1.1489e-04, 2.3766e-02, 3.0130e-02,  ..., 1.4408e-03,\n",
       "            2.0764e-03, 9.7012e-03],\n",
       "           ...,\n",
       "           [7.2913e-03, 6.0284e-03, 9.5242e-04,  ..., 3.7917e-02,\n",
       "            3.4498e-02, 9.8943e-02],\n",
       "           [3.2253e-03, 3.0500e-03, 2.8904e-04,  ..., 9.5786e-03,\n",
       "            1.0366e-02, 3.5726e-02],\n",
       "           [3.3737e-03, 1.2618e-03, 9.8554e-05,  ..., 9.9637e-03,\n",
       "            9.1472e-03, 2.7449e-02]],\n",
       " \n",
       "          [[4.1069e-02, 1.1154e-02, 2.9014e-03,  ..., 7.1542e-02,\n",
       "            1.1057e-01, 4.1503e-01],\n",
       "           [4.5987e-04, 2.9040e-02, 7.6413e-02,  ..., 1.4349e-02,\n",
       "            1.0875e-02, 3.2460e-02],\n",
       "           [2.8112e-04, 3.6934e-03, 5.7889e-02,  ..., 8.4029e-04,\n",
       "            9.5740e-04, 2.9438e-03],\n",
       "           ...,\n",
       "           [8.1525e-04, 5.1659e-03, 1.0754e-03,  ..., 5.1386e-02,\n",
       "            3.9244e-02, 1.1474e-01],\n",
       "           [3.1583e-03, 3.4251e-03, 9.8851e-04,  ..., 6.7777e-02,\n",
       "            5.0008e-02, 1.3847e-01],\n",
       "           [1.0541e-02, 5.1922e-03, 2.2228e-03,  ..., 6.4948e-02,\n",
       "            4.8793e-02, 1.1541e-01]],\n",
       " \n",
       "          [[4.9076e-03, 4.8234e-03, 1.8345e-04,  ..., 2.8620e-01,\n",
       "            3.0777e-02, 1.2166e-01],\n",
       "           [9.1579e-04, 2.0620e-02, 1.4877e-03,  ..., 4.2065e-03,\n",
       "            5.0230e-03, 3.0485e-02],\n",
       "           [1.4356e-04, 2.3904e-02, 3.5082e-02,  ..., 9.0004e-04,\n",
       "            5.9075e-04, 3.2133e-03],\n",
       "           ...,\n",
       "           [2.3220e-03, 4.3067e-04, 9.7066e-05,  ..., 1.3823e-01,\n",
       "            2.3223e-01, 2.8656e-01],\n",
       "           [4.1521e-03, 8.1960e-04, 1.2412e-04,  ..., 5.7628e-02,\n",
       "            1.7652e-01, 5.0255e-01],\n",
       "           [1.0609e-02, 3.1844e-03, 5.0310e-04,  ..., 4.4193e-02,\n",
       "            8.7240e-02, 4.0421e-01]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.8284e-02, 2.8224e-02, 5.7247e-03,  ..., 1.2724e-03,\n",
       "            4.2056e-03, 1.8992e-01],\n",
       "           [2.3414e-02, 1.4723e-03, 8.9286e-03,  ..., 5.4274e-03,\n",
       "            1.1697e-03, 7.3860e-01],\n",
       "           [1.4443e-03, 8.9847e-04, 3.3551e-03,  ..., 3.3793e-03,\n",
       "            2.2136e-04, 8.9840e-01],\n",
       "           ...,\n",
       "           [1.0940e-02, 1.1947e-03, 2.3205e-03,  ..., 1.4289e-02,\n",
       "            7.9520e-04, 7.9394e-01],\n",
       "           [1.2464e-01, 4.4960e-03, 1.5044e-03,  ..., 7.9171e-03,\n",
       "            6.5860e-03, 3.9209e-01],\n",
       "           [9.0965e-03, 6.8465e-03, 2.9240e-03,  ..., 6.6902e-03,\n",
       "            1.8568e-03, 5.4147e-01]],\n",
       " \n",
       "          [[3.7422e-02, 6.1565e-04, 4.8757e-05,  ..., 1.2685e-03,\n",
       "            3.2422e-02, 1.4693e-02],\n",
       "           [7.7200e-03, 2.0207e-02, 1.1620e-02,  ..., 9.1435e-03,\n",
       "            1.8144e-02, 5.2177e-01],\n",
       "           [3.6548e-03, 1.1113e-02, 2.5387e-02,  ..., 8.9225e-04,\n",
       "            9.2524e-04, 7.2518e-01],\n",
       "           ...,\n",
       "           [5.3544e-03, 8.4837e-03, 1.4666e-03,  ..., 8.4961e-02,\n",
       "            1.1382e-02, 2.5842e-01],\n",
       "           [7.8517e-02, 2.1865e-03, 5.4720e-04,  ..., 7.5816e-04,\n",
       "            3.3639e-02, 2.5258e-02],\n",
       "           [1.3976e-02, 7.5063e-03, 1.8312e-02,  ..., 1.1317e-02,\n",
       "            1.1573e-02, 3.3503e-01]],\n",
       " \n",
       "          [[1.3831e-02, 1.2034e-03, 2.5086e-04,  ..., 5.0613e-03,\n",
       "            4.8028e-02, 1.7517e-01],\n",
       "           [2.7633e-03, 5.0739e-02, 2.0081e-03,  ..., 4.1421e-03,\n",
       "            1.4270e-02, 7.7034e-01],\n",
       "           [4.0527e-04, 4.9902e-05, 6.5912e-01,  ..., 9.9771e-06,\n",
       "            1.3610e-03, 3.3541e-01],\n",
       "           ...,\n",
       "           [1.7455e-03, 5.2707e-04, 4.1291e-05,  ..., 1.7602e-01,\n",
       "            1.4903e-03, 5.0944e-01],\n",
       "           [3.7300e-02, 2.2656e-03, 4.4763e-04,  ..., 2.2397e-03,\n",
       "            6.5944e-02, 1.5788e-01],\n",
       "           [2.1634e-02, 1.3880e-03, 1.0011e-02,  ..., 1.8480e-03,\n",
       "            1.1973e-02, 7.1615e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.4789e-01, 5.9379e-05, 6.5133e-05,  ..., 6.3298e-05,\n",
       "            4.2198e-01, 5.9594e-02],\n",
       "           [4.7369e-03, 4.8767e-02, 1.0771e-02,  ..., 2.8764e-05,\n",
       "            2.2234e-03, 5.8431e-01],\n",
       "           [1.4593e-03, 5.3699e-01, 8.2260e-03,  ..., 1.2548e-04,\n",
       "            1.4851e-04, 2.7405e-01],\n",
       "           ...,\n",
       "           [1.0571e-03, 5.3963e-06, 2.6197e-05,  ..., 1.8637e-02,\n",
       "            1.8875e-03, 3.1843e-01],\n",
       "           [1.5305e-02, 6.9051e-05, 1.2770e-04,  ..., 1.4176e-03,\n",
       "            1.0959e-03, 8.9935e-01],\n",
       "           [3.5273e-02, 1.8874e-03, 4.0314e-03,  ..., 5.8565e-03,\n",
       "            6.3014e-03, 5.7597e-01]],\n",
       " \n",
       "          [[3.3520e-01, 3.5745e-03, 1.5183e-03,  ..., 1.9876e-03,\n",
       "            6.2244e-03, 5.4669e-02],\n",
       "           [1.0652e-01, 1.0380e-02, 8.0912e-04,  ..., 3.2492e-03,\n",
       "            1.4763e-02, 2.3658e-01],\n",
       "           [3.0647e-02, 1.6016e-02, 3.1091e-03,  ..., 3.0027e-03,\n",
       "            5.0317e-03, 3.8988e-01],\n",
       "           ...,\n",
       "           [1.3103e-01, 3.4198e-02, 1.4480e-03,  ..., 3.1934e-03,\n",
       "            1.1739e-02, 2.6098e-01],\n",
       "           [1.9761e-01, 2.5089e-03, 4.5074e-04,  ..., 1.6225e-03,\n",
       "            6.2120e-02, 1.0308e-01],\n",
       "           [4.6312e-02, 3.0747e-03, 8.2404e-03,  ..., 4.6780e-03,\n",
       "            7.4528e-02, 8.2643e-03]],\n",
       " \n",
       "          [[5.3671e-03, 2.3140e-03, 4.5489e-04,  ..., 3.3134e-03,\n",
       "            9.8813e-02, 1.1132e-01],\n",
       "           [3.3268e-02, 2.6367e-02, 1.9782e-03,  ..., 5.4624e-03,\n",
       "            2.4495e-03, 4.1205e-01],\n",
       "           [3.6084e-02, 8.0034e-03, 6.5574e-04,  ..., 2.1613e-03,\n",
       "            2.7697e-03, 6.8881e-01],\n",
       "           ...,\n",
       "           [3.6834e-02, 5.3496e-03, 5.6631e-04,  ..., 2.7086e-03,\n",
       "            1.0852e-02, 5.3826e-01],\n",
       "           [1.7484e-01, 9.3079e-03, 1.1727e-03,  ..., 4.5106e-03,\n",
       "            9.4530e-03, 6.2037e-02],\n",
       "           [1.7480e-02, 8.7371e-03, 1.7698e-03,  ..., 2.9244e-03,\n",
       "            2.8041e-03, 7.8760e-01]]],\n",
       " \n",
       " \n",
       "         [[[3.5498e-03, 2.1615e-02, 1.7478e-03,  ..., 8.4504e-03,\n",
       "            2.3006e-02, 7.6791e-02],\n",
       "           [1.5187e-02, 5.5859e-03, 6.3901e-04,  ..., 2.5819e-02,\n",
       "            4.3059e-02, 1.4082e-01],\n",
       "           [4.4130e-03, 8.5887e-03, 8.4441e-04,  ..., 1.6503e-02,\n",
       "            3.9518e-02, 1.3524e-01],\n",
       "           ...,\n",
       "           [6.9753e-03, 1.2351e-03, 1.3948e-04,  ..., 1.4290e-01,\n",
       "            1.1930e-01, 5.3900e-01],\n",
       "           [1.0445e-02, 4.9463e-03, 5.3990e-04,  ..., 1.3088e-01,\n",
       "            1.3443e-01, 4.1070e-01],\n",
       "           [9.2529e-03, 9.3964e-03, 1.0768e-03,  ..., 7.7379e-02,\n",
       "            7.1430e-02, 2.1353e-01]],\n",
       " \n",
       "          [[9.3240e-02, 6.8700e-03, 7.1440e-04,  ..., 3.3173e-02,\n",
       "            3.8802e-02, 2.7000e-01],\n",
       "           [1.4798e-02, 7.1805e-02, 4.7029e-02,  ..., 6.7120e-03,\n",
       "            6.9161e-03, 6.9702e-02],\n",
       "           [3.6860e-03, 2.2720e-02, 1.5247e-01,  ..., 4.9783e-03,\n",
       "            2.7948e-03, 1.8212e-02],\n",
       "           ...,\n",
       "           [1.6321e-02, 1.2342e-03, 5.6787e-05,  ..., 2.9581e-02,\n",
       "            2.7066e-02, 1.7478e-01],\n",
       "           [1.6649e-02, 1.6543e-03, 3.1857e-04,  ..., 5.3338e-02,\n",
       "            5.6711e-02, 1.7148e-01],\n",
       "           [2.5897e-02, 1.4155e-03, 6.1221e-04,  ..., 5.9304e-02,\n",
       "            5.7591e-02, 1.4046e-01]],\n",
       " \n",
       "          [[2.5972e-02, 1.0955e-03, 1.5216e-04,  ..., 5.5317e-02,\n",
       "            3.7417e-02, 1.2918e-01],\n",
       "           [4.6537e-03, 1.8736e-02, 1.0101e-02,  ..., 2.5173e-03,\n",
       "            1.3513e-03, 1.9128e-02],\n",
       "           [1.1497e-03, 3.1073e-03, 1.7327e-01,  ..., 7.0887e-05,\n",
       "            3.8561e-04, 8.6760e-03],\n",
       "           ...,\n",
       "           [2.5066e-02, 1.5500e-03, 1.0797e-04,  ..., 1.4366e-01,\n",
       "            1.2466e-02, 6.1979e-02],\n",
       "           [3.1778e-02, 1.4942e-03, 2.5526e-04,  ..., 9.9840e-03,\n",
       "            1.6523e-03, 1.5531e-02],\n",
       "           [3.1255e-02, 1.3054e-03, 2.5112e-04,  ..., 3.1207e-03,\n",
       "            5.7272e-04, 9.4078e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1523e-01, 7.9764e-04, 1.6991e-04,  ..., 1.1072e-01,\n",
       "            3.4758e-02, 2.5294e-01],\n",
       "           [8.6708e-04, 3.7953e-02, 7.7684e-03,  ..., 3.8930e-03,\n",
       "            4.6311e-03, 4.9542e-02],\n",
       "           [5.6041e-05, 3.0500e-02, 2.9554e-02,  ..., 1.8537e-04,\n",
       "            2.5707e-04, 4.5769e-03],\n",
       "           ...,\n",
       "           [3.8980e-03, 9.4287e-03, 4.2727e-04,  ..., 3.7833e-02,\n",
       "            3.8215e-02, 2.5579e-01],\n",
       "           [3.9648e-03, 4.8053e-03, 2.0418e-04,  ..., 2.2399e-02,\n",
       "            1.3460e-02, 1.1196e-01],\n",
       "           [7.6483e-03, 2.7007e-03, 1.8653e-04,  ..., 2.5967e-02,\n",
       "            1.7652e-02, 1.2421e-01]],\n",
       " \n",
       "          [[1.8785e-01, 1.0513e-02, 3.1550e-03,  ..., 3.3914e-02,\n",
       "            3.2434e-02, 9.7822e-02],\n",
       "           [5.7091e-02, 1.8336e-02, 1.4463e-03,  ..., 5.3706e-02,\n",
       "            5.9196e-02, 1.5014e-01],\n",
       "           [2.2533e-02, 3.0118e-03, 7.4952e-04,  ..., 9.1889e-02,\n",
       "            8.4213e-02, 1.2974e-01],\n",
       "           ...,\n",
       "           [1.6922e-02, 4.0380e-02, 8.0705e-03,  ..., 1.5569e-02,\n",
       "            2.0307e-02, 6.3197e-02],\n",
       "           [2.5360e-02, 3.4654e-02, 1.5367e-02,  ..., 1.9532e-02,\n",
       "            1.3931e-02, 4.7437e-02],\n",
       "           [3.9588e-02, 4.0171e-02, 1.7319e-02,  ..., 2.8655e-02,\n",
       "            1.7768e-02, 3.3135e-02]],\n",
       " \n",
       "          [[9.2105e-03, 2.2173e-02, 2.3365e-03,  ..., 4.8451e-02,\n",
       "            3.5766e-02, 1.2220e-01],\n",
       "           [4.2072e-02, 1.5597e-02, 7.1313e-03,  ..., 5.7065e-03,\n",
       "            7.5639e-03, 2.3942e-02],\n",
       "           [9.4331e-03, 5.5653e-02, 9.5093e-04,  ..., 5.7563e-03,\n",
       "            6.6596e-03, 2.9560e-02],\n",
       "           ...,\n",
       "           [4.0499e-03, 1.8011e-03, 2.4430e-03,  ..., 1.9411e-03,\n",
       "            2.4525e-03, 1.4004e-02],\n",
       "           [4.8740e-03, 2.1508e-03, 1.8569e-03,  ..., 3.9273e-03,\n",
       "            4.6529e-03, 2.0831e-02],\n",
       "           [8.1916e-03, 2.2966e-03, 1.2388e-03,  ..., 4.3268e-03,\n",
       "            4.9953e-03, 1.7808e-02]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[5.1341e-02, 7.4733e-03, 1.3101e-03,  ..., 3.3730e-03,\n",
       "            6.0700e-03, 4.0988e-01],\n",
       "           [1.3340e-03, 7.1289e-03, 1.4912e-03,  ..., 1.1837e-03,\n",
       "            4.2735e-04, 9.2006e-01],\n",
       "           [3.9044e-04, 3.0227e-03, 4.2925e-04,  ..., 1.8465e-04,\n",
       "            1.2615e-04, 9.6777e-01],\n",
       "           ...,\n",
       "           [1.7822e-03, 3.2752e-03, 1.9338e-04,  ..., 4.1643e-04,\n",
       "            9.7472e-04, 9.1458e-01],\n",
       "           [8.6066e-03, 8.7201e-03, 1.4516e-03,  ..., 2.0938e-03,\n",
       "            4.8259e-03, 6.9175e-01],\n",
       "           [1.4109e-02, 1.2563e-02, 1.2933e-02,  ..., 1.0871e-02,\n",
       "            1.3325e-02, 1.7656e-02]],\n",
       " \n",
       "          [[6.0498e-01, 1.2864e-03, 8.8298e-05,  ..., 2.6735e-03,\n",
       "            2.3540e-02, 5.1349e-02],\n",
       "           [7.4744e-04, 9.6491e-03, 1.1796e-03,  ..., 2.9463e-03,\n",
       "            3.4926e-03, 8.5597e-01],\n",
       "           [7.9377e-05, 1.4392e-03, 2.4385e-02,  ..., 2.2794e-04,\n",
       "            9.2481e-05, 9.5697e-01],\n",
       "           ...,\n",
       "           [3.1976e-04, 4.5696e-03, 3.1981e-04,  ..., 1.7358e-02,\n",
       "            9.3059e-05, 8.6257e-01],\n",
       "           [3.5332e-02, 1.4596e-02, 8.9864e-04,  ..., 4.3151e-03,\n",
       "            1.2666e-02, 7.3424e-01],\n",
       "           [1.4940e-02, 1.8898e-02, 9.3646e-03,  ..., 1.5950e-02,\n",
       "            1.6746e-02, 2.1494e-02]],\n",
       " \n",
       "          [[8.6917e-02, 1.8151e-03, 6.5505e-05,  ..., 1.5200e-03,\n",
       "            7.5899e-03, 6.3607e-01],\n",
       "           [7.8844e-04, 1.1938e-02, 1.4361e-03,  ..., 6.6223e-03,\n",
       "            1.7134e-03, 8.8410e-01],\n",
       "           [2.2599e-05, 4.7171e-05, 8.8682e-01,  ..., 2.5248e-06,\n",
       "            4.1249e-05, 1.0999e-01],\n",
       "           ...,\n",
       "           [1.9240e-03, 6.1711e-03, 1.0852e-04,  ..., 2.0595e-01,\n",
       "            3.9821e-04, 4.7581e-01],\n",
       "           [2.8033e-02, 6.9521e-03, 8.0679e-04,  ..., 3.5786e-03,\n",
       "            2.5723e-02, 2.9688e-01],\n",
       "           [1.3526e-02, 1.4032e-02, 1.2978e-02,  ..., 1.5233e-02,\n",
       "            1.3660e-02, 4.7105e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.3698e-01, 6.0954e-03, 2.3668e-03,  ..., 1.0277e-03,\n",
       "            4.3656e-02, 1.3687e-01],\n",
       "           [3.7697e-03, 3.4386e-02, 1.4831e-03,  ..., 5.4122e-04,\n",
       "            2.2261e-03, 7.3092e-01],\n",
       "           [3.2365e-04, 2.6475e-03, 7.3929e-03,  ..., 1.5766e-04,\n",
       "            4.7102e-04, 9.2630e-01],\n",
       "           ...,\n",
       "           [1.2926e-02, 3.4826e-04, 1.3713e-04,  ..., 3.4400e-03,\n",
       "            7.5565e-03, 4.8832e-01],\n",
       "           [4.6729e-02, 1.4604e-03, 1.8467e-04,  ..., 1.8163e-04,\n",
       "            1.3481e-02, 1.4856e-01],\n",
       "           [1.2625e-02, 7.5618e-03, 8.8981e-03,  ..., 7.4359e-03,\n",
       "            1.0491e-02, 1.0724e-02]],\n",
       " \n",
       "          [[5.0783e-03, 2.4967e-03, 1.1785e-03,  ..., 1.2443e-02,\n",
       "            2.1481e-02, 4.2427e-01],\n",
       "           [4.1292e-04, 1.4120e-02, 6.9840e-03,  ..., 2.9444e-03,\n",
       "            1.4736e-03, 7.6847e-01],\n",
       "           [9.9583e-06, 1.6606e-03, 1.4456e-02,  ..., 1.8746e-04,\n",
       "            8.0440e-05, 9.7019e-01],\n",
       "           ...,\n",
       "           [2.9950e-03, 2.6043e-03, 4.3649e-04,  ..., 3.3913e-02,\n",
       "            5.4874e-04, 8.3778e-01],\n",
       "           [8.1828e-03, 6.2925e-03, 2.7647e-04,  ..., 5.7570e-03,\n",
       "            5.2966e-03, 7.5653e-01],\n",
       "           [9.8471e-03, 1.1019e-02, 1.1294e-02,  ..., 1.8540e-02,\n",
       "            1.2889e-02, 2.8496e-02]],\n",
       " \n",
       "          [[7.9334e-02, 1.2663e-03, 6.2400e-04,  ..., 8.3278e-04,\n",
       "            4.3000e-02, 1.5414e-01],\n",
       "           [7.3179e-04, 1.1632e-02, 5.7834e-03,  ..., 3.8485e-03,\n",
       "            4.7160e-03, 8.0291e-01],\n",
       "           [5.4985e-05, 1.0791e-04, 9.4298e-01,  ..., 1.7906e-05,\n",
       "            6.8842e-05, 5.0050e-02],\n",
       "           ...,\n",
       "           [3.8231e-04, 6.7130e-03, 1.6721e-04,  ..., 3.2424e-01,\n",
       "            4.2043e-04, 2.3597e-01],\n",
       "           [4.4266e-02, 8.2957e-03, 9.8647e-04,  ..., 8.9840e-04,\n",
       "            5.0344e-02, 1.7431e-01],\n",
       "           [1.1931e-02, 1.2128e-02, 8.7088e-03,  ..., 9.8515e-03,\n",
       "            1.8040e-02, 2.1049e-02]]],\n",
       " \n",
       " \n",
       "         [[[5.3575e-02, 5.2972e-03, 2.7662e-03,  ..., 1.5603e-03,\n",
       "            6.7638e-03, 8.6489e-02],\n",
       "           [2.4933e-03, 8.3419e-04, 1.7318e-04,  ..., 5.8013e-04,\n",
       "            3.9093e-03, 1.6811e-01],\n",
       "           [8.2987e-04, 1.2829e-03, 1.0486e-03,  ..., 7.3978e-04,\n",
       "            1.6097e-03, 1.2644e-01],\n",
       "           ...,\n",
       "           [1.9096e-02, 6.3838e-03, 1.5614e-03,  ..., 1.3385e-03,\n",
       "            3.6992e-03, 1.1885e-01],\n",
       "           [1.8687e-02, 1.0996e-02, 2.1501e-03,  ..., 2.6271e-03,\n",
       "            5.7982e-03, 1.3109e-01],\n",
       "           [2.4188e-02, 1.4760e-02, 1.1127e-02,  ..., 1.2200e-02,\n",
       "            1.6186e-02, 5.2742e-02]],\n",
       " \n",
       "          [[2.5924e-01, 1.4696e-02, 9.5073e-04,  ..., 1.4733e-02,\n",
       "            4.4108e-02, 1.5611e-01],\n",
       "           [1.3728e-03, 1.3069e-03, 4.6818e-04,  ..., 5.5953e-04,\n",
       "            1.8636e-03, 1.1464e-01],\n",
       "           [1.8535e-04, 1.8811e-04, 1.1397e-03,  ..., 3.5458e-04,\n",
       "            1.9514e-03, 1.5734e-01],\n",
       "           ...,\n",
       "           [4.2986e-03, 8.9715e-04, 1.6154e-04,  ..., 5.9668e-02,\n",
       "            2.4446e-01, 4.9107e-01],\n",
       "           [1.0230e-02, 8.9364e-04, 1.8631e-04,  ..., 2.6269e-02,\n",
       "            1.6306e-01, 5.2634e-01],\n",
       "           [2.2095e-02, 5.0986e-03, 4.7193e-03,  ..., 5.9362e-02,\n",
       "            1.2025e-01, 1.3502e-01]],\n",
       " \n",
       "          [[2.1168e-02, 7.1183e-04, 5.7085e-05,  ..., 6.0202e-04,\n",
       "            4.1619e-03, 1.7390e-01],\n",
       "           [2.2896e-03, 8.3577e-03, 6.2790e-04,  ..., 1.7997e-04,\n",
       "            1.0796e-03, 1.2772e-01],\n",
       "           [1.6612e-03, 1.8493e-03, 2.8417e-01,  ..., 1.9852e-04,\n",
       "            1.3884e-03, 4.3794e-02],\n",
       "           ...,\n",
       "           [1.8945e-03, 1.9019e-03, 9.0818e-04,  ..., 1.5019e-03,\n",
       "            2.3345e-03, 1.2908e-01],\n",
       "           [1.2729e-03, 1.9535e-03, 6.2171e-04,  ..., 7.9207e-05,\n",
       "            1.1114e-03, 1.3395e-01],\n",
       "           [6.7483e-03, 9.8150e-03, 4.7271e-03,  ..., 9.2857e-04,\n",
       "            3.7050e-03, 5.2117e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4913e-01, 1.5111e-03, 2.9327e-04,  ..., 1.1159e-01,\n",
       "            6.5101e-02, 9.8850e-02],\n",
       "           [1.9821e-02, 3.5254e-02, 6.9584e-03,  ..., 5.1640e-03,\n",
       "            3.1845e-03, 1.5426e-01],\n",
       "           [1.7995e-04, 5.3917e-03, 1.9208e-03,  ..., 5.0455e-05,\n",
       "            4.2155e-05, 3.7042e-02],\n",
       "           ...,\n",
       "           [8.3693e-04, 5.4595e-04, 3.5956e-05,  ..., 5.6221e-02,\n",
       "            4.2176e-02, 3.7046e-01],\n",
       "           [1.4295e-03, 7.7526e-04, 6.6507e-05,  ..., 1.6636e-02,\n",
       "            4.0753e-02, 3.3959e-01],\n",
       "           [2.0866e-02, 7.0848e-03, 3.4279e-03,  ..., 3.3169e-02,\n",
       "            4.6974e-02, 9.3701e-02]],\n",
       " \n",
       "          [[2.8475e-02, 3.5512e-03, 5.2807e-04,  ..., 3.1394e-02,\n",
       "            2.8415e-02, 1.4141e-01],\n",
       "           [9.8558e-04, 1.0648e-03, 2.5141e-04,  ..., 1.2247e-03,\n",
       "            2.4628e-03, 1.4848e-01],\n",
       "           [3.0770e-05, 4.7406e-04, 2.1184e-03,  ..., 4.2517e-05,\n",
       "            1.2952e-04, 6.5587e-02],\n",
       "           ...,\n",
       "           [2.0478e-02, 6.2683e-03, 5.9412e-04,  ..., 6.6937e-02,\n",
       "            1.0213e-01, 3.1937e-01],\n",
       "           [2.5057e-02, 2.0687e-03, 1.6299e-04,  ..., 2.4850e-02,\n",
       "            9.5793e-02, 4.5323e-01],\n",
       "           [3.2702e-02, 4.4745e-03, 1.2429e-03,  ..., 3.3001e-02,\n",
       "            6.1522e-02, 9.3045e-02]],\n",
       " \n",
       "          [[3.2083e-02, 1.2004e-03, 2.5599e-03,  ..., 1.3724e-02,\n",
       "            5.3627e-02, 3.2770e-01],\n",
       "           [1.4459e-03, 1.2902e-02, 3.3122e-03,  ..., 9.6999e-04,\n",
       "            7.6753e-03, 2.8495e-01],\n",
       "           [2.1160e-03, 1.6011e-03, 2.8342e-01,  ..., 5.9116e-04,\n",
       "            2.7771e-03, 5.0150e-02],\n",
       "           ...,\n",
       "           [4.0703e-03, 4.4465e-03, 1.5323e-04,  ..., 3.0245e-02,\n",
       "            2.7984e-02, 4.1862e-01],\n",
       "           [8.1184e-04, 1.2617e-03, 6.7532e-05,  ..., 1.6610e-04,\n",
       "            1.0648e-02, 5.3622e-01],\n",
       "           [5.8781e-03, 1.2289e-02, 2.3149e-03,  ..., 2.3701e-03,\n",
       "            1.5476e-02, 1.3459e-01]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_att[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8329e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e9229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[4.2342e-01, 1.0329e-02, 2.1425e-02,  ..., 2.6812e-03,\n",
       "            9.6221e-03, 9.5543e-02],\n",
       "           [1.5462e-03, 2.1176e-03, 5.7858e-03,  ..., 4.3158e-03,\n",
       "            3.9451e-03, 6.8317e-04],\n",
       "           [3.6258e-03, 1.5284e-02, 1.5809e-02,  ..., 1.4516e-02,\n",
       "            6.4294e-03, 1.0809e-03],\n",
       "           ...,\n",
       "           [1.0016e-02, 3.9249e-03, 1.4834e-02,  ..., 5.2647e-03,\n",
       "            3.4583e-03, 1.1564e-03],\n",
       "           [1.7533e-02, 3.1164e-03, 5.0359e-03,  ..., 5.9237e-03,\n",
       "            5.3873e-03, 3.1608e-02],\n",
       "           [3.1473e-01, 8.9481e-04, 6.2489e-03,  ..., 1.3714e-04,\n",
       "            4.2550e-04, 5.8916e-01]],\n",
       " \n",
       "          [[2.1291e-01, 1.4604e-02, 4.2316e-02,  ..., 5.4483e-03,\n",
       "            8.7653e-03, 6.9382e-03],\n",
       "           [6.3862e-01, 2.5806e-02, 8.0980e-02,  ..., 1.9607e-04,\n",
       "            5.1085e-05, 2.0034e-05],\n",
       "           [1.8794e-01, 6.9394e-02, 1.2233e-01,  ..., 2.3485e-04,\n",
       "            9.0569e-05, 4.2227e-05],\n",
       "           ...,\n",
       "           [7.0465e-03, 3.1036e-04, 7.7365e-04,  ..., 1.8401e-02,\n",
       "            3.6160e-02, 4.3924e-02],\n",
       "           [1.0477e-04, 7.3438e-05, 5.4740e-04,  ..., 9.5891e-02,\n",
       "            1.4777e-01, 1.2898e-01],\n",
       "           [1.9837e-03, 1.7544e-04, 1.3408e-03,  ..., 8.2889e-02,\n",
       "            2.5903e-01, 7.4874e-02]],\n",
       " \n",
       "          [[5.6987e-02, 1.4942e-02, 2.2877e-02,  ..., 4.1344e-03,\n",
       "            1.0106e-02, 7.8444e-03],\n",
       "           [8.1779e-03, 6.3672e-03, 4.3761e-03,  ..., 2.0070e-03,\n",
       "            3.4011e-03, 6.5128e-04],\n",
       "           [6.8799e-02, 5.5790e-03, 1.6064e-02,  ..., 2.0707e-03,\n",
       "            8.5502e-03, 9.4555e-03],\n",
       "           ...,\n",
       "           [3.3656e-02, 1.5107e-02, 3.8226e-03,  ..., 5.5688e-03,\n",
       "            3.7067e-03, 3.3087e-03],\n",
       "           [7.3056e-04, 3.9298e-03, 3.4627e-03,  ..., 9.2497e-03,\n",
       "            1.1704e-02, 8.0794e-03],\n",
       "           [5.6228e-03, 7.6340e-03, 5.9718e-03,  ..., 6.0844e-03,\n",
       "            2.1961e-02, 8.5943e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.6431e-02, 2.4178e-02, 3.3328e-02,  ..., 1.1138e-02,\n",
       "            3.4082e-03, 6.3698e-03],\n",
       "           [5.0120e-03, 2.3812e-03, 7.7749e-03,  ..., 6.1886e-03,\n",
       "            1.7819e-03, 2.9042e-04],\n",
       "           [9.5403e-03, 6.3154e-03, 5.9676e-04,  ..., 1.6959e-02,\n",
       "            2.6409e-03, 1.1253e-03],\n",
       "           ...,\n",
       "           [1.5447e-02, 3.5778e-03, 7.1891e-03,  ..., 7.8868e-04,\n",
       "            1.3167e-02, 1.5297e-02],\n",
       "           [1.6200e-03, 6.8072e-03, 4.0818e-03,  ..., 1.0280e-02,\n",
       "            1.4050e-02, 5.1528e-03],\n",
       "           [1.8910e-03, 4.3083e-03, 1.4290e-02,  ..., 4.3854e-03,\n",
       "            1.7415e-02, 1.4882e-03]],\n",
       " \n",
       "          [[4.1854e-01, 3.2857e-03, 7.9372e-03,  ..., 2.3913e-03,\n",
       "            3.3418e-03, 2.7166e-02],\n",
       "           [1.2324e-01, 1.5330e-01, 1.9767e-03,  ..., 8.7628e-03,\n",
       "            2.1620e-03, 1.0164e-03],\n",
       "           [1.8972e-01, 7.5016e-03, 2.1379e-01,  ..., 1.4973e-03,\n",
       "            2.4333e-03, 1.3838e-03],\n",
       "           ...,\n",
       "           [1.4508e-02, 1.5158e-02, 1.3994e-03,  ..., 1.3545e-01,\n",
       "            1.9747e-03, 6.4363e-03],\n",
       "           [7.2265e-02, 2.5954e-03, 5.7251e-03,  ..., 6.0973e-03,\n",
       "            2.9216e-03, 9.1902e-03],\n",
       "           [2.0787e-01, 7.1710e-03, 5.3650e-02,  ..., 2.4276e-03,\n",
       "            4.5644e-03, 1.2273e-01]],\n",
       " \n",
       "          [[2.1613e-06, 1.8255e-07, 6.3971e-07,  ..., 6.4301e-08,\n",
       "            4.1856e-07, 9.9535e-01],\n",
       "           [2.2776e-03, 2.2297e-02, 6.2621e-02,  ..., 3.7517e-02,\n",
       "            1.2150e-03, 2.5558e-03],\n",
       "           [1.0493e-02, 3.4188e-02, 5.7054e-03,  ..., 7.2006e-03,\n",
       "            7.6002e-04, 1.4649e-03],\n",
       "           ...,\n",
       "           [1.8080e-03, 5.8831e-02, 2.6745e-02,  ..., 3.6623e-02,\n",
       "            1.4710e-03, 8.8129e-04],\n",
       "           [1.2850e-03, 7.5932e-04, 9.0554e-04,  ..., 3.1860e-03,\n",
       "            2.7735e-01, 1.7062e-04],\n",
       "           [1.6706e-02, 6.0578e-04, 6.1475e-04,  ..., 1.4377e-03,\n",
       "            1.9746e-02, 2.4079e-02]]],\n",
       " \n",
       " \n",
       "         [[[4.1894e-01, 6.4577e-03, 4.7420e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.0845e-03, 3.7855e-03, 1.4426e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0731e-01, 5.3814e-03, 6.7136e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [4.9573e-02, 9.2831e-03, 7.0225e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.6031e-02, 8.7334e-03, 7.6199e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.0581e-02, 9.5340e-03, 7.5169e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.1607e-01, 7.1518e-03, 1.5907e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.6871e-02, 1.6834e-01, 7.4738e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6083e-02, 3.0612e-01, 4.7875e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.7209e-02, 6.9124e-05, 7.2882e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4719e-02, 4.5117e-05, 8.2292e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5701e-02, 5.7684e-05, 9.0824e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[5.4840e-02, 1.0765e-02, 1.8095e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.7514e-03, 3.4223e-03, 1.2824e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3980e-03, 1.0894e-02, 8.4432e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.6575e-02, 9.7494e-03, 1.5879e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8804e-02, 9.9912e-03, 1.5858e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8489e-02, 9.4600e-03, 1.7186e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4178e-02, 1.4636e-02, 1.9520e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8789e-03, 4.6195e-03, 3.9899e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8658e-02, 1.9260e-02, 1.6040e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.3151e-02, 1.0718e-02, 9.8664e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2432e-02, 1.0123e-02, 9.4333e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6875e-02, 9.3900e-03, 9.5073e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[4.0169e-01, 7.0218e-03, 7.1710e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8130e-01, 5.5999e-02, 5.7488e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.4195e-01, 1.7344e-03, 6.5889e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [9.4164e-02, 6.1908e-03, 2.0558e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.3749e-02, 5.4452e-03, 2.5016e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0719e-01, 5.2333e-03, 1.6904e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.2523e-06, 3.7384e-06, 1.3091e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.9622e-03, 2.6970e-03, 2.0433e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.3136e-03, 4.9920e-03, 1.6765e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.6562e-04, 2.4910e-03, 2.2401e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2904e-04, 2.5757e-03, 2.2604e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0158e-04, 2.5325e-03, 2.6721e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.2301e-01, 1.4900e-03, 1.5696e-03,  ..., 8.1778e-04,\n",
       "            8.4365e-03, 1.5166e-02],\n",
       "           [1.2864e-01, 5.7028e-03, 6.7928e-03,  ..., 1.8462e-03,\n",
       "            8.0679e-03, 3.1552e-03],\n",
       "           [2.1109e-01, 3.2258e-03, 1.8859e-03,  ..., 3.1574e-04,\n",
       "            7.0050e-03, 2.6070e-03],\n",
       "           ...,\n",
       "           [4.0066e-02, 5.4236e-03, 2.5699e-03,  ..., 7.8027e-03,\n",
       "            3.2056e-02, 1.0274e-01],\n",
       "           [4.8282e-02, 9.6208e-04, 2.1859e-03,  ..., 5.8397e-03,\n",
       "            3.6335e-02, 1.0301e-01],\n",
       "           [6.8313e-01, 4.8623e-05, 5.7974e-05,  ..., 1.2232e-04,\n",
       "            1.4884e-02, 1.7532e-01]],\n",
       " \n",
       "          [[4.3347e-01, 5.7920e-03, 7.7769e-03,  ..., 3.7439e-03,\n",
       "            1.3640e-02, 1.5482e-02],\n",
       "           [6.4006e-01, 4.9109e-03, 4.7606e-03,  ..., 6.0927e-06,\n",
       "            4.9304e-05, 1.5083e-04],\n",
       "           [7.0823e-01, 1.0100e-02, 1.3827e-03,  ..., 2.6926e-05,\n",
       "            6.0889e-04, 2.8995e-03],\n",
       "           ...,\n",
       "           [7.5243e-01, 1.5625e-05, 7.6098e-06,  ..., 3.2057e-04,\n",
       "            5.8575e-02, 1.7819e-01],\n",
       "           [5.0383e-01, 3.7992e-05, 2.2599e-04,  ..., 1.0330e-02,\n",
       "            1.0122e-01, 3.3729e-01],\n",
       "           [9.9134e-01, 6.7377e-07, 2.2348e-06,  ..., 8.9919e-05,\n",
       "            2.4370e-03, 5.6739e-03]],\n",
       " \n",
       "          [[4.5846e-01, 1.7047e-03, 3.6263e-03,  ..., 9.8865e-04,\n",
       "            1.9038e-02, 1.8658e-02],\n",
       "           [9.8431e-01, 3.8548e-03, 1.3316e-04,  ..., 6.4746e-07,\n",
       "            6.3610e-06, 6.6354e-06],\n",
       "           [9.6574e-01, 2.9576e-02, 2.2109e-04,  ..., 9.2036e-07,\n",
       "            6.2464e-05, 2.0905e-04],\n",
       "           ...,\n",
       "           [8.9532e-01, 2.0690e-05, 3.2758e-05,  ..., 2.2920e-03,\n",
       "            1.2285e-02, 4.1850e-02],\n",
       "           [8.0120e-01, 1.5216e-06, 1.1324e-05,  ..., 2.3585e-03,\n",
       "            1.3003e-01, 4.2221e-02],\n",
       "           [9.1446e-01, 4.4722e-07, 7.8103e-06,  ..., 6.9688e-05,\n",
       "            5.7632e-02, 2.6281e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2105e-01, 3.5627e-03, 6.0753e-03,  ..., 2.9745e-03,\n",
       "            1.2006e-02, 3.0725e-02],\n",
       "           [1.8170e-01, 7.9235e-04, 3.5296e-03,  ..., 6.8500e-05,\n",
       "            3.0244e-03, 1.6268e-03],\n",
       "           [3.4128e-01, 1.2776e-03, 7.2690e-04,  ..., 9.9026e-04,\n",
       "            6.1647e-03, 3.2313e-03],\n",
       "           ...,\n",
       "           [3.1155e-01, 7.6206e-04, 5.2356e-04,  ..., 4.9515e-04,\n",
       "            1.7656e-01, 1.2277e-01],\n",
       "           [1.9636e-01, 4.4282e-04, 5.7413e-04,  ..., 1.8264e-02,\n",
       "            1.2014e-01, 9.3289e-02],\n",
       "           [7.9510e-01, 2.4908e-04, 1.6144e-04,  ..., 5.6568e-03,\n",
       "            3.2575e-02, 5.1531e-02]],\n",
       " \n",
       "          [[4.5969e-01, 1.4092e-03, 4.9882e-04,  ..., 5.2496e-04,\n",
       "            4.8406e-02, 5.5372e-02],\n",
       "           [4.0626e-01, 1.6886e-02, 3.3271e-03,  ..., 3.2405e-03,\n",
       "            1.2696e-02, 7.7619e-03],\n",
       "           [2.5005e-01, 2.0235e-03, 2.8363e-04,  ..., 1.6873e-04,\n",
       "            3.7890e-02, 5.0930e-03],\n",
       "           ...,\n",
       "           [7.3470e-02, 7.1167e-02, 1.6582e-03,  ..., 1.3552e-02,\n",
       "            2.9999e-02, 1.2933e-02],\n",
       "           [2.8988e-01, 1.7133e-03, 4.2432e-04,  ..., 1.1782e-03,\n",
       "            6.7429e-02, 1.1370e-02],\n",
       "           [1.9307e-03, 4.9340e-06, 7.5325e-06,  ..., 4.4650e-05,\n",
       "            4.4190e-03, 9.2198e-05]],\n",
       " \n",
       "          [[5.3719e-01, 1.2564e-02, 1.6548e-02,  ..., 8.4286e-04,\n",
       "            2.8674e-03, 4.0832e-03],\n",
       "           [1.3022e-01, 5.0101e-03, 8.5815e-01,  ..., 8.6529e-11,\n",
       "            1.0877e-09, 5.0444e-07],\n",
       "           [1.6909e-03, 2.1666e-05, 1.4461e-04,  ..., 1.2644e-06,\n",
       "            1.3667e-08, 1.2229e-09],\n",
       "           ...,\n",
       "           [1.4662e-02, 2.4083e-08, 2.6775e-09,  ..., 2.5389e-04,\n",
       "            9.8264e-01, 7.8180e-04],\n",
       "           [1.1407e-02, 4.6406e-08, 5.0626e-08,  ..., 6.1965e-07,\n",
       "            9.0777e-05, 9.8841e-01],\n",
       "           [9.9999e-01, 2.5289e-10, 2.4575e-08,  ..., 4.1538e-08,\n",
       "            1.1523e-07, 1.3840e-05]]],\n",
       " \n",
       " \n",
       "         [[[5.7053e-01, 8.3550e-04, 1.4745e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8098e-02, 1.7272e-02, 3.0578e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4955e-02, 4.7299e-02, 4.6007e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.2339e-03, 1.8434e-02, 1.3987e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3294e-03, 1.7131e-02, 1.3376e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.1064e-03, 1.4851e-02, 1.1970e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[4.4518e-01, 8.5375e-03, 8.3906e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.3655e-02, 3.5726e-02, 8.5111e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0859e-02, 4.6061e-03, 9.5523e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.2523e-03, 1.5567e-03, 5.8018e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1649e-03, 9.4470e-04, 1.0361e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.1625e-03, 5.7552e-04, 6.5947e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[4.5152e-01, 8.0739e-03, 8.8455e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.7492e-01, 4.4427e-03, 2.8824e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.8748e-01, 7.3536e-02, 4.6105e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.7193e-01, 1.5804e-04, 3.1185e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.2717e-01, 5.5601e-05, 2.3606e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0084e-01, 6.1186e-05, 1.9992e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2163e-01, 6.7368e-03, 8.2960e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8591e-02, 1.3582e-02, 5.6417e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.6240e-02, 2.7866e-02, 1.7397e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.6077e-03, 1.8562e-03, 5.9847e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.7039e-03, 1.8048e-03, 4.9684e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8876e-03, 1.6481e-03, 4.5696e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[4.5024e-01, 5.9839e-04, 3.9840e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.6283e-01, 1.9301e-03, 2.7424e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2097e-01, 1.1587e-02, 1.3321e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.8190e-04, 2.9118e-04, 6.7935e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.5258e-04, 3.0493e-04, 6.3129e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.4027e-04, 3.4033e-04, 5.8753e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[5.6482e-01, 1.2340e-02, 3.0298e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.8779e-03, 1.6204e-04, 9.9274e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1948e-03, 6.8581e-06, 3.8113e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0811e-05, 1.6282e-06, 3.8033e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4032e-05, 2.1484e-05, 4.2203e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.7255e-05, 2.3509e-05, 2.7974e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[2.7372e-01, 1.3117e-02, 2.7810e-03,  ..., 1.6241e-02,\n",
       "            2.6664e-02, 8.4264e-02],\n",
       "           [2.5007e-01, 1.4223e-02, 2.1273e-03,  ..., 1.8355e-03,\n",
       "            4.5957e-02, 2.4977e-01],\n",
       "           [1.5833e-01, 1.7085e-02, 1.5215e-03,  ..., 2.8356e-03,\n",
       "            3.9802e-02, 1.7995e-01],\n",
       "           ...,\n",
       "           [1.4133e-01, 2.7779e-02, 3.7638e-04,  ..., 3.9815e-02,\n",
       "            2.4076e-02, 2.5245e-01],\n",
       "           [4.6742e-01, 5.7135e-03, 1.3994e-03,  ..., 1.0668e-02,\n",
       "            1.7732e-02, 7.1980e-02],\n",
       "           [6.0169e-01, 1.0407e-02, 2.0987e-03,  ..., 1.1860e-02,\n",
       "            2.3700e-02, 9.6628e-02]],\n",
       " \n",
       "          [[3.0446e-01, 1.5978e-02, 8.1813e-03,  ..., 3.7890e-03,\n",
       "            1.4881e-02, 9.8076e-02],\n",
       "           [9.0753e-01, 1.8515e-03, 6.3385e-05,  ..., 2.0881e-05,\n",
       "            4.9675e-04, 6.7254e-02],\n",
       "           [6.6031e-01, 1.6332e-01, 2.9552e-04,  ..., 3.2644e-05,\n",
       "            1.2086e-03, 9.4264e-02],\n",
       "           ...,\n",
       "           [6.7414e-01, 1.7206e-05, 2.6954e-05,  ..., 2.9866e-02,\n",
       "            5.1099e-02, 1.9903e-01],\n",
       "           [1.7655e-01, 8.2697e-05, 3.1818e-04,  ..., 4.1525e-01,\n",
       "            1.2460e-01, 8.7942e-02],\n",
       "           [6.6859e-01, 6.0839e-03, 2.0303e-03,  ..., 1.5405e-03,\n",
       "            2.5529e-02, 7.5974e-02]],\n",
       " \n",
       "          [[6.3102e-02, 1.0718e-02, 6.4619e-03,  ..., 7.7497e-03,\n",
       "            1.4372e-02, 5.2844e-02],\n",
       "           [6.2334e-02, 1.1818e-01, 3.4261e-03,  ..., 4.2632e-03,\n",
       "            2.0755e-03, 2.8895e-02],\n",
       "           [1.1559e-01, 6.9228e-03, 4.8004e-02,  ..., 1.4797e-03,\n",
       "            5.5623e-03, 6.9953e-02],\n",
       "           ...,\n",
       "           [1.3368e-01, 1.1444e-03, 3.2948e-04,  ..., 1.1492e-01,\n",
       "            9.9381e-02, 1.0013e-01],\n",
       "           [1.0402e-01, 9.0906e-04, 1.4448e-03,  ..., 1.0920e-02,\n",
       "            7.3158e-02, 6.1454e-02],\n",
       "           [2.5896e-01, 4.3963e-03, 3.6475e-03,  ..., 8.2088e-03,\n",
       "            2.3519e-02, 1.4650e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.8214e-01, 4.6482e-03, 4.1057e-03,  ..., 2.1066e-03,\n",
       "            1.2125e-02, 2.7523e-01],\n",
       "           [2.4541e-01, 7.2567e-03, 9.5166e-03,  ..., 2.0253e-03,\n",
       "            2.3878e-03, 2.9735e-01],\n",
       "           [3.1328e-01, 6.1061e-03, 6.4925e-05,  ..., 8.8194e-04,\n",
       "            2.5105e-03, 3.8682e-01],\n",
       "           ...,\n",
       "           [2.4413e-01, 6.1213e-03, 2.3400e-03,  ..., 3.7873e-02,\n",
       "            2.6700e-02, 2.1619e-01],\n",
       "           [1.3580e-01, 7.8234e-04, 1.6783e-03,  ..., 6.2983e-03,\n",
       "            7.4377e-02, 1.1197e-01],\n",
       "           [2.7701e-01, 2.2312e-03, 4.4056e-03,  ..., 2.2107e-03,\n",
       "            2.0675e-02, 3.4755e-01]],\n",
       " \n",
       "          [[3.5147e-01, 2.4455e-03, 3.2603e-03,  ..., 3.3441e-03,\n",
       "            1.6365e-02, 3.2934e-01],\n",
       "           [3.4981e-01, 3.6349e-03, 7.7727e-03,  ..., 5.4879e-04,\n",
       "            4.8445e-03, 2.1011e-01],\n",
       "           [5.3431e-01, 1.8146e-04, 4.0922e-05,  ..., 2.6782e-05,\n",
       "            4.1100e-03, 2.5462e-01],\n",
       "           ...,\n",
       "           [3.3919e-01, 3.4880e-04, 7.0239e-04,  ..., 2.0674e-02,\n",
       "            9.0669e-02, 4.5190e-01],\n",
       "           [1.8975e-01, 3.9797e-04, 1.4028e-03,  ..., 1.6513e-02,\n",
       "            1.3008e-01, 3.4538e-01],\n",
       "           [5.3056e-01, 1.3728e-04, 3.9962e-04,  ..., 2.1968e-04,\n",
       "            5.4298e-02, 1.6458e-01]],\n",
       " \n",
       "          [[4.1615e-02, 2.0381e-02, 3.3649e-03,  ..., 9.6477e-03,\n",
       "            5.8917e-03, 5.2276e-03],\n",
       "           [5.6209e-01, 7.5183e-04, 2.4571e-03,  ..., 3.1049e-03,\n",
       "            2.9109e-02, 4.5225e-02],\n",
       "           [4.8976e-01, 4.8122e-03, 4.0354e-04,  ..., 3.8721e-03,\n",
       "            4.1343e-02, 6.8496e-02],\n",
       "           ...,\n",
       "           [2.5454e-01, 8.1450e-03, 8.2398e-03,  ..., 4.4128e-02,\n",
       "            8.4091e-02, 3.9162e-02],\n",
       "           [5.5732e-02, 8.7963e-03, 6.0752e-03,  ..., 2.3200e-02,\n",
       "            3.0585e-02, 1.7943e-02],\n",
       "           [1.7231e-01, 1.4193e-03, 1.8034e-03,  ..., 4.2277e-03,\n",
       "            1.0564e-02, 6.4586e-01]]],\n",
       " \n",
       " \n",
       "         [[[2.5611e-01, 1.5043e-02, 5.4936e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.0080e-02, 2.5917e-02, 7.0960e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1862e-02, 4.5254e-02, 8.2381e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.1355e-01, 1.0885e-02, 6.8778e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7926e-01, 1.2578e-02, 7.3505e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5915e-01, 1.2928e-02, 8.3598e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.4764e-01, 1.7124e-02, 8.0084e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.0053e-01, 5.1485e-03, 3.8350e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.2973e-01, 3.4216e-02, 9.6739e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [4.6818e-01, 7.6188e-03, 8.2185e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.9225e-01, 5.8090e-03, 5.4661e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6743e-01, 1.0538e-02, 4.7776e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[6.9345e-02, 8.7614e-03, 7.3836e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8578e-02, 8.3349e-02, 2.7023e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.4534e-02, 2.0581e-02, 1.0629e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [4.0747e-02, 5.1730e-03, 2.3637e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.9990e-02, 4.8089e-03, 2.1355e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.3316e-02, 5.4872e-03, 2.1040e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.9067e-01, 6.6677e-03, 4.7460e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2026e-01, 7.3166e-02, 1.4210e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3410e-01, 2.9773e-02, 2.2302e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [9.3684e-02, 1.4212e-02, 1.4488e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.6431e-02, 1.3118e-02, 1.3885e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.1064e-02, 1.2575e-02, 1.5130e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.7989e-01, 2.5154e-03, 2.0092e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.2790e-02, 1.4887e-02, 1.7550e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4264e-02, 2.1111e-02, 1.3746e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.2887e-01, 3.8317e-03, 5.6946e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.2860e-01, 3.6235e-03, 5.2331e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6066e-01, 4.5063e-03, 7.5938e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[7.3114e-02, 1.0194e-02, 3.9395e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3275e-02, 2.9206e-02, 7.3092e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.8992e-02, 4.0376e-02, 1.7447e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.4860e-01, 7.6175e-03, 3.3733e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4707e-01, 8.3780e-03, 3.5111e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2207e-01, 1.0396e-02, 5.0760e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[9.4068e-02, 1.7352e-02, 4.1670e-03,  ..., 9.8703e-03,\n",
       "            1.3922e-02, 6.3811e-01],\n",
       "           [1.2483e-02, 3.4259e-02, 8.1961e-03,  ..., 5.7044e-02,\n",
       "            7.9875e-03, 3.9850e-02],\n",
       "           [4.4291e-02, 2.9871e-02, 9.9617e-04,  ..., 2.6489e-02,\n",
       "            1.9718e-02, 3.8184e-01],\n",
       "           ...,\n",
       "           [6.4397e-02, 1.1522e-01, 5.5044e-04,  ..., 2.3838e-02,\n",
       "            3.3557e-02, 2.1946e-01],\n",
       "           [1.1880e-01, 7.1362e-03, 9.3044e-04,  ..., 3.2006e-03,\n",
       "            2.6534e-02, 4.7746e-01],\n",
       "           [1.2289e-01, 1.3221e-03, 1.4125e-03,  ..., 1.1271e-03,\n",
       "            6.9686e-02, 6.0665e-01]],\n",
       " \n",
       "          [[3.1972e-02, 3.7772e-02, 1.1040e-02,  ..., 2.3144e-02,\n",
       "            1.6670e-02, 2.9391e-02],\n",
       "           [1.3967e-01, 7.5443e-04, 3.9576e-03,  ..., 1.3264e-03,\n",
       "            2.7893e-02, 1.8391e-01],\n",
       "           [1.0195e-01, 7.0077e-03, 8.7691e-04,  ..., 1.2942e-03,\n",
       "            1.8091e-02, 4.8721e-01],\n",
       "           ...,\n",
       "           [9.0612e-02, 1.3860e-02, 7.2821e-03,  ..., 1.6784e-03,\n",
       "            8.5584e-02, 2.0794e-01],\n",
       "           [5.2594e-02, 4.1767e-03, 2.2463e-03,  ..., 7.3143e-03,\n",
       "            1.3017e-01, 1.0798e-01],\n",
       "           [3.3526e-02, 2.9672e-03, 2.3710e-03,  ..., 4.0926e-03,\n",
       "            8.2586e-03, 6.6748e-01]],\n",
       " \n",
       "          [[1.0372e-01, 2.5323e-02, 4.6278e-03,  ..., 1.9414e-02,\n",
       "            2.2046e-03, 4.2478e-01],\n",
       "           [2.6329e-02, 2.4792e-02, 3.9249e-02,  ..., 1.5288e-02,\n",
       "            7.6479e-04, 1.5128e-01],\n",
       "           [4.1445e-02, 2.1530e-02, 3.6278e-03,  ..., 1.2414e-02,\n",
       "            1.3370e-02, 5.6664e-01],\n",
       "           ...,\n",
       "           [3.1160e-02, 2.3873e-02, 1.4680e-02,  ..., 6.9388e-03,\n",
       "            2.3431e-03, 4.3195e-01],\n",
       "           [4.4784e-02, 1.8550e-02, 6.2340e-03,  ..., 2.2224e-02,\n",
       "            2.9536e-03, 1.1150e-01],\n",
       "           [5.5497e-03, 1.5102e-03, 8.3529e-04,  ..., 4.4419e-04,\n",
       "            2.2422e-02, 8.5402e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.0323e-02, 8.9455e-03, 4.7587e-03,  ..., 5.2420e-03,\n",
       "            1.0630e-02, 4.7647e-01],\n",
       "           [4.5954e-02, 2.0464e-02, 9.0983e-03,  ..., 3.6960e-04,\n",
       "            5.0095e-03, 6.7879e-01],\n",
       "           [6.2590e-02, 1.1997e-01, 1.3760e-05,  ..., 7.6490e-05,\n",
       "            7.7827e-03, 4.4641e-01],\n",
       "           ...,\n",
       "           [1.7972e-02, 5.0213e-04, 1.5985e-05,  ..., 2.4720e-02,\n",
       "            2.3956e-02, 8.5245e-01],\n",
       "           [2.1182e-02, 4.7289e-04, 2.1178e-04,  ..., 7.3243e-03,\n",
       "            2.4887e-02, 4.7507e-01],\n",
       "           [3.5502e-02, 9.7983e-03, 2.0167e-03,  ..., 1.4072e-02,\n",
       "            1.3850e-02, 5.7611e-01]],\n",
       " \n",
       "          [[1.8777e-01, 1.0125e-02, 3.1812e-03,  ..., 3.7963e-03,\n",
       "            6.6648e-03, 7.1662e-01],\n",
       "           [1.4863e-01, 5.4058e-02, 2.1567e-02,  ..., 6.6937e-04,\n",
       "            1.0428e-03, 6.0411e-01],\n",
       "           [1.5268e-01, 1.0349e-01, 3.3720e-03,  ..., 2.3321e-04,\n",
       "            5.9958e-04, 6.2173e-01],\n",
       "           ...,\n",
       "           [7.6858e-02, 2.6183e-04, 6.9517e-05,  ..., 1.1178e-02,\n",
       "            6.1296e-02, 2.0927e-01],\n",
       "           [6.7149e-02, 1.0483e-04, 3.2565e-05,  ..., 2.3646e-02,\n",
       "            6.9690e-02, 3.5905e-01],\n",
       "           [8.7248e-02, 1.4774e-03, 1.2504e-03,  ..., 5.7242e-04,\n",
       "            5.5523e-03, 8.7121e-01]],\n",
       " \n",
       "          [[3.4068e-01, 5.4357e-03, 1.0101e-03,  ..., 2.6972e-03,\n",
       "            1.4698e-01, 8.4357e-02],\n",
       "           [9.1013e-01, 5.7347e-05, 9.5335e-09,  ..., 1.0686e-09,\n",
       "            3.0257e-07, 8.9685e-02],\n",
       "           [8.4267e-04, 9.9688e-01, 4.3217e-06,  ..., 4.0042e-11,\n",
       "            3.0645e-08, 1.1428e-03],\n",
       "           ...,\n",
       "           [3.3254e-03, 1.8500e-10, 9.1484e-09,  ..., 6.1426e-07,\n",
       "            2.5355e-05, 1.1100e-05],\n",
       "           [3.6950e-03, 4.4735e-09, 1.9111e-08,  ..., 9.4643e-01,\n",
       "            3.4043e-02, 1.4343e-04],\n",
       "           [4.0189e-01, 4.8501e-03, 2.2119e-03,  ..., 2.9520e-03,\n",
       "            1.4898e-02, 2.5972e-02]]],\n",
       " \n",
       " \n",
       "         [[[7.6771e-02, 1.0130e-02, 1.9296e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.1941e-03, 4.1575e-03, 8.4490e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5210e-02, 1.7694e-02, 1.4054e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.2175e-02, 1.7956e-02, 6.9924e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.7394e-02, 1.7062e-02, 9.4299e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.5425e-02, 1.8389e-02, 1.1847e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.2698e-02, 1.0621e-02, 2.0378e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.9551e-02, 3.2971e-02, 2.5919e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.7466e-02, 2.5338e-02, 1.2795e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.1534e-01, 4.0265e-03, 1.3536e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1737e-01, 3.8497e-03, 1.3907e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0794e-01, 5.1992e-03, 1.8812e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.2683e-01, 1.1913e-02, 9.0248e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1187e-02, 6.2217e-03, 1.2045e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.3570e-02, 6.4492e-03, 3.0165e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.5986e-02, 2.6615e-02, 8.6210e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.7925e-02, 2.8379e-02, 8.2239e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.9392e-02, 3.1755e-02, 8.9941e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.3355e-02, 8.0734e-03, 4.1532e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9552e-02, 6.8906e-03, 1.3991e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5600e-02, 3.7482e-02, 3.5920e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.3549e-02, 2.9201e-03, 2.4527e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8164e-02, 3.3665e-03, 2.8554e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0311e-02, 4.9464e-03, 4.0283e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.1104e-01, 3.8995e-03, 1.0674e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5693e-02, 7.1624e-03, 2.3128e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.9951e-02, 5.7084e-02, 2.4319e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.1062e-01, 1.6100e-03, 8.1950e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0928e-01, 1.5753e-03, 1.0839e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2333e-01, 1.8541e-03, 1.4493e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.2975e-01, 4.6882e-03, 1.5125e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.4443e-01, 3.7313e-04, 1.0780e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.6030e-03, 9.7067e-01, 2.1529e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [8.4717e-01, 1.1280e-05, 1.2122e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1729e-01, 1.3923e-04, 8.8403e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4860e-01, 4.2355e-04, 3.0001e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[9.1038e-02, 3.4963e-03, 1.8293e-03,  ..., 1.3059e-02,\n",
       "            4.3170e-02, 2.6138e-01],\n",
       "           [1.3282e-01, 9.0776e-02, 5.8537e-06,  ..., 1.0128e-02,\n",
       "            3.9190e-02, 5.0791e-01],\n",
       "           [1.2000e-01, 1.1652e-05, 2.2083e-02,  ..., 1.9349e-03,\n",
       "            3.4573e-02, 7.5060e-01],\n",
       "           ...,\n",
       "           [8.8646e-04, 1.8021e-05, 1.6779e-05,  ..., 2.7353e-04,\n",
       "            3.3846e-04, 1.1060e-03],\n",
       "           [8.5261e-02, 1.3160e-03, 7.0602e-04,  ..., 5.6008e-03,\n",
       "            3.8193e-02, 3.5025e-01],\n",
       "           [5.6532e-02, 1.2068e-03, 1.3761e-03,  ..., 9.6351e-04,\n",
       "            2.3174e-02, 7.3304e-01]],\n",
       " \n",
       "          [[1.1383e-01, 2.7238e-02, 8.4678e-03,  ..., 1.6533e-02,\n",
       "            2.4238e-03, 2.1270e-01],\n",
       "           [6.5304e-04, 3.7086e-02, 1.1632e-02,  ..., 3.3373e-02,\n",
       "            6.3474e-04, 8.8748e-03],\n",
       "           [1.7100e-02, 8.4733e-02, 1.5368e-02,  ..., 3.0411e-02,\n",
       "            6.8346e-03, 1.6608e-01],\n",
       "           ...,\n",
       "           [3.0796e-03, 4.9256e-02, 1.0673e-02,  ..., 3.3424e-02,\n",
       "            1.7502e-03, 4.3715e-02],\n",
       "           [1.3303e-03, 1.9560e-02, 1.9431e-03,  ..., 1.1446e-02,\n",
       "            2.7981e-03, 9.3693e-02],\n",
       "           [7.9473e-03, 7.9377e-04, 7.3358e-04,  ..., 6.0398e-04,\n",
       "            1.1917e-02, 8.7495e-01]],\n",
       " \n",
       "          [[2.4582e-02, 8.5601e-03, 1.5342e-03,  ..., 3.3554e-03,\n",
       "            2.7540e-02, 7.5226e-01],\n",
       "           [1.8558e-01, 6.1660e-02, 4.5639e-03,  ..., 3.5116e-06,\n",
       "            3.2952e-05, 5.8402e-01],\n",
       "           [3.8730e-02, 1.2714e-01, 5.2610e-03,  ..., 9.1434e-07,\n",
       "            1.0981e-05, 7.8097e-01],\n",
       "           ...,\n",
       "           [3.2976e-04, 2.1118e-06, 4.6823e-07,  ..., 3.8760e-03,\n",
       "            6.3146e-03, 3.8625e-01],\n",
       "           [6.7278e-04, 7.5355e-06, 2.6410e-06,  ..., 5.2190e-02,\n",
       "            3.4656e-02, 4.0999e-01],\n",
       "           [2.6382e-02, 3.9710e-03, 1.8799e-03,  ..., 1.6420e-03,\n",
       "            1.6144e-02, 7.6712e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.4692e-02, 6.9817e-03, 1.2318e-03,  ..., 7.9098e-03,\n",
       "            5.3638e-03, 8.2694e-01],\n",
       "           [9.1540e-03, 7.4043e-02, 4.8171e-02,  ..., 6.1670e-04,\n",
       "            6.7984e-04, 6.9580e-01],\n",
       "           [1.1790e-02, 1.5084e-02, 6.5171e-03,  ..., 9.1729e-05,\n",
       "            1.6523e-03, 8.5060e-01],\n",
       "           ...,\n",
       "           [8.3479e-03, 4.5164e-04, 1.6429e-04,  ..., 8.8739e-03,\n",
       "            1.1670e-02, 9.3416e-01],\n",
       "           [1.5373e-02, 7.7571e-04, 1.3738e-04,  ..., 7.6139e-02,\n",
       "            1.6647e-02, 5.5401e-01],\n",
       "           [5.6227e-03, 1.0406e-02, 2.3877e-03,  ..., 6.2330e-03,\n",
       "            2.0092e-03, 8.4355e-01]],\n",
       " \n",
       "          [[1.6317e-01, 3.1194e-03, 2.5026e-04,  ..., 2.6979e-03,\n",
       "            6.3290e-03, 3.5884e-01],\n",
       "           [3.3309e-02, 6.6945e-03, 2.0617e-03,  ..., 2.4887e-03,\n",
       "            2.4268e-02, 9.3689e-02],\n",
       "           [7.8023e-02, 1.3498e-02, 1.4719e-03,  ..., 1.9732e-03,\n",
       "            5.5037e-03, 7.1821e-01],\n",
       "           ...,\n",
       "           [5.5416e-02, 3.5892e-03, 5.6217e-04,  ..., 5.5093e-03,\n",
       "            2.8803e-02, 3.2305e-01],\n",
       "           [7.9633e-02, 2.8015e-03, 4.0835e-04,  ..., 3.3253e-03,\n",
       "            2.0959e-02, 1.9083e-01],\n",
       "           [2.0060e-03, 4.7312e-04, 1.8826e-04,  ..., 2.8212e-04,\n",
       "            6.4964e-04, 9.8212e-01]],\n",
       " \n",
       "          [[3.9016e-03, 7.8944e-03, 1.5240e-03,  ..., 3.3391e-03,\n",
       "            6.4029e-02, 2.9999e-01],\n",
       "           [2.5789e-02, 1.0807e-03, 3.1078e-03,  ..., 3.5807e-02,\n",
       "            1.5830e-02, 1.0094e-01],\n",
       "           [1.4539e-02, 6.8513e-03, 3.1091e-04,  ..., 3.7749e-03,\n",
       "            1.0116e-02, 7.6647e-01],\n",
       "           ...,\n",
       "           [1.6073e-02, 1.8400e-01, 4.7536e-03,  ..., 2.8635e-03,\n",
       "            1.6103e-02, 9.9600e-02],\n",
       "           [2.2812e-03, 1.0425e-02, 3.8887e-04,  ..., 1.9400e-03,\n",
       "            9.9068e-02, 1.8089e-01],\n",
       "           [1.2810e-02, 1.0895e-03, 3.2364e-04,  ..., 1.1589e-03,\n",
       "            1.9518e-02, 8.8103e-01]]],\n",
       " \n",
       " \n",
       "         [[[1.0450e-01, 9.9543e-03, 2.3833e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1231e-01, 1.0179e-02, 1.7986e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.2092e-02, 1.3207e-04, 2.5267e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [8.8427e-02, 5.1398e-03, 6.5093e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.1426e-02, 3.7376e-03, 5.2108e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.2604e-02, 3.7442e-03, 4.4072e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.2118e-01, 1.7439e-03, 4.6490e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.8808e-04, 7.0570e-04, 1.8199e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0058e-03, 1.1808e-03, 5.6607e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.4778e-03, 4.4078e-03, 4.5968e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.9676e-03, 4.5943e-03, 4.5768e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5778e-03, 4.2573e-03, 5.7122e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.5904e-02, 3.0328e-03, 9.8943e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7065e-01, 4.8705e-02, 6.1937e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0373e-01, 2.9331e-01, 3.7563e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [4.0532e-02, 4.0652e-04, 1.2354e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5475e-02, 6.3471e-04, 4.1467e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.8560e-02, 1.1319e-03, 1.3184e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.5995e-02, 5.7356e-04, 2.8621e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.5082e-03, 2.1910e-02, 1.1319e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5594e-02, 3.0766e-02, 6.0474e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.5596e-02, 1.9311e-03, 9.4677e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7071e-02, 2.1481e-03, 1.1820e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0894e-02, 2.2583e-03, 1.5261e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.1476e-01, 2.0412e-02, 5.0712e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.0351e-02, 2.3930e-02, 3.9721e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8664e-02, 3.5202e-02, 4.9723e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.5683e-02, 5.1595e-02, 1.0504e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.0100e-02, 5.4973e-02, 1.2317e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8826e-02, 5.3928e-02, 1.5581e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[6.1884e-03, 6.4471e-03, 1.0075e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0073e-02, 2.7870e-02, 1.3536e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.4638e-03, 1.6749e-02, 5.9846e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.0993e-03, 8.6050e-03, 7.3051e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.9710e-03, 9.2303e-03, 7.7549e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.1581e-03, 1.1954e-02, 1.3319e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.4199e-01, 5.5256e-03, 1.0483e-03,  ..., 9.3995e-03,\n",
       "            9.5605e-03, 8.8387e-02],\n",
       "           [9.4982e-04, 7.1428e-03, 2.3166e-03,  ..., 5.5189e-03,\n",
       "            1.7105e-02, 5.6024e-02],\n",
       "           [8.9192e-04, 2.3597e-02, 1.5028e-03,  ..., 1.2386e-02,\n",
       "            5.4667e-03, 5.0838e-01],\n",
       "           ...,\n",
       "           [1.4852e-03, 8.0049e-03, 3.2262e-03,  ..., 3.4657e-03,\n",
       "            4.3462e-02, 1.8667e-01],\n",
       "           [4.1714e-03, 5.3553e-03, 2.0588e-03,  ..., 2.3584e-02,\n",
       "            2.4255e-02, 2.4506e-01],\n",
       "           [3.7810e-04, 9.8949e-05, 7.7552e-05,  ..., 2.5972e-04,\n",
       "            1.1988e-03, 9.8155e-01]],\n",
       " \n",
       "          [[1.5895e-01, 3.0841e-04, 1.0705e-05,  ..., 2.2028e-05,\n",
       "            2.2179e-02, 7.9556e-01],\n",
       "           [3.6024e-03, 7.6488e-04, 3.6543e-08,  ..., 2.8614e-10,\n",
       "            5.9938e-06, 9.9296e-01],\n",
       "           [1.1635e-06, 9.9984e-01, 6.8619e-07,  ..., 4.4441e-12,\n",
       "            3.9574e-10, 1.0608e-04],\n",
       "           ...,\n",
       "           [5.6723e-03, 3.2069e-10, 5.9005e-12,  ..., 3.6374e-05,\n",
       "            5.8558e-04, 1.5232e-01],\n",
       "           [6.6515e-03, 2.4019e-08, 1.8972e-07,  ..., 7.7528e-01,\n",
       "            2.6489e-02, 1.7752e-01],\n",
       "           [3.0278e-01, 6.8516e-03, 3.3033e-03,  ..., 4.3361e-03,\n",
       "            2.3070e-02, 3.1439e-01]],\n",
       " \n",
       "          [[1.1778e-02, 8.7250e-04, 2.2813e-04,  ..., 8.7709e-04,\n",
       "            4.8439e-03, 6.8484e-01],\n",
       "           [3.4115e-03, 1.4667e-02, 4.4327e-04,  ..., 3.5442e-06,\n",
       "            6.2476e-05, 9.0994e-01],\n",
       "           [1.3323e-03, 4.1481e-03, 6.8652e-04,  ..., 8.4258e-06,\n",
       "            6.0885e-05, 9.8143e-01],\n",
       "           ...,\n",
       "           [1.5607e-02, 1.1825e-05, 1.3132e-07,  ..., 2.5006e-03,\n",
       "            6.2175e-03, 7.7241e-01],\n",
       "           [1.4727e-02, 8.6670e-06, 2.1493e-06,  ..., 1.0228e-03,\n",
       "            7.4769e-03, 4.8410e-01],\n",
       "           [2.9844e-03, 1.5092e-03, 8.5224e-04,  ..., 1.1939e-03,\n",
       "            9.0208e-04, 9.4300e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.1667e-02, 1.5550e-03, 7.9283e-04,  ..., 3.9297e-03,\n",
       "            7.0886e-04, 8.3138e-01],\n",
       "           [2.7755e-03, 1.3230e-03, 1.1574e-03,  ..., 1.2585e-06,\n",
       "            1.3487e-05, 9.4919e-01],\n",
       "           [1.5016e-02, 2.8642e-01, 4.8844e-02,  ..., 4.0177e-06,\n",
       "            6.4969e-05, 3.4318e-01],\n",
       "           ...,\n",
       "           [1.8149e-03, 6.0051e-07, 1.2981e-07,  ..., 1.8091e-03,\n",
       "            6.5489e-03, 9.8410e-01],\n",
       "           [2.7863e-02, 1.8655e-05, 4.0898e-05,  ..., 5.2558e-02,\n",
       "            4.1193e-02, 7.8192e-01],\n",
       "           [1.5816e-02, 7.1748e-04, 1.0487e-03,  ..., 7.3869e-04,\n",
       "            3.6476e-03, 8.7800e-01]],\n",
       " \n",
       "          [[1.9243e-02, 2.5982e-03, 8.2491e-04,  ..., 8.5516e-03,\n",
       "            1.9233e-02, 6.9971e-01],\n",
       "           [3.0521e-02, 1.9360e-03, 5.9779e-04,  ..., 4.2837e-07,\n",
       "            4.6920e-04, 9.5824e-01],\n",
       "           [4.1946e-02, 1.7097e-01, 2.2803e-02,  ..., 3.3770e-06,\n",
       "            1.0197e-03, 7.3446e-01],\n",
       "           ...,\n",
       "           [1.4442e-02, 1.5543e-07, 3.2509e-06,  ..., 1.0123e-03,\n",
       "            1.1686e-02, 9.6096e-01],\n",
       "           [1.4415e-02, 5.4494e-06, 1.6282e-05,  ..., 8.8373e-02,\n",
       "            1.1316e-01, 6.8058e-01],\n",
       "           [2.0060e-02, 1.1818e-03, 1.2808e-03,  ..., 5.2649e-04,\n",
       "            1.6553e-02, 8.8490e-01]],\n",
       " \n",
       "          [[3.2357e-02, 1.1735e-02, 4.4178e-03,  ..., 3.0947e-04,\n",
       "            2.2536e-03, 6.0719e-01],\n",
       "           [1.2529e-02, 7.2324e-03, 3.7662e-03,  ..., 6.3643e-06,\n",
       "            9.3826e-05, 7.0845e-01],\n",
       "           [8.2742e-04, 3.6909e-04, 2.8422e-05,  ..., 1.4553e-06,\n",
       "            1.9447e-05, 9.9280e-01],\n",
       "           ...,\n",
       "           [3.1916e-03, 3.7632e-06, 2.4067e-06,  ..., 3.9171e-04,\n",
       "            2.0829e-03, 9.8892e-01],\n",
       "           [1.4084e-02, 1.1160e-04, 1.0893e-04,  ..., 2.6911e-03,\n",
       "            1.6069e-02, 9.0770e-01],\n",
       "           [1.9637e-02, 1.4899e-04, 1.2754e-04,  ..., 4.5730e-04,\n",
       "            2.0841e-03, 9.5946e-01]]],\n",
       " \n",
       " \n",
       "         [[[6.9437e-01, 5.0945e-03, 7.0125e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.6687e-04, 4.0150e-03, 6.8176e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1985e-03, 1.8910e-02, 4.5681e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.5754e-03, 7.8604e-03, 1.9552e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7834e-03, 1.0495e-02, 2.8194e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3631e-03, 1.4759e-02, 4.0815e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.7235e-01, 6.2555e-03, 2.6081e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6903e-02, 2.2665e-03, 2.2493e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.1996e-03, 7.7971e-01, 1.4870e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.9518e-02, 4.5231e-06, 4.1049e-09,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.4055e-02, 2.6789e-05, 8.3476e-08,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.4614e-02, 1.5668e-04, 8.1418e-07,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[9.4351e-03, 1.5870e-03, 1.2354e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.3785e-03, 2.3312e-02, 1.2280e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0806e-02, 2.3851e-02, 1.6953e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [8.8173e-03, 1.4064e-04, 1.0201e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2239e-02, 2.2920e-04, 2.2579e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6534e-02, 3.6803e-04, 3.2338e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.0769e-02, 1.5511e-03, 1.5145e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.0012e-03, 9.2663e-03, 2.8101e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.9329e-03, 1.7895e-02, 1.5164e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.1704e-02, 6.6167e-04, 7.3105e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.2397e-02, 1.0891e-03, 1.0360e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.3559e-02, 1.4560e-03, 1.2953e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.4957e-02, 3.5489e-03, 1.1976e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1905e-02, 1.1507e-02, 6.0707e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6917e-02, 2.2606e-02, 9.7649e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.4092e-02, 1.1353e-04, 1.7150e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.5062e-02, 2.9661e-04, 5.8113e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.1933e-02, 7.4073e-04, 1.3796e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.4472e-02, 2.1419e-02, 8.7556e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.4916e-03, 1.4986e-02, 6.8092e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.6957e-03, 1.1955e-02, 3.3311e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [8.2796e-03, 1.1063e-04, 3.6169e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.7798e-03, 1.7544e-04, 5.4271e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0260e-02, 3.1125e-04, 1.0969e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[6.3829e-03, 3.1136e-03, 5.0756e-04,  ..., 4.7941e-02,\n",
       "            8.5587e-02, 1.0262e-01],\n",
       "           [1.3036e-02, 5.8154e-03, 3.3897e-04,  ..., 9.9701e-02,\n",
       "            1.9183e-02, 3.6789e-01],\n",
       "           [1.3926e-02, 1.4564e-03, 2.4694e-03,  ..., 2.8023e-03,\n",
       "            1.5717e-02, 8.0678e-01],\n",
       "           ...,\n",
       "           [1.7097e-02, 1.2939e-03, 4.3715e-05,  ..., 3.4333e-02,\n",
       "            3.6685e-02, 6.1947e-01],\n",
       "           [2.4250e-02, 1.2412e-03, 7.7435e-05,  ..., 1.7668e-02,\n",
       "            8.9245e-02, 3.3530e-01],\n",
       "           [1.3083e-02, 1.0006e-03, 1.0116e-03,  ..., 8.3214e-04,\n",
       "            1.3748e-03, 8.2061e-01]],\n",
       " \n",
       "          [[2.0891e-02, 6.4903e-05, 1.0041e-05,  ..., 5.4092e-03,\n",
       "            4.5412e-01, 4.3891e-01],\n",
       "           [5.7725e-02, 1.6354e-03, 5.7281e-06,  ..., 5.4952e-08,\n",
       "            1.4493e-05, 9.2943e-01],\n",
       "           [1.3320e-04, 9.0128e-01, 7.0369e-04,  ..., 3.9745e-07,\n",
       "            5.7454e-07, 9.5173e-02],\n",
       "           ...,\n",
       "           [9.2836e-05, 4.6520e-07, 1.1373e-07,  ..., 6.7796e-04,\n",
       "            3.9040e-02, 4.1704e-01],\n",
       "           [1.1760e-04, 2.1477e-07, 9.1107e-08,  ..., 4.9360e-02,\n",
       "            1.2479e-01, 8.1632e-01],\n",
       "           [3.3812e-03, 2.3053e-05, 1.2927e-05,  ..., 1.0520e-04,\n",
       "            1.2155e-02, 9.7643e-01]],\n",
       " \n",
       "          [[8.7185e-02, 2.4723e-04, 2.3770e-04,  ..., 2.7683e-04,\n",
       "            3.7101e-03, 8.7956e-01],\n",
       "           [2.9879e-03, 1.3797e-03, 8.1939e-05,  ..., 3.1443e-07,\n",
       "            7.0794e-06, 9.9239e-01],\n",
       "           [2.9877e-03, 2.4254e-02, 4.0877e-03,  ..., 5.5563e-06,\n",
       "            1.0307e-05, 9.6510e-01],\n",
       "           ...,\n",
       "           [4.0991e-03, 8.0554e-07, 1.6649e-07,  ..., 2.8561e-04,\n",
       "            1.5041e-03, 9.8874e-01],\n",
       "           [8.7485e-03, 7.8183e-07, 2.1395e-06,  ..., 6.1395e-03,\n",
       "            8.8436e-02, 8.7902e-01],\n",
       "           [6.7218e-02, 1.7322e-03, 1.0895e-03,  ..., 7.3078e-04,\n",
       "            5.2105e-03, 8.2225e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.5977e-02, 2.5444e-04, 1.7201e-05,  ..., 3.7765e-02,\n",
       "            1.6232e-01, 5.8877e-01],\n",
       "           [2.5609e-06, 2.5205e-04, 9.9946e-01,  ..., 3.7423e-09,\n",
       "            3.0514e-08, 5.8367e-05],\n",
       "           [2.6785e-10, 3.7806e-09, 6.9019e-07,  ..., 8.0961e-13,\n",
       "            5.7116e-13, 3.0192e-09],\n",
       "           ...,\n",
       "           [8.8076e-04, 1.0449e-09, 7.6437e-09,  ..., 2.2749e-04,\n",
       "            8.4336e-01, 1.5331e-01],\n",
       "           [1.6149e-04, 3.6978e-09, 6.1887e-11,  ..., 8.3497e-05,\n",
       "            4.1176e-02, 9.5826e-01],\n",
       "           [6.1867e-02, 2.9458e-03, 7.4153e-03,  ..., 7.4995e-03,\n",
       "            1.8519e-02, 7.1888e-01]],\n",
       " \n",
       "          [[1.4068e-02, 9.7056e-03, 1.7503e-03,  ..., 6.3729e-03,\n",
       "            4.7148e-03, 6.9092e-01],\n",
       "           [6.8979e-04, 4.0037e-03, 1.4236e-01,  ..., 1.0248e-05,\n",
       "            3.8695e-05, 8.1632e-01],\n",
       "           [7.2144e-04, 4.2094e-04, 1.9586e-03,  ..., 4.4123e-06,\n",
       "            1.3189e-05, 8.3225e-01],\n",
       "           ...,\n",
       "           [1.6718e-03, 1.1456e-06, 2.5913e-06,  ..., 2.0972e-02,\n",
       "            6.7122e-02, 8.9033e-01],\n",
       "           [1.3756e-03, 1.9503e-05, 1.8371e-06,  ..., 2.7903e-03,\n",
       "            1.4717e-02, 9.5007e-01],\n",
       "           [1.3969e-02, 5.5703e-04, 1.9197e-03,  ..., 1.2600e-03,\n",
       "            5.1953e-03, 8.5577e-01]],\n",
       " \n",
       "          [[5.9169e-01, 1.5948e-03, 1.2196e-03,  ..., 6.6840e-04,\n",
       "            1.5574e-03, 2.9346e-01],\n",
       "           [2.6332e-03, 1.4363e-01, 3.8601e-03,  ..., 2.1722e-03,\n",
       "            1.2307e-03, 1.9857e-01],\n",
       "           [5.6281e-03, 1.3031e-01, 2.2666e-02,  ..., 8.7579e-04,\n",
       "            5.7123e-04, 5.5269e-01],\n",
       "           ...,\n",
       "           [3.9028e-03, 3.0554e-03, 8.9566e-04,  ..., 1.6317e-01,\n",
       "            7.3951e-02, 2.1365e-01],\n",
       "           [6.7448e-03, 1.7035e-03, 6.2503e-04,  ..., 2.1738e-02,\n",
       "            6.1356e-02, 6.0622e-01],\n",
       "           [1.6303e-02, 9.1941e-04, 2.8809e-04,  ..., 3.1423e-03,\n",
       "            8.6451e-03, 8.8220e-01]]],\n",
       " \n",
       " \n",
       "         [[[8.6312e-03, 2.3640e-02, 9.0572e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.1562e-02, 4.0008e-02, 1.7211e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.7323e-02, 4.8646e-02, 6.3738e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.9886e-02, 1.1800e-02, 1.4343e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9451e-02, 1.2230e-02, 1.1372e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4456e-02, 1.6225e-02, 1.2022e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.9172e-02, 2.2199e-04, 4.8307e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.7165e-02, 1.8098e-02, 1.1484e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.3241e-03, 2.5774e-01, 1.1507e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.1687e-02, 6.0845e-05, 1.1800e-07,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.3808e-02, 1.3850e-04, 5.7603e-07,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.4199e-02, 1.7801e-04, 1.8503e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[7.1274e-02, 5.4169e-04, 3.3247e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3129e-03, 1.7587e-02, 9.6108e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0462e-03, 2.0943e-02, 1.7139e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.7685e-03, 8.6916e-05, 1.9635e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.7981e-03, 2.1661e-04, 4.2583e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.7359e-03, 3.7487e-04, 5.8720e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.8997e-02, 8.0583e-05, 1.1583e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1406e-03, 3.4833e-02, 7.2566e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4465e-05, 1.6691e-04, 3.9958e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0407e-03, 1.9873e-04, 4.1913e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3707e-04, 2.0807e-05, 1.0889e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.5944e-05, 1.4740e-06, 1.8913e-08,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.0836e-02, 5.4783e-03, 2.3345e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.6426e-04, 8.1762e-03, 2.3257e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.1690e-04, 4.9277e-03, 4.6303e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.4209e-03, 2.3043e-03, 5.5815e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2676e-03, 8.6171e-04, 2.3192e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7800e-03, 3.1512e-04, 8.8521e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[5.7418e-01, 2.1222e-03, 3.1656e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.2876e-03, 6.9801e-02, 2.8322e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8514e-03, 5.8063e-02, 1.3771e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.6199e-03, 4.7700e-03, 1.3549e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.2238e-03, 4.3272e-03, 1.1523e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.9172e-03, 5.2732e-03, 1.1478e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.7307e-03, 4.1686e-05, 2.1869e-05,  ..., 3.0684e-03,\n",
       "            5.9373e-01, 3.2806e-01],\n",
       "           [2.4411e-03, 6.0921e-03, 1.2518e-05,  ..., 1.6045e-07,\n",
       "            2.7530e-05, 9.8940e-01],\n",
       "           [4.5089e-04, 4.1598e-01, 1.2282e-02,  ..., 5.1237e-06,\n",
       "            1.7964e-04, 5.5106e-01],\n",
       "           ...,\n",
       "           [1.7904e-04, 1.3392e-06, 6.3478e-08,  ..., 1.1744e-03,\n",
       "            1.0278e-03, 9.0607e-01],\n",
       "           [2.1273e-04, 1.9708e-06, 2.4382e-06,  ..., 2.1057e-02,\n",
       "            9.1413e-02, 8.5374e-01],\n",
       "           [2.0608e-03, 1.0463e-03, 1.7745e-03,  ..., 1.2267e-03,\n",
       "            1.1304e-02, 9.0768e-01]],\n",
       " \n",
       "          [[2.5808e-02, 3.0933e-04, 5.8430e-05,  ..., 2.0823e-04,\n",
       "            7.0637e-03, 9.5276e-01],\n",
       "           [2.6302e-03, 2.1299e-03, 3.6648e-06,  ..., 4.1712e-06,\n",
       "            1.0577e-03, 9.6712e-01],\n",
       "           [2.1458e-03, 4.8567e-01, 8.2208e-04,  ..., 4.0173e-05,\n",
       "            8.3291e-05, 4.2867e-01],\n",
       "           ...,\n",
       "           [8.8021e-04, 4.6151e-06, 2.0858e-07,  ..., 1.2319e-03,\n",
       "            2.1404e-04, 8.6444e-02],\n",
       "           [6.8477e-03, 5.5565e-05, 4.7093e-05,  ..., 2.1996e-02,\n",
       "            2.1113e-02, 8.9220e-01],\n",
       "           [8.7102e-03, 8.1499e-03, 1.5414e-03,  ..., 1.7389e-03,\n",
       "            2.0811e-03, 7.6532e-01]],\n",
       " \n",
       "          [[6.2301e-03, 3.4790e-06, 1.3691e-05,  ..., 4.5256e-06,\n",
       "            4.1781e-03, 9.7764e-01],\n",
       "           [3.4159e-04, 1.9139e-03, 2.2999e-03,  ..., 2.0939e-06,\n",
       "            9.1893e-05, 9.5887e-01],\n",
       "           [1.1513e-03, 4.1996e-03, 1.8571e-03,  ..., 2.5692e-05,\n",
       "            1.6251e-04, 8.3621e-01],\n",
       "           ...,\n",
       "           [2.3824e-03, 5.1343e-06, 6.2918e-07,  ..., 1.7992e-03,\n",
       "            9.9065e-03, 9.3819e-01],\n",
       "           [7.7764e-03, 1.2623e-05, 1.5521e-05,  ..., 8.2317e-04,\n",
       "            4.5879e-02, 8.6364e-01],\n",
       "           [1.9394e-02, 2.6806e-03, 2.3924e-03,  ..., 4.3645e-03,\n",
       "            1.7369e-02, 7.1328e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.1653e-02, 4.1066e-05, 1.0302e-05,  ..., 6.3661e-05,\n",
       "            7.0796e-03, 9.3259e-01],\n",
       "           [1.5522e-03, 1.9568e-04, 8.1395e-06,  ..., 2.7384e-08,\n",
       "            4.0881e-04, 9.9576e-01],\n",
       "           [9.3020e-04, 6.2853e-03, 2.1596e-04,  ..., 1.5336e-07,\n",
       "            4.2888e-04, 9.8920e-01],\n",
       "           ...,\n",
       "           [1.9053e-03, 4.9862e-07, 3.2484e-07,  ..., 8.5162e-05,\n",
       "            3.0866e-03, 9.8983e-01],\n",
       "           [2.8052e-02, 3.2609e-06, 5.4894e-06,  ..., 1.2697e-02,\n",
       "            4.4022e-01, 4.3855e-01],\n",
       "           [1.5781e-02, 2.0380e-03, 1.9160e-03,  ..., 1.7911e-03,\n",
       "            2.3970e-02, 7.7508e-01]],\n",
       " \n",
       "          [[2.1788e-02, 4.2165e-04, 2.4852e-04,  ..., 3.0086e-03,\n",
       "            2.6397e-03, 9.3098e-01],\n",
       "           [2.8772e-04, 7.3268e-03, 2.8098e-03,  ..., 2.3343e-05,\n",
       "            4.2279e-05, 9.4390e-01],\n",
       "           [4.0036e-04, 2.1780e-02, 1.0682e-02,  ..., 1.3330e-05,\n",
       "            7.6556e-05, 9.2891e-01],\n",
       "           ...,\n",
       "           [4.4129e-03, 1.9682e-05, 3.3152e-05,  ..., 5.1552e-02,\n",
       "            6.0665e-03, 4.9087e-01],\n",
       "           [1.3139e-03, 5.0542e-05, 2.3653e-05,  ..., 3.8124e-02,\n",
       "            1.0892e-02, 3.3821e-01],\n",
       "           [6.3655e-03, 9.7392e-04, 3.2701e-04,  ..., 1.7436e-03,\n",
       "            2.0615e-03, 9.3169e-01]],\n",
       " \n",
       "          [[2.2297e-02, 3.0735e-05, 1.2557e-05,  ..., 3.3256e-03,\n",
       "            2.2539e-02, 5.5161e-01],\n",
       "           [6.1339e-05, 8.4810e-05, 5.7171e-06,  ..., 2.3356e-07,\n",
       "            4.1106e-06, 9.9901e-01],\n",
       "           [1.0117e-04, 8.2643e-03, 9.5074e-04,  ..., 8.0896e-06,\n",
       "            1.1347e-05, 9.8363e-01],\n",
       "           ...,\n",
       "           [5.3720e-04, 1.9581e-07, 1.1967e-07,  ..., 1.4680e-03,\n",
       "            7.0808e-04, 9.8157e-01],\n",
       "           [6.0522e-03, 3.1298e-06, 4.1756e-06,  ..., 1.1535e-02,\n",
       "            1.0271e-02, 8.4037e-01],\n",
       "           [3.3135e-03, 7.4139e-04, 4.2878e-04,  ..., 1.8122e-03,\n",
       "            1.4781e-03, 9.0672e-01]]],\n",
       " \n",
       " \n",
       "         [[[1.1857e-02, 6.7751e-04, 9.4114e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4870e-02, 1.5800e-02, 1.0257e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9399e-02, 3.8454e-01, 2.4029e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [9.7961e-03, 1.3487e-04, 6.6822e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3574e-02, 1.6470e-04, 1.8722e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.4980e-03, 1.1030e-04, 3.7237e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.8558e-02, 5.3917e-03, 5.1535e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6050e-03, 8.5894e-03, 1.5722e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8914e-03, 6.5994e-02, 1.4585e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [3.4368e-03, 7.6772e-04, 2.1969e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.4925e-03, 9.6301e-04, 5.6375e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.0463e-03, 1.1474e-03, 1.0247e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[7.9102e-03, 6.8985e-05, 5.2682e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1624e-03, 4.6969e-02, 1.5517e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7090e-04, 2.6897e-02, 1.7154e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.5566e-03, 1.5289e-03, 9.1439e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.9459e-03, 9.8706e-04, 1.0144e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5662e-03, 4.9942e-04, 7.4814e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[5.0266e-02, 3.1447e-04, 1.3497e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0677e-03, 2.0347e-02, 4.8001e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.1279e-04, 1.0651e-02, 1.0501e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.1649e-02, 5.2750e-04, 2.7317e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5197e-02, 5.0523e-04, 2.2180e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.3717e-02, 4.5502e-04, 1.0108e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.1585e-02, 1.5926e-04, 5.4544e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.2375e-04, 1.8760e-02, 5.2332e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.7331e-04, 7.0052e-02, 7.3458e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.2474e-03, 1.8316e-04, 2.2942e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5819e-03, 1.6766e-04, 2.7062e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2883e-03, 1.5816e-04, 4.0953e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.3693e-02, 4.1664e-05, 1.7412e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.9605e-04, 2.3313e-03, 5.7255e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.7555e-04, 4.5812e-03, 8.5507e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.7910e-03, 9.2658e-05, 4.3200e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.0772e-03, 6.3789e-05, 5.5152e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0566e-02, 2.9630e-05, 3.4150e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[8.1404e-02, 3.4726e-05, 1.9888e-06,  ..., 3.5679e-04,\n",
       "            2.7905e-02, 8.8168e-01],\n",
       "           [7.7438e-03, 1.1699e-02, 9.9525e-05,  ..., 1.1190e-06,\n",
       "            5.3916e-03, 9.3412e-01],\n",
       "           [1.4031e-02, 2.7691e-01, 2.7229e-02,  ..., 5.8555e-06,\n",
       "            3.6497e-03, 5.7743e-01],\n",
       "           ...,\n",
       "           [3.2179e-04, 1.0167e-06, 1.5140e-06,  ..., 9.8500e-03,\n",
       "            1.6597e-03, 6.2087e-02],\n",
       "           [3.6783e-03, 9.4107e-06, 3.0429e-06,  ..., 3.1497e-02,\n",
       "            1.2226e-01, 5.4244e-01],\n",
       "           [1.5586e-02, 6.8864e-03, 2.8412e-03,  ..., 4.5946e-03,\n",
       "            1.9029e-02, 7.5475e-01]],\n",
       " \n",
       "          [[4.0428e-01, 1.0531e-03, 8.8368e-05,  ..., 9.9328e-05,\n",
       "            6.5414e-04, 5.7923e-01],\n",
       "           [6.4250e-04, 3.6757e-03, 1.0079e-02,  ..., 1.4492e-06,\n",
       "            1.8521e-04, 9.1845e-01],\n",
       "           [7.4242e-05, 2.2846e-03, 7.6800e-03,  ..., 3.1062e-06,\n",
       "            5.9707e-05, 9.4467e-01],\n",
       "           ...,\n",
       "           [9.8209e-04, 5.3355e-06, 2.8255e-06,  ..., 1.0776e-02,\n",
       "            2.7364e-03, 9.1154e-01],\n",
       "           [1.3175e-01, 1.3511e-03, 1.0505e-04,  ..., 2.4365e-03,\n",
       "            3.5884e-03, 6.9149e-01],\n",
       "           [6.5630e-03, 9.3316e-04, 7.8010e-04,  ..., 1.4219e-03,\n",
       "            2.2345e-03, 9.5275e-01]],\n",
       " \n",
       "          [[9.2282e-03, 2.0700e-03, 5.2320e-04,  ..., 9.6363e-03,\n",
       "            1.6026e-02, 4.1238e-02],\n",
       "           [1.8273e-02, 1.3341e-02, 3.9366e-04,  ..., 1.2230e-01,\n",
       "            3.1993e-03, 1.5816e-01],\n",
       "           [1.0172e-02, 5.5243e-03, 6.7747e-04,  ..., 1.1256e-02,\n",
       "            6.8229e-03, 7.8236e-01],\n",
       "           ...,\n",
       "           [6.3614e-03, 7.2081e-02, 1.0951e-03,  ..., 1.0387e-02,\n",
       "            1.5328e-03, 4.2696e-02],\n",
       "           [6.9185e-02, 6.0295e-03, 7.2050e-04,  ..., 5.0735e-03,\n",
       "            4.4732e-02, 2.6445e-01],\n",
       "           [8.0605e-03, 5.2741e-04, 4.3753e-04,  ..., 2.0473e-03,\n",
       "            4.5044e-03, 9.4019e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.1207e-01, 7.8714e-04, 1.4658e-04,  ..., 4.2063e-05,\n",
       "            5.9108e-04, 8.5843e-01],\n",
       "           [1.7206e-03, 3.5425e-02, 2.4237e-02,  ..., 9.8070e-06,\n",
       "            7.3179e-05, 6.3739e-01],\n",
       "           [1.5059e-03, 6.6454e-03, 2.6976e-02,  ..., 1.9209e-05,\n",
       "            1.3056e-04, 5.4589e-01],\n",
       "           ...,\n",
       "           [3.5684e-03, 2.5989e-06, 1.7080e-06,  ..., 2.0470e-02,\n",
       "            1.8121e-02, 9.2524e-01],\n",
       "           [3.6369e-02, 3.0912e-03, 4.0265e-04,  ..., 2.0945e-04,\n",
       "            5.1494e-03, 7.9345e-01],\n",
       "           [4.9936e-02, 4.2959e-03, 3.0234e-03,  ..., 5.2604e-03,\n",
       "            2.4672e-03, 7.5444e-01]],\n",
       " \n",
       "          [[1.7062e-01, 1.3742e-03, 2.0154e-04,  ..., 3.7366e-04,\n",
       "            1.9035e-03, 7.8920e-01],\n",
       "           [4.4391e-04, 1.5056e-02, 3.3799e-03,  ..., 8.6852e-06,\n",
       "            9.9725e-04, 6.5018e-01],\n",
       "           [3.1139e-04, 2.8190e-04, 6.0020e-03,  ..., 5.2484e-06,\n",
       "            3.0504e-04, 9.3571e-01],\n",
       "           ...,\n",
       "           [4.3190e-03, 1.4566e-05, 1.1965e-05,  ..., 3.4230e-03,\n",
       "            6.6140e-02, 6.9236e-01],\n",
       "           [4.0701e-03, 2.1071e-04, 1.3432e-04,  ..., 3.1889e-02,\n",
       "            2.1194e-02, 7.1576e-01],\n",
       "           [6.8634e-03, 2.8369e-03, 3.3934e-03,  ..., 1.5539e-03,\n",
       "            2.4814e-03, 7.6294e-01]],\n",
       " \n",
       "          [[4.8252e-03, 1.1602e-03, 4.3498e-04,  ..., 1.0727e-03,\n",
       "            2.8001e-02, 1.9183e-01],\n",
       "           [4.1201e-02, 7.7212e-02, 7.7603e-04,  ..., 3.0257e-03,\n",
       "            2.5070e-03, 7.4742e-01],\n",
       "           [8.9436e-03, 1.0497e-02, 2.8481e-02,  ..., 1.4880e-03,\n",
       "            7.2083e-04, 8.9755e-01],\n",
       "           ...,\n",
       "           [7.7429e-04, 1.6148e-03, 3.7761e-04,  ..., 5.8736e-03,\n",
       "            1.9017e-04, 3.0443e-02],\n",
       "           [3.2511e-02, 1.4887e-02, 9.6620e-04,  ..., 1.8834e-03,\n",
       "            1.8751e-02, 9.6100e-02],\n",
       "           [7.0014e-03, 1.3494e-03, 1.2427e-03,  ..., 3.2709e-03,\n",
       "            5.7502e-03, 8.7549e-01]]],\n",
       " \n",
       " \n",
       "         [[[1.3564e-01, 1.2594e-04, 2.3907e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4742e-04, 3.9749e-03, 4.6204e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9699e-04, 1.9219e-02, 9.2968e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.7183e-04, 6.6636e-05, 3.9000e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1546e-03, 8.9707e-05, 7.3794e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5509e-03, 1.3301e-04, 1.8218e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[4.0423e-01, 1.1293e-04, 3.8299e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8058e-04, 7.9573e-03, 2.1310e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0112e-04, 1.7498e-02, 7.0361e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.6828e-03, 3.6675e-04, 1.4657e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.7059e-03, 3.9139e-04, 1.8955e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8497e-02, 7.2867e-04, 8.8317e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.0945e-02, 4.9529e-02, 3.8046e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.3480e-01, 1.6324e-02, 1.2643e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.9358e-02, 7.5329e-03, 1.8226e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.1152e-02, 5.7341e-03, 7.8431e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6634e-02, 7.0203e-03, 9.5215e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8582e-02, 7.1978e-03, 1.2297e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[9.9327e-02, 1.1122e-02, 4.6564e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5925e-04, 9.1958e-02, 1.3691e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7991e-04, 2.1017e-02, 6.6930e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.1914e-03, 5.2645e-03, 2.7550e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.8870e-03, 1.4806e-02, 5.4148e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.4805e-03, 7.0194e-02, 2.1106e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.1250e-01, 2.2509e-04, 1.2020e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.4482e-04, 8.8792e-03, 5.4743e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8676e-04, 2.0933e-02, 5.9138e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.1164e-04, 2.9086e-04, 5.5558e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.6131e-04, 6.6578e-04, 8.2840e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5486e-03, 1.8109e-03, 1.5886e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[9.3666e-03, 5.0311e-03, 3.3001e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4433e-02, 2.9235e-02, 8.2385e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3531e-02, 1.0535e-02, 1.9352e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0485e-02, 1.2830e-03, 1.0332e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3918e-02, 1.2760e-03, 1.1166e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1582e-02, 1.3433e-03, 9.5609e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[4.2096e-02, 4.1139e-03, 2.2623e-04,  ..., 1.0342e-02,\n",
       "            1.0205e-01, 3.1153e-01],\n",
       "           [3.3079e-03, 3.8837e-02, 5.1844e-02,  ..., 1.9634e-04,\n",
       "            8.1777e-04, 2.3874e-01],\n",
       "           [3.0202e-03, 2.1108e-02, 2.4358e-02,  ..., 1.2206e-04,\n",
       "            8.4020e-04, 4.4082e-01],\n",
       "           ...,\n",
       "           [1.7799e-03, 1.5355e-05, 2.4783e-05,  ..., 2.2812e-01,\n",
       "            8.1640e-04, 6.9466e-01],\n",
       "           [2.1869e-02, 5.6191e-03, 6.9059e-04,  ..., 6.5643e-03,\n",
       "            3.5886e-03, 6.0330e-01],\n",
       "           [2.7545e-02, 2.4139e-03, 1.3138e-03,  ..., 3.8925e-03,\n",
       "            1.3400e-03, 6.7434e-01]],\n",
       " \n",
       "          [[4.5451e-02, 7.8349e-03, 9.3008e-04,  ..., 3.0262e-03,\n",
       "            8.0219e-03, 3.8779e-03],\n",
       "           [3.6930e-04, 2.8890e-01, 2.6563e-04,  ..., 4.9249e-04,\n",
       "            2.7134e-04, 6.4665e-01],\n",
       "           [1.1839e-04, 1.1489e-03, 1.1315e-01,  ..., 2.2172e-05,\n",
       "            4.8428e-05, 8.7365e-01],\n",
       "           ...,\n",
       "           [1.5396e-03, 1.2230e-03, 1.0356e-04,  ..., 5.5410e-02,\n",
       "            1.6727e-03, 8.5755e-01],\n",
       "           [6.2925e-03, 1.2033e-03, 5.3691e-04,  ..., 1.4360e-04,\n",
       "            1.2877e-01, 6.5379e-01],\n",
       "           [8.3475e-03, 1.1470e-03, 7.2934e-04,  ..., 4.5812e-04,\n",
       "            6.3050e-03, 8.7362e-01]],\n",
       " \n",
       "          [[1.2882e-01, 9.9749e-04, 8.2079e-05,  ..., 2.2089e-03,\n",
       "            1.3265e-01, 4.2555e-02],\n",
       "           [4.1369e-03, 1.2203e-03, 3.0824e-04,  ..., 1.8668e-04,\n",
       "            9.1362e-04, 9.6472e-01],\n",
       "           [2.4580e-03, 4.0944e-02, 6.1259e-03,  ..., 4.4575e-04,\n",
       "            8.1544e-04, 8.8325e-01],\n",
       "           ...,\n",
       "           [1.4661e-02, 1.0836e-04, 2.5869e-04,  ..., 1.0174e-02,\n",
       "            5.9508e-04, 9.2270e-01],\n",
       "           [5.2034e-01, 5.0104e-04, 2.6936e-04,  ..., 4.0141e-03,\n",
       "            3.8111e-03, 1.1527e-01],\n",
       "           [9.1461e-03, 4.8035e-03, 5.1443e-03,  ..., 8.9492e-03,\n",
       "            6.7469e-03, 4.5784e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.8935e-01, 2.0252e-04, 8.2586e-06,  ..., 2.4377e-04,\n",
       "            1.0743e-03, 7.7110e-01],\n",
       "           [2.5018e-04, 8.2406e-03, 3.4351e-03,  ..., 2.8551e-04,\n",
       "            8.2253e-05, 8.7326e-01],\n",
       "           [6.5003e-05, 2.2008e-03, 8.4612e-04,  ..., 4.9952e-05,\n",
       "            7.8805e-06, 9.8525e-01],\n",
       "           ...,\n",
       "           [2.4810e-04, 4.7949e-05, 2.9503e-05,  ..., 4.4966e-02,\n",
       "            4.1883e-05, 9.0319e-01],\n",
       "           [9.3197e-03, 4.6193e-04, 1.6423e-04,  ..., 7.7301e-03,\n",
       "            7.0255e-03, 4.0752e-01],\n",
       "           [6.8393e-03, 2.4798e-03, 7.7129e-04,  ..., 1.7540e-03,\n",
       "            3.1365e-03, 9.0091e-01]],\n",
       " \n",
       "          [[4.3269e-02, 1.1505e-02, 2.2318e-03,  ..., 2.1851e-03,\n",
       "            8.7892e-03, 1.0406e-02],\n",
       "           [4.4717e-03, 2.4924e-02, 1.3104e-02,  ..., 2.9779e-03,\n",
       "            8.5592e-03, 2.8941e-01],\n",
       "           [4.3028e-03, 4.8292e-02, 6.1569e-03,  ..., 1.4345e-02,\n",
       "            2.6618e-03, 2.6883e-01],\n",
       "           ...,\n",
       "           [1.0286e-02, 8.7715e-03, 2.8669e-03,  ..., 1.1634e-02,\n",
       "            3.3157e-03, 5.7100e-01],\n",
       "           [1.8377e-02, 2.5327e-03, 6.8123e-04,  ..., 3.7179e-03,\n",
       "            2.8256e-03, 2.4900e-02],\n",
       "           [3.3034e-03, 4.8530e-03, 6.3959e-03,  ..., 5.4632e-03,\n",
       "            1.0340e-02, 4.8956e-01]],\n",
       " \n",
       "          [[6.8898e-03, 1.4896e-03, 3.7395e-05,  ..., 5.2205e-03,\n",
       "            6.7133e-01, 9.6517e-02],\n",
       "           [4.4406e-04, 1.8869e-03, 1.7429e-02,  ..., 5.6263e-06,\n",
       "            2.7569e-05, 9.7031e-01],\n",
       "           [9.4180e-05, 4.1543e-04, 8.9094e-03,  ..., 4.8931e-06,\n",
       "            7.5381e-05, 6.4623e-01],\n",
       "           ...,\n",
       "           [2.0007e-03, 9.3415e-07, 2.1143e-06,  ..., 9.0072e-04,\n",
       "            1.2326e-03, 9.9400e-01],\n",
       "           [1.2513e-02, 9.3693e-02, 1.4797e-03,  ..., 4.8426e-03,\n",
       "            6.3720e-03, 7.9034e-02],\n",
       "           [3.6212e-02, 1.3439e-02, 2.3232e-03,  ..., 1.0880e-02,\n",
       "            3.0786e-02, 4.1718e-01]]],\n",
       " \n",
       " \n",
       "         [[[3.0590e-02, 5.3759e-03, 2.3830e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.8905e-03, 2.9469e-02, 9.4233e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4439e-03, 8.2771e-03, 8.6433e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.0085e-03, 1.5338e-03, 1.7044e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.4263e-03, 1.6354e-03, 2.3859e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.0012e-03, 1.2026e-03, 4.9100e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[9.8187e-02, 2.6247e-02, 2.3053e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.2689e-03, 1.2643e-01, 1.4615e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2537e-03, 4.6514e-03, 1.7164e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.3425e-02, 2.3056e-02, 1.9112e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3995e-02, 2.3269e-02, 2.4755e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.9021e-03, 1.4208e-02, 2.0076e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.4755e-01, 2.5144e-02, 1.5944e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4158e-03, 6.5632e-02, 4.7228e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.5723e-04, 5.7549e-01, 2.0756e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.5515e-02, 2.9192e-03, 3.0924e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.0533e-02, 2.5131e-03, 2.6941e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.7759e-02, 2.6342e-03, 3.6712e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.3689e-01, 6.9368e-04, 9.3089e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.4855e-04, 1.1158e-02, 1.3822e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8891e-04, 1.8789e-02, 2.8236e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.7208e-03, 5.9940e-04, 5.4296e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.9477e-03, 7.8691e-04, 8.1128e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0161e-03, 1.3386e-03, 1.6233e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.1906e-01, 3.1157e-02, 1.0691e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0919e-03, 3.6667e-02, 8.1997e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.1668e-04, 4.2011e-03, 5.9596e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.5484e-03, 1.8917e-02, 3.0761e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1254e-03, 1.8471e-02, 2.8525e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.8986e-03, 2.5065e-02, 5.3057e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.9119e-03, 9.5981e-03, 2.2828e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0318e-03, 2.4554e-02, 1.8222e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6415e-04, 2.1564e-02, 3.4785e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [9.3213e-03, 8.2984e-03, 1.3904e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.1395e-02, 1.9983e-02, 2.5583e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.8236e-03, 2.6104e-02, 2.1849e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[1.8284e-02, 2.8224e-02, 5.7247e-03,  ..., 1.2724e-03,\n",
       "            4.2056e-03, 1.8992e-01],\n",
       "           [2.3414e-02, 1.4723e-03, 8.9286e-03,  ..., 5.4274e-03,\n",
       "            1.1697e-03, 7.3860e-01],\n",
       "           [1.4443e-03, 8.9847e-04, 3.3551e-03,  ..., 3.3793e-03,\n",
       "            2.2136e-04, 8.9840e-01],\n",
       "           ...,\n",
       "           [1.0940e-02, 1.1947e-03, 2.3205e-03,  ..., 1.4289e-02,\n",
       "            7.9520e-04, 7.9394e-01],\n",
       "           [1.2464e-01, 4.4960e-03, 1.5044e-03,  ..., 7.9171e-03,\n",
       "            6.5860e-03, 3.9209e-01],\n",
       "           [9.0965e-03, 6.8465e-03, 2.9240e-03,  ..., 6.6902e-03,\n",
       "            1.8568e-03, 5.4147e-01]],\n",
       " \n",
       "          [[3.7422e-02, 6.1565e-04, 4.8757e-05,  ..., 1.2685e-03,\n",
       "            3.2422e-02, 1.4693e-02],\n",
       "           [7.7200e-03, 2.0207e-02, 1.1620e-02,  ..., 9.1435e-03,\n",
       "            1.8144e-02, 5.2177e-01],\n",
       "           [3.6548e-03, 1.1113e-02, 2.5387e-02,  ..., 8.9225e-04,\n",
       "            9.2524e-04, 7.2518e-01],\n",
       "           ...,\n",
       "           [5.3544e-03, 8.4837e-03, 1.4666e-03,  ..., 8.4961e-02,\n",
       "            1.1382e-02, 2.5842e-01],\n",
       "           [7.8517e-02, 2.1865e-03, 5.4720e-04,  ..., 7.5816e-04,\n",
       "            3.3639e-02, 2.5258e-02],\n",
       "           [1.3976e-02, 7.5063e-03, 1.8312e-02,  ..., 1.1317e-02,\n",
       "            1.1573e-02, 3.3503e-01]],\n",
       " \n",
       "          [[1.3831e-02, 1.2034e-03, 2.5086e-04,  ..., 5.0613e-03,\n",
       "            4.8028e-02, 1.7517e-01],\n",
       "           [2.7633e-03, 5.0739e-02, 2.0081e-03,  ..., 4.1421e-03,\n",
       "            1.4270e-02, 7.7034e-01],\n",
       "           [4.0527e-04, 4.9902e-05, 6.5912e-01,  ..., 9.9771e-06,\n",
       "            1.3610e-03, 3.3541e-01],\n",
       "           ...,\n",
       "           [1.7455e-03, 5.2707e-04, 4.1291e-05,  ..., 1.7602e-01,\n",
       "            1.4903e-03, 5.0944e-01],\n",
       "           [3.7300e-02, 2.2656e-03, 4.4763e-04,  ..., 2.2397e-03,\n",
       "            6.5944e-02, 1.5788e-01],\n",
       "           [2.1634e-02, 1.3880e-03, 1.0011e-02,  ..., 1.8480e-03,\n",
       "            1.1973e-02, 7.1615e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.4789e-01, 5.9379e-05, 6.5133e-05,  ..., 6.3298e-05,\n",
       "            4.2198e-01, 5.9594e-02],\n",
       "           [4.7369e-03, 4.8767e-02, 1.0771e-02,  ..., 2.8764e-05,\n",
       "            2.2234e-03, 5.8431e-01],\n",
       "           [1.4593e-03, 5.3699e-01, 8.2260e-03,  ..., 1.2548e-04,\n",
       "            1.4851e-04, 2.7405e-01],\n",
       "           ...,\n",
       "           [1.0571e-03, 5.3963e-06, 2.6197e-05,  ..., 1.8637e-02,\n",
       "            1.8875e-03, 3.1843e-01],\n",
       "           [1.5305e-02, 6.9051e-05, 1.2770e-04,  ..., 1.4176e-03,\n",
       "            1.0959e-03, 8.9935e-01],\n",
       "           [3.5273e-02, 1.8874e-03, 4.0314e-03,  ..., 5.8565e-03,\n",
       "            6.3014e-03, 5.7597e-01]],\n",
       " \n",
       "          [[3.3520e-01, 3.5745e-03, 1.5183e-03,  ..., 1.9876e-03,\n",
       "            6.2244e-03, 5.4669e-02],\n",
       "           [1.0652e-01, 1.0380e-02, 8.0912e-04,  ..., 3.2492e-03,\n",
       "            1.4763e-02, 2.3658e-01],\n",
       "           [3.0647e-02, 1.6016e-02, 3.1091e-03,  ..., 3.0027e-03,\n",
       "            5.0317e-03, 3.8988e-01],\n",
       "           ...,\n",
       "           [1.3103e-01, 3.4198e-02, 1.4480e-03,  ..., 3.1934e-03,\n",
       "            1.1739e-02, 2.6098e-01],\n",
       "           [1.9761e-01, 2.5089e-03, 4.5074e-04,  ..., 1.6225e-03,\n",
       "            6.2120e-02, 1.0308e-01],\n",
       "           [4.6312e-02, 3.0747e-03, 8.2404e-03,  ..., 4.6780e-03,\n",
       "            7.4528e-02, 8.2643e-03]],\n",
       " \n",
       "          [[5.3671e-03, 2.3140e-03, 4.5489e-04,  ..., 3.3134e-03,\n",
       "            9.8813e-02, 1.1132e-01],\n",
       "           [3.3268e-02, 2.6367e-02, 1.9782e-03,  ..., 5.4624e-03,\n",
       "            2.4495e-03, 4.1205e-01],\n",
       "           [3.6084e-02, 8.0034e-03, 6.5574e-04,  ..., 2.1613e-03,\n",
       "            2.7697e-03, 6.8881e-01],\n",
       "           ...,\n",
       "           [3.6834e-02, 5.3496e-03, 5.6631e-04,  ..., 2.7086e-03,\n",
       "            1.0852e-02, 5.3826e-01],\n",
       "           [1.7484e-01, 9.3079e-03, 1.1727e-03,  ..., 4.5106e-03,\n",
       "            9.4530e-03, 6.2037e-02],\n",
       "           [1.7480e-02, 8.7371e-03, 1.7698e-03,  ..., 2.9244e-03,\n",
       "            2.8041e-03, 7.8760e-01]]],\n",
       " \n",
       " \n",
       "         [[[4.9160e-03, 2.8935e-02, 3.1098e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4276e-02, 9.5781e-03, 9.4693e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.5057e-03, 1.0977e-02, 1.2223e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.5566e-02, 5.0987e-03, 9.2724e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.7758e-02, 5.1048e-03, 1.4325e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.1850e-02, 1.0008e-02, 3.7260e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.2825e-01, 2.4672e-02, 4.2189e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0316e-02, 8.6913e-02, 5.9597e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.6971e-03, 1.7024e-02, 1.6393e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [9.7060e-02, 4.1806e-02, 3.6548e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.6386e-01, 4.5423e-02, 5.7025e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.9113e-01, 7.2783e-03, 4.5941e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.4524e-02, 7.8328e-04, 2.4046e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.3681e-03, 1.6161e-02, 9.3712e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.1327e-03, 2.8871e-03, 1.5248e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [6.1377e-03, 9.4833e-04, 3.7998e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.9283e-03, 1.1099e-03, 5.9321e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.8963e-02, 1.1950e-03, 7.4974e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[6.7608e-02, 1.9131e-03, 3.8164e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7929e-03, 2.8426e-02, 5.0514e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4580e-04, 1.8586e-02, 2.1522e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.1511e-02, 3.3462e-04, 7.9206e-06,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3935e-02, 4.8463e-04, 1.1530e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2500e-02, 5.4720e-04, 2.4667e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[2.5672e-01, 2.3180e-02, 4.3613e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.6557e-02, 2.7036e-02, 2.7405e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.8489e-02, 4.8899e-03, 1.3222e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.7728e-02, 5.2703e-02, 1.6910e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.6127e-02, 6.5008e-02, 7.5483e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.7326e-02, 5.1461e-02, 4.9644e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[5.9005e-03, 2.8186e-02, 3.1359e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0856e-02, 1.4334e-02, 6.2208e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.0468e-03, 5.1083e-02, 7.6336e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.4910e-03, 8.5061e-03, 2.7891e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4807e-02, 1.0643e-02, 4.6027e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5728e-02, 1.4830e-02, 9.1042e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[[5.1341e-02, 7.4733e-03, 1.3101e-03,  ..., 3.3730e-03,\n",
       "            6.0700e-03, 4.0988e-01],\n",
       "           [1.3340e-03, 7.1289e-03, 1.4912e-03,  ..., 1.1837e-03,\n",
       "            4.2735e-04, 9.2006e-01],\n",
       "           [3.9044e-04, 3.0227e-03, 4.2925e-04,  ..., 1.8465e-04,\n",
       "            1.2615e-04, 9.6777e-01],\n",
       "           ...,\n",
       "           [1.7822e-03, 3.2752e-03, 1.9338e-04,  ..., 4.1643e-04,\n",
       "            9.7472e-04, 9.1458e-01],\n",
       "           [8.6066e-03, 8.7201e-03, 1.4516e-03,  ..., 2.0938e-03,\n",
       "            4.8259e-03, 6.9175e-01],\n",
       "           [1.4109e-02, 1.2563e-02, 1.2933e-02,  ..., 1.0871e-02,\n",
       "            1.3325e-02, 1.7656e-02]],\n",
       " \n",
       "          [[6.0498e-01, 1.2864e-03, 8.8298e-05,  ..., 2.6735e-03,\n",
       "            2.3540e-02, 5.1349e-02],\n",
       "           [7.4744e-04, 9.6491e-03, 1.1796e-03,  ..., 2.9463e-03,\n",
       "            3.4926e-03, 8.5597e-01],\n",
       "           [7.9377e-05, 1.4392e-03, 2.4385e-02,  ..., 2.2794e-04,\n",
       "            9.2481e-05, 9.5697e-01],\n",
       "           ...,\n",
       "           [3.1976e-04, 4.5696e-03, 3.1981e-04,  ..., 1.7358e-02,\n",
       "            9.3059e-05, 8.6257e-01],\n",
       "           [3.5332e-02, 1.4596e-02, 8.9864e-04,  ..., 4.3151e-03,\n",
       "            1.2666e-02, 7.3424e-01],\n",
       "           [1.4940e-02, 1.8898e-02, 9.3646e-03,  ..., 1.5950e-02,\n",
       "            1.6746e-02, 2.1494e-02]],\n",
       " \n",
       "          [[8.6917e-02, 1.8151e-03, 6.5505e-05,  ..., 1.5200e-03,\n",
       "            7.5899e-03, 6.3607e-01],\n",
       "           [7.8844e-04, 1.1938e-02, 1.4361e-03,  ..., 6.6223e-03,\n",
       "            1.7134e-03, 8.8410e-01],\n",
       "           [2.2599e-05, 4.7171e-05, 8.8682e-01,  ..., 2.5248e-06,\n",
       "            4.1249e-05, 1.0999e-01],\n",
       "           ...,\n",
       "           [1.9240e-03, 6.1711e-03, 1.0852e-04,  ..., 2.0595e-01,\n",
       "            3.9821e-04, 4.7581e-01],\n",
       "           [2.8033e-02, 6.9521e-03, 8.0679e-04,  ..., 3.5786e-03,\n",
       "            2.5723e-02, 2.9688e-01],\n",
       "           [1.3526e-02, 1.4032e-02, 1.2978e-02,  ..., 1.5233e-02,\n",
       "            1.3660e-02, 4.7105e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.3698e-01, 6.0954e-03, 2.3668e-03,  ..., 1.0277e-03,\n",
       "            4.3656e-02, 1.3687e-01],\n",
       "           [3.7697e-03, 3.4386e-02, 1.4831e-03,  ..., 5.4122e-04,\n",
       "            2.2261e-03, 7.3092e-01],\n",
       "           [3.2365e-04, 2.6475e-03, 7.3929e-03,  ..., 1.5766e-04,\n",
       "            4.7102e-04, 9.2630e-01],\n",
       "           ...,\n",
       "           [1.2926e-02, 3.4826e-04, 1.3713e-04,  ..., 3.4400e-03,\n",
       "            7.5565e-03, 4.8832e-01],\n",
       "           [4.6729e-02, 1.4604e-03, 1.8467e-04,  ..., 1.8163e-04,\n",
       "            1.3481e-02, 1.4856e-01],\n",
       "           [1.2625e-02, 7.5618e-03, 8.8981e-03,  ..., 7.4359e-03,\n",
       "            1.0491e-02, 1.0724e-02]],\n",
       " \n",
       "          [[5.0783e-03, 2.4967e-03, 1.1785e-03,  ..., 1.2443e-02,\n",
       "            2.1481e-02, 4.2427e-01],\n",
       "           [4.1292e-04, 1.4120e-02, 6.9840e-03,  ..., 2.9444e-03,\n",
       "            1.4736e-03, 7.6847e-01],\n",
       "           [9.9583e-06, 1.6606e-03, 1.4456e-02,  ..., 1.8746e-04,\n",
       "            8.0440e-05, 9.7019e-01],\n",
       "           ...,\n",
       "           [2.9950e-03, 2.6043e-03, 4.3649e-04,  ..., 3.3913e-02,\n",
       "            5.4874e-04, 8.3778e-01],\n",
       "           [8.1828e-03, 6.2925e-03, 2.7647e-04,  ..., 5.7570e-03,\n",
       "            5.2966e-03, 7.5653e-01],\n",
       "           [9.8471e-03, 1.1019e-02, 1.1294e-02,  ..., 1.8540e-02,\n",
       "            1.2889e-02, 2.8496e-02]],\n",
       " \n",
       "          [[7.9334e-02, 1.2663e-03, 6.2400e-04,  ..., 8.3278e-04,\n",
       "            4.3000e-02, 1.5414e-01],\n",
       "           [7.3179e-04, 1.1632e-02, 5.7834e-03,  ..., 3.8485e-03,\n",
       "            4.7160e-03, 8.0291e-01],\n",
       "           [5.4985e-05, 1.0791e-04, 9.4298e-01,  ..., 1.7906e-05,\n",
       "            6.8842e-05, 5.0050e-02],\n",
       "           ...,\n",
       "           [3.8231e-04, 6.7130e-03, 1.6721e-04,  ..., 3.2424e-01,\n",
       "            4.2043e-04, 2.3597e-01],\n",
       "           [4.4266e-02, 8.2957e-03, 9.8647e-04,  ..., 8.9840e-04,\n",
       "            5.0344e-02, 1.7431e-01],\n",
       "           [1.1931e-02, 1.2128e-02, 8.7088e-03,  ..., 9.8515e-03,\n",
       "            1.8040e-02, 2.1049e-02]]],\n",
       " \n",
       " \n",
       "         [[[5.4321e-02, 5.2373e-03, 2.9862e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.4212e-03, 1.3327e-03, 3.0785e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [6.6048e-04, 1.6655e-03, 1.3688e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0852e-02, 6.8259e-03, 1.2098e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0545e-02, 1.0883e-02, 1.7403e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.3254e-03, 3.1595e-02, 6.4911e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.5417e-01, 1.1957e-02, 1.0201e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2727e-03, 1.4528e-03, 6.7490e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.0592e-04, 2.0816e-04, 1.1922e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.0027e-02, 3.6861e-03, 6.9360e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [8.9167e-03, 2.7265e-03, 4.9744e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.3869e-02, 1.7059e-03, 4.8714e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.4781e-02, 2.2211e-03, 8.4469e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.2582e-03, 1.3240e-02, 7.6313e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.5165e-03, 2.4111e-03, 2.7625e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [1.2202e-03, 1.0389e-03, 9.4890e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [7.1116e-04, 6.5185e-04, 6.8663e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.4963e-03, 7.0255e-04, 1.1438e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.0718e-01, 4.0140e-03, 1.7477e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.9951e-02, 5.6897e-02, 1.2042e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.6124e-04, 8.8794e-03, 2.8371e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [5.8640e-03, 3.0715e-03, 2.4687e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.5658e-02, 3.9499e-03, 5.4585e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [2.6015e-02, 5.6207e-03, 2.2202e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[1.9855e-02, 6.8604e-03, 7.6322e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0702e-03, 1.4227e-03, 3.4777e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.7526e-05, 7.4692e-04, 2.3329e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.4135e-03, 4.6602e-03, 4.9280e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [5.5512e-03, 3.9254e-03, 5.3872e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.9853e-03, 1.9771e-03, 9.0792e-05,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          [[3.7647e-02, 3.1112e-03, 7.0399e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.0900e-03, 2.1746e-02, 4.5687e-03,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [3.5349e-03, 1.8876e-03, 2.8066e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [2.1291e-03, 7.7767e-03, 3.6353e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.7661e-03, 9.6428e-03, 3.5952e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.3632e-03, 1.2166e-02, 5.2506e-04,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac9f2bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 99, 99])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2600d997",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (2, 12, 99, 99) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7156\\762655650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mmatshow\u001b[1;34m(A, fignum, **kwargs)\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfignum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfigaspect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2193\u001b[0m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.09\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.775\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.775\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2194\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2195\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2196\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mmatshow\u001b[1;34m(self, Z, **kwargs)\u001b[0m\n\u001b[0;32m   7832\u001b[0m               \u001b[1;34m'aspect'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'equal'\u001b[0m\u001b[1;33m,\u001b[0m          \u001b[1;31m# (already the imshow default)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7833\u001b[0m               **kwargs}\n\u001b[1;32m-> 7834\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7835\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7836\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick_top\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;31m# Don't modify *func*'s signature, as boilerplate.py needs it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1412\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5486\u001b[0m                               **kwargs)\n\u001b[0;32m   5487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5488\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5489\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\DL\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    714\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m    715\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[1;32m--> 716\u001b[1;33m                             .format(self._A.shape))\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (2, 12, 99, 99) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x192 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(att[-1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de73d18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 66, 74, 81, 89, 94],\n",
       "        [ 1, 16, 35, 65,  0,  0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouns_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0644f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6db64c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 99, 768])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "837f5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_embeddings[1][pronouns_offsets[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "070fb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = bert_embeddings[0][pronouns_offsets[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "daeb28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tensor = torch.zeros(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fdcb6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0594, -0.1547, -0.3239,  ...,  0.6756, -0.0039, -0.2668],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([t, pad_tensor], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55ad949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5304, -0.7988,  0.6365,  ..., -0.3602,  0.1951,  0.0088],\n",
      "         [-0.0570, -0.2975,  0.3291,  ..., -0.2745,  0.4777,  0.0476],\n",
      "         [-0.0615, -0.4559,  0.4566,  ..., -0.1023,  0.5021,  0.4123],\n",
      "         [ 0.1311, -0.6237,  0.5702,  ..., -0.0278,  0.6753,  0.3164],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[[ 0.5304, -0.7988,  0.6365,  ..., -0.3602,  0.1951,  0.0088],\n",
      "         [-0.0570, -0.2975,  0.3291,  ..., -0.2745,  0.4777,  0.0476],\n",
      "         [-0.0615, -0.4559,  0.4566,  ..., -0.1023,  0.5021,  0.4123],\n",
      "         [ 0.1311, -0.6237,  0.5702,  ..., -0.0278,  0.6753,  0.3164],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[[ 0.5304, -0.7988,  0.6365,  ..., -0.3602,  0.1951,  0.0088],\n",
      "         [-0.0570, -0.2975,  0.3291,  ..., -0.2745,  0.4777,  0.0476],\n",
      "         [-0.0615, -0.4559,  0.4566,  ..., -0.1023,  0.5021,  0.4123],\n",
      "         [ 0.1311, -0.6237,  0.5702,  ..., -0.0278,  0.6753,  0.3164],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([[[ 0.5304, -0.7988,  0.6365,  ..., -0.3602,  0.1951,  0.0088],\n",
      "         [-0.0570, -0.2975,  0.3291,  ..., -0.2745,  0.4777,  0.0476],\n",
      "         [-0.0615, -0.4559,  0.4566,  ..., -0.1023,  0.5021,  0.4123],\n",
      "         [ 0.1311, -0.6237,  0.5702,  ..., -0.0278,  0.6753,  0.3164],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358],\n",
      "         [ 0.6046, -0.2931,  0.1895,  ..., -0.4473,  0.2404, -0.1358]]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "tensor([], size=(0, 6, 768), grad_fn=<IndexBackward0>)\n",
      "tensor([], size=(0, 6, 768), grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for mask_val in mask:\n",
    "#     if not mask_val:\n",
    "    print(embeddings[offsets[mask_val]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ffd772d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pronouns = []\n",
    "\n",
    "# Consider embeddings and offsets in each batch separately\n",
    "for embeddings, offsets in zip(bert_embeddings, pronouns_offsets):\n",
    "#     mask = offsets != 0\n",
    "    \n",
    "#     embedding_length = embeddings.shape[-1]\n",
    "#     pad_tensor = torch.zeros(embedding_length)\n",
    "# #     embeddings_pronouns.append(embeddings[offsets[mask]])\n",
    "#     embeddings_pronouns.append(embeddings[offsets[mask]])\n",
    "\n",
    "    \n",
    "    #     for off in offsets:\n",
    "#         print(off)\n",
    "#         if off != 0:\n",
    "#             pron_embeddings = embeddings[off]\n",
    "            \n",
    "#         print(pron_embeddings)\n",
    "        \n",
    "#         break\n",
    "    embeddings_pronouns.append(embeddings[offsets])\n",
    "\n",
    "\n",
    "# Merge outputs\n",
    "# merged_entities_and_pron_embeddings = torch.cat([\n",
    "#     torch.stack(embeddings_A, dim=0),\n",
    "#     torch.stack(embeddings_B, dim=0),\n",
    "#     torch.stack(embeddings_pron, dim=0)\n",
    "# ], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "376fe246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 768])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack(embeddings_pronouns, dim=0)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4958619",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [6, 768] at entry 0 and [4, 768] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20924\\2740877009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_pronouns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [6, 768] at entry 0 and [4, 768] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack(embeddings_pronouns, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f5d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, labels in train_dataloader:\n",
    "    output = model(features)\n",
    "#     res = head(output.hidden_states[-1], offsets)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8185e7a8",
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1660751850397,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "8185e7a8"
   },
   "outputs": [],
   "source": [
    "class CorefHead(nn.Module):\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        self.head_hidden_size = 512\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(bert_hidden_size * 3, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 3)\n",
    "#         )\n",
    "\n",
    "        # a) Always BN -> AC, (Nothing b/w them).\n",
    "        # b) BN -> Dropout over Dropout -> BN, but try both. [Newer research, finds 1st better ]\n",
    "        # c) BN eliminates the need of Dropout, no need to use Dropout.\n",
    "        # e) BN before Dropout is data Leakage.\n",
    "        # f) Best thing is to try every combination.\n",
    "        # SO CALLED BEST METHOD -\n",
    "        # Layer -> BN -> AC -> Dropout ->Layer\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            #             nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            #             nn.Dropout(0.5),\n",
    "            #             nn.LeakyReLU(),\n",
    "            #             nn.Linear(bert_hidden_size * 3, self.head_hidden_size),\n",
    "            #             nn.BatchNorm1d(self.head_hidden_size),\n",
    "            #             nn.Dropout(0.5),\n",
    "            #             nn.Linear(self.head_hidden_size, self.head_hidden_size),\n",
    "            #             nn.ReLU(),\n",
    "            #             nn.BatchNorm1d(self.head_hidden_size),\n",
    "            #             nn.Dropout(0.5),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(bert_hidden_size * 3, self.head_hidden_size),\n",
    "#             nn.BatchNorm1d(self.head_hidden_size),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(self.head_hidden_size, 3)\n",
    "            nn.Linear(bert_hidden_size * 4, self.head_hidden_size),\n",
    "#             nn.BatchNorm1d(self.head_hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "\n",
    "#         lstm_hidden_dim = 256\n",
    "#         bidirectional = True\n",
    "#         self.lstm = nn.LSTM(self.head_hidden_size, \n",
    "#                             lstm_hidden_dim, \n",
    "#                             bidirectional=bidirectional,\n",
    "#                             num_layers=1,\n",
    "#                             dropout=0.1,\n",
    "#                             batch_first=True)\n",
    "        \n",
    "#         lstm_output_dim = lstm_hidden_dim if bidirectional is False \\\n",
    "#                             else lstm_hidden_dim * 2\n",
    "\n",
    "#         self.relu = nn.LeakyReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(self.head_hidden_size, 3)\n",
    "\n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        embeddings = self._retrieve_pronouns_embeddings(bert_outputs, offsets)\n",
    "\n",
    "        x = self.fc(embeddings)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.relu(x)\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "    \n",
    "    def _retrieve_pronouns_embeddings(self, bert_embeddings, pronouns_offsets):\n",
    "        pronouns_embeddings = []\n",
    "\n",
    "        # Consider embeddings and offsets in each batch separately\n",
    "        for embeddings, offsets in zip(bert_embeddings, pronouns_offsets):\n",
    "            pronouns_embeddings.append(embeddings[offsets])\n",
    "\n",
    "        # Merge outputs\n",
    "        merged_pronouns_embeddings = torch.stack(pronouns_embeddings, dim=0)\n",
    "        \n",
    "        # shape: batch_size x seq_length x embedding_dim\n",
    "        return merged_pronouns_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb7c679d",
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1660751878734,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "eb7c679d"
   },
   "outputs": [],
   "source": [
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "\n",
    "    def __init__(self, bert_model: str):\n",
    "        super().__init__()\n",
    "\n",
    "        if bert_model in {\"bert-base-uncased\", \"bert-base-cased\"}:\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in {\"bert-large-uncased\", \"bert-large-cased\"}:\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            bert_model).to(device, non_blocking=True)\n",
    "        self.head = CorefHead(self.bert_hidden_size).to(\n",
    "            device, non_blocking=True)\n",
    "\n",
    "    def forward(self, x, offsets):\n",
    "        bert_outputs = self.bert(\n",
    "            x, attention_mask=(x > 0).long(),\n",
    "            token_type_ids=None, output_hidden_states=True)\n",
    "#         concat_bert = torch.cat((bert_outputs[-1],bert_outputs[-2],bert_outputs[-3]),dim=-1)\n",
    "        # concat_bert = torch.cat((bert_outputs.hidden_states[-1], bert_outputs.hidden_states[-2],\n",
    "        #                          bert_outputs.hidden_states[-3], bert_outputs.hidden_states[-4]), dim=-1)\n",
    "        \n",
    "#         layers_to_sum = torch.stack([bert_outputs.hidden_states[x] for x in [-1, -2, -3, -4]], axis=0)\n",
    "#         out = torch.sum(layers_to_sum, axis=0)\n",
    "\n",
    "        out = torch.cat([bert_outputs.hidden_states[x] for x in [-1, -2, -3, -4]], axis=0)\n",
    "        \n",
    "#         out = bert_outputs.last_hidden_state\n",
    "        head_outputs = self.head(out, offsets)\n",
    "#         return concat_bert\n",
    "        return head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897420f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4f489d7",
   "metadata": {
    "id": "f4f489d7"
   },
   "source": [
    "GradScaler: https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-clipping\n",
    "\n",
    "https://pytorch.org/docs/stable/amp.html#gradient-scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a41065",
   "metadata": {
    "id": "21a41065"
   },
   "source": [
    "**Gradient Scaling**\n",
    "\n",
    "If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (underflow), so the update for the corresponding parameters will be lost.\n",
    "\n",
    "To prevent underflow, gradient scaling multiplies the networks loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they dont flush to zero.\n",
    "\n",
    "The method `step(optimizer, *args, **kwargs)` internally invokes `unscale_(optimizer)`and if no inf/NaN gradients are found, invokes `optimizer.step()` using the unscaled gradients. Otherwise `optimizer.step()` is skipped to avoid corrupting the params.\n",
    "\n",
    "\\**Note for Gradient Clipping*\n",
    "\n",
    "If you wish to modify the gradients (like in gradient clipping), you should unscale them first. If you attempted to clip *without* unscaling, the gradients' norm magnitude would also be scaled, so your requested threshold would be invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "766de1e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1660751890792,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "766de1e5"
   },
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model: nn.Module,\n",
    "#         args: CustomTrainingArguments,\n",
    "#         train_dataloader: DataLoader,\n",
    "#         valid_dataloader: DataLoader,\n",
    "#         criterion: torch.nn,\n",
    "#         optimizer: torch.optim.Optimizer,\n",
    "#         scheduler: torch.optim.lr_scheduler = None,\n",
    "        \n",
    "#     ):\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.train_dataloader = train_dataloader\n",
    "#         self.valid_dataloader = valid_dataloader\n",
    "#         self.criterion = criterion\n",
    "#         self.optimizer = optimizer\n",
    "#         self.scheduler = scheduler\n",
    "        \n",
    "#         if args is None:\n",
    "#             output_dir = \"../../model/tmp_trainer\"\n",
    "#             print(f\"No 'TrainingArguments' passed, using 'output_dir={output_dir}'.\")\n",
    "#             args = CustomTrainingArguments(output_dir=output_dir)\n",
    "        \n",
    "#         self.args = args\n",
    "        \n",
    "#     def train(self):\n",
    "#         args = self.args\n",
    "#         valid_dataloader = self.valid_dataloader\n",
    "#         epochs = args.num_train_epochs\n",
    "        \n",
    "#         train_losses = []\n",
    "#         train_acc_list = []\n",
    "#         valid_losses = []\n",
    "#         valid_acc_list = []\n",
    "        \n",
    "#         if args.early_stopping:\n",
    "#             patience_counter = 0 \n",
    "\n",
    "#         scaler = GradScaler()\n",
    "\n",
    "#         if args.resume_from_checkpoint is not None:\n",
    "#             self._resume_model(args.resume_from_checkpoint, scaler)\n",
    "\n",
    "#         training_start_time = time.time()\n",
    "#         print(\"\\nTraining...\")\n",
    "#         for epoch in range(epochs):\n",
    "#             train_loss, train_acc = self._inner_training_loop(scaler)\n",
    "#             train_losses.append(train_loss)\n",
    "#             train_acc_list.append(train_acc)\n",
    "\n",
    "#             valid_loss, valid_acc = self.evaluate(valid_dataloader)\n",
    "#             valid_losses.append(valid_loss)\n",
    "#             valid_acc_list.append(valid_acc)\n",
    "\n",
    "#             if self.scheduler is not None:\n",
    "#                 print('-' * 17)\n",
    "#                 print(f\"| LR: {self.scheduler.get_last_lr()[0]:.3e} |\")\n",
    "#                 self.scheduler.step()\n",
    "\n",
    "#             self._print_epoch_log(epoch, epochs, train_loss, valid_loss, valid_acc)\n",
    "\n",
    "#             if args.early_stopping and len(valid_acc_list) >= 2:\n",
    "#                 # stop = args.early_stopping_mode == 'min' and epoch > 0 and valid_acc_list[-1] > valid_acc_list[-2]\n",
    "#                 stop = args.early_stopping_mode == 'max' and epoch > 0 and valid_acc_list[-1] < valid_acc_list[-2]\n",
    "#                 if stop:\n",
    "#                     if patience_counter >= args.early_stopping_patience:\n",
    "#                         print('Early stop.')\n",
    "#                         break\n",
    "#                     else:\n",
    "#                         print('-- Patience.\\n')\n",
    "#                         patience_counter += 1\n",
    "        \n",
    "#         training_time = time.time() - training_start_time\n",
    "#         print(f'Training time: {training_time:.2f}s')\n",
    "\n",
    "#         metrics_history = {\n",
    "#             \"train_losses\": train_losses,\n",
    "#             \"train_acc\": train_acc_list,\n",
    "#             \"valid_losses\": valid_losses,\n",
    "#             \"valid_acc\": valid_acc_list,\n",
    "#         }\n",
    "#         print(metrics_history)\n",
    "#         if args.save_model:\n",
    "#             self._save_model(epoch, valid_acc, scaler, metrics_history)\n",
    "    \n",
    "#         return #metrics_history\n",
    "\n",
    "#     def _inner_training_loop(self, scaler):\n",
    "#         args = self.args\n",
    "#         train_dataloader = self.train_dataloader\n",
    "        \n",
    "#         train_loss = 0.0\n",
    "#         train_correct, total_count = 0.0, 0.0\n",
    "\n",
    "#         self.model.train()\n",
    "#         for step, (features, labels) in enumerate(train_dataloader):\n",
    "#             # Empty gradients\n",
    "#             self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "#             # Forward\n",
    "#             # predictions = self.model(features, offsets)\n",
    "#             # loss = self.criterion(predictions, labels)\n",
    "# #             with torch.cuda.amp.autocast(): # autocast as a context manager\n",
    "#             predictions = self.model(features)\n",
    "\n",
    "#             predictions = predictions.view(-1, predictions.shape[-1])\n",
    "#             labels = labels.view(-1)\n",
    "#             loss = self.criterion(predictions, labels)\n",
    "\n",
    "#             mask = labels != 0\n",
    "#             predictions = predictions.argmax(1)\n",
    "#             predictions = predictions[mask]\n",
    "#             labels = labels[mask]\n",
    "#             train_correct += (predictions == labels).sum().item()\n",
    "#             total_count += labels.shape[0]\n",
    "            \n",
    "#             # Backward  \n",
    "#             loss.backward()\n",
    "#             # Backward pass without mixed precision\n",
    "#             # It's not recommended to use mixed precision for backward pass\n",
    "#             # Because we need more precise loss\n",
    "# #             scaler.scale(loss).backward()\n",
    "            \n",
    "#             if args.grad_clipping is not None:\n",
    "# #                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.grad_clipping)\n",
    "            \n",
    "#             # Update weights \n",
    "#             self.optimizer.step()\n",
    "# #             scaler.step(self.optimizer)\n",
    "# #             scaler.update()\n",
    "\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#             if step % args.logging_steps == args.logging_steps - 1:\n",
    "#                 running_loss = train_loss / (step + 1)\n",
    "#                 running_acc = train_correct / total_count\n",
    "#                 self._print_step_log(step, running_loss, running_acc)\n",
    "                \n",
    "#         return train_loss / len(train_dataloader), train_correct / total_count\n",
    "\n",
    "\n",
    "#     def evaluate(self, eval_dataloader):\n",
    "#         valid_loss = 0.0\n",
    "#         eval_correct, total_count = 0, 0\n",
    "        \n",
    "#         self.model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for (features, labels) in eval_dataloader:\n",
    "                \n",
    "#                 predictions = self.model(features)\n",
    "                \n",
    "#                 predictions = predictions.view(-1, predictions.shape[-1])\n",
    "#                 labels = labels.view(-1)\n",
    "#                 loss = self.criterion(predictions, labels)\n",
    "#                 valid_loss += loss.item()\n",
    "                \n",
    "#                 mask = labels != 0\n",
    "#                 predictions = predictions.argmax(1)\n",
    "#                 predictions = predictions[mask]\n",
    "#                 labels = labels[mask]\n",
    "#                 eval_correct += (predictions == labels).sum().item()\n",
    "#                 total_count += labels.shape[0]\n",
    "        \n",
    "#         return valid_loss / len(eval_dataloader), eval_correct / total_count\n",
    "\n",
    "\n",
    "#     def _print_step_log(self, step, running_loss, running_acc):\n",
    "#         print(f'\\t| step {step+1:3d}/{len(self.train_dataloader):d} | train_loss: {running_loss:.3f} | ' \\\n",
    "#                 f'train_acc: {running_acc:.3f} |')\n",
    "\n",
    "#     def _print_epoch_log(self, epoch, epochs, train_loss, valid_loss, valid_acc):\n",
    "#         print('-' * 76)\n",
    "#         print(f'| epoch {epoch+1:>3d}/{epochs:<3d} | train_loss: {train_loss:.3f} | ' \\\n",
    "#                 f'valid_loss: {valid_loss:.3f} | valid_acc: {valid_acc:.3f} |')\n",
    "#         print('-' * 76)\n",
    "        \n",
    "    \n",
    "#     def _save_model(self, epoch, valid_acc, scaler):\n",
    "#         print(\"Saving model...\")\n",
    "#         if self.scheduler is None:\n",
    "#             torch.save({\n",
    "#                     \"epoch\": epoch,\n",
    "#                     \"model_state_dict\": self.model.state_dict(),\n",
    "#                     \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "#                     \"scaler_state_dict\": scaler.state_dict(),\n",
    "#                 }, f\"{self.args.output_dir}my_model_{str(valid_acc)[2:5]}_{epoch+1}.pth\")\n",
    "#         else:\n",
    "#             torch.save({\n",
    "#                     \"epoch\": epoch,\n",
    "#                     \"model_state_dict\": self.model.state_dict(),\n",
    "#                     \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "#                     \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "#                     \"scaler_state_dict\": scaler.state_dict(),\n",
    "#                 }, f\"{self.args.output_dir}my_model_{str(valid_acc)[2:5]}_{epoch+1}.pth\")\n",
    "\n",
    "#         print(\"Model saved.\")\n",
    "\n",
    "#     def _resume_model(self, path, scaler):\n",
    "#         checkpoint = torch.load(path, map_location=device)\n",
    "#         self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         scaler.load_state_dict(checkpoint['scaler_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00c6f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        args: CustomTrainingArguments,\n",
    "        train_dataloader: DataLoader,\n",
    "        valid_dataloader: DataLoader,\n",
    "        criterion: torch.nn,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler = None,\n",
    "        \n",
    "    ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        if args is None:\n",
    "            output_dir = \"../../model/tmp_trainer\"\n",
    "            print(f\"No 'TrainingArguments' passed, using 'output_dir={output_dir}'.\")\n",
    "            args = CustomTrainingArguments(output_dir=output_dir)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def train(self):\n",
    "        args = self.args\n",
    "        valid_dataloader = self.valid_dataloader\n",
    "        epochs = args.num_train_epochs\n",
    "        \n",
    "        train_losses = []\n",
    "        train_acc_list = []\n",
    "        valid_losses = []\n",
    "        valid_acc_list = []\n",
    "        \n",
    "        if args.early_stopping:\n",
    "            patience_counter = 0 \n",
    "\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        if args.resume_from_checkpoint is not None:\n",
    "            self._resume_model(args.resume_from_checkpoint, scaler)\n",
    "\n",
    "        training_start_time = time.time()\n",
    "        print(\"\\nTraining...\")\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self._inner_training_loop(scaler)\n",
    "            train_losses.append(train_loss)\n",
    "            train_acc_list.append(train_acc)\n",
    "\n",
    "            valid_loss, valid_acc = self.evaluate(valid_dataloader)\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_acc_list.append(valid_acc)\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                print('-' * 17)\n",
    "                print(f\"| LR: {self.scheduler.get_last_lr()[0]:.3e} |\")\n",
    "                self.scheduler.step()\n",
    "\n",
    "            self._print_epoch_log(epoch, epochs, train_loss, valid_loss, valid_acc)\n",
    "\n",
    "            if args.early_stopping and len(valid_acc_list) >= 2:\n",
    "                # stop = args.early_stopping_mode == 'min' and epoch > 0 and valid_acc_list[-1] > valid_acc_list[-2]\n",
    "                stop = args.early_stopping_mode == 'max' and epoch > 0 and valid_acc_list[-1] < valid_acc_list[-2]\n",
    "                if stop:\n",
    "                    if patience_counter >= args.early_stopping_patience:\n",
    "                        print('Early stop.')\n",
    "                        break\n",
    "                    else:\n",
    "                        print('-- Patience.\\n')\n",
    "                        patience_counter += 1\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        print(f'Training time: {training_time:.2f}s')\n",
    "\n",
    "        metrics_history = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_acc\": train_acc_list,\n",
    "            \"valid_losses\": valid_losses,\n",
    "            \"valid_acc\": valid_acc_list,\n",
    "        }\n",
    "        print(metrics_history)\n",
    "        if args.save_model:\n",
    "            self._save_model(\"1\", epoch, valid_acc, scaler, metrics_history)\n",
    "    \n",
    "        return #metrics_history\n",
    "\n",
    "    def _inner_training_loop(self, scaler):\n",
    "        args = self.args\n",
    "        train_dataloader = self.train_dataloader\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct, total_count = 0.0, 0.0\n",
    "\n",
    "        self.model.train()\n",
    "        for step, (features, offsets, labels) in enumerate(train_dataloader):\n",
    "            # Empty gradients\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward\n",
    "            # predictions = self.model(features, offsets)\n",
    "            # loss = self.criterion(predictions, labels)\n",
    "#             with torch.cuda.amp.autocast(): # autocast as a context manager\n",
    "            predictions = self.model(features, offsets)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            labels = labels.view(-1)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "\n",
    "            mask = labels != 0\n",
    "            predictions = predictions.argmax(1)\n",
    "            predictions = predictions[mask]\n",
    "            labels = labels[mask]\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            total_count += labels.shape[0]\n",
    "            \n",
    "            # Backward  \n",
    "            loss.backward()\n",
    "            # Backward pass without mixed precision\n",
    "            # It's not recommended to use mixed precision for backward pass\n",
    "            # Because we need more precise loss\n",
    "#             scaler.scale(loss).backward()\n",
    "            \n",
    "            if args.grad_clipping is not None:\n",
    "#                 scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.grad_clipping)\n",
    "            \n",
    "            # Update weights \n",
    "            self.optimizer.step()\n",
    "#             scaler.step(self.optimizer)\n",
    "#             scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if step % args.logging_steps == args.logging_steps - 1:\n",
    "                running_loss = train_loss / (step + 1)\n",
    "                running_acc = train_correct / total_count\n",
    "                self._print_step_log(step, running_loss, running_acc)\n",
    "                \n",
    "        return train_loss / len(train_dataloader), train_correct / total_count\n",
    "\n",
    "\n",
    "    def evaluate(self, eval_dataloader):\n",
    "        valid_loss = 0.0\n",
    "        eval_correct, total_count = 0, 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (features, offsets, labels) in eval_dataloader:\n",
    "                \n",
    "                predictions = self.model(features, offsets)\n",
    "                \n",
    "                predictions = predictions.view(-1, predictions.shape[-1])\n",
    "                labels = labels.view(-1)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "#                 accuracy = compute_score(predictions, labels)\n",
    "                \n",
    "                mask = labels != 0\n",
    "                predictions = predictions.argmax(1)\n",
    "                predictions = predictions[mask]\n",
    "                labels = labels[mask]\n",
    "                eval_correct += (predictions == labels).sum().item()\n",
    "                total_count += labels.shape[0]\n",
    "        \n",
    "        return valid_loss / len(eval_dataloader), eval_correct / total_count\n",
    "\n",
    "    \n",
    "    def compute_score(self, predictions: torch.Tensor, labels: torch.Tensor):\n",
    "        mask = labels != 0\n",
    "        labels = labels[mask]        \n",
    "        \n",
    "        predictions = predictions[mask]\n",
    "        maximum_logits, predicted_labels = predictions.max(1)\n",
    "        \n",
    "        # It may happen that more than one pronoun is classify as ambiguous\n",
    "        multiple_ambiguous_pronouns_mask = predicted_labels == 2\n",
    "        ambiguous_pronouns_logits = maximum_logits[multiple_ambiguous_pronouns_mask]\n",
    "\n",
    "        # More than one pronoun is classify as ambiguous\n",
    "        if len(ambiguous_pronouns_logits) > 1:\n",
    "            # Get the highest logit among the ambiguous ones\n",
    "            highest_ambiguous_pronoun_logit = ambiguous_pronouns_logits.max()\n",
    "\n",
    "            # Identity the position of the logit that should correspond to the ambiguous prononun class (2)\n",
    "            ambiguous_pronoun_mask = maximum_logits == highest_ambiguous_pronoun_logit\n",
    "\n",
    "            # All the predictions that are not of that class are set to the \"not ambiguous class\" (1)\n",
    "            predicted_labels[~ambiguous_pronoun_mask] = 1\n",
    "\n",
    "            # However, it may happen again that we have multiple pronouns classified as ambiguous, \n",
    "            # since there may be more than one logit with value = highest_ambiguous_pronoun_logit\n",
    "        \n",
    "        \n",
    "        label_ambiguous_mask = labels == 2\n",
    "        eval_correct += int(labels[label_ambiguous_mask] == predicted_labels[label_ambiguous_mask])\n",
    "        \n",
    "        return eval_correct\n",
    "    \n",
    "\n",
    "    def _print_step_log(self, step, running_loss, running_acc):\n",
    "        print(f'\\t| step {step+1:3d}/{len(self.train_dataloader):d} | train_loss: {running_loss:.3f} | ' \\\n",
    "                f'train_acc: {running_acc:.3f} |')\n",
    "\n",
    "    def _print_epoch_log(self, epoch, epochs, train_loss, valid_loss, valid_acc):\n",
    "        print('-' * 76)\n",
    "        print(f'| epoch {epoch+1:>3d}/{epochs:<3d} | train_loss: {train_loss:.3f} | ' \\\n",
    "                f'valid_loss: {valid_loss:.3f} | valid_acc: {valid_acc:.3f} |')\n",
    "        print('-' * 76)\n",
    "        \n",
    "    \n",
    "    def _save_model(self, task_type, epoch, valid_acc, scaler, metrics_history):\n",
    "        print(\"Saving model...\")\n",
    "        params_to_save = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"metrics_history\": metrics_history,\n",
    "        }\n",
    "        \n",
    "        if self.scheduler is not None:\n",
    "            params_to_save[\"scheduler_state_dict\"] = self.scheduler.state_dict()\n",
    "            \n",
    "        if scaler is not None:\n",
    "            params_to_save[\"scaler_state_dict\"] = scaler.state_dict(),\n",
    "            \n",
    "        save_path = f\"{self.args.output_dir}my_model{str(task_type)}_{str(valid_acc)[2:5]}_{epoch+1}\"\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H-%M-%S\")\n",
    "        \n",
    "        if os.path.exists(f\"{save_path}_{current_time}.pth\"):\n",
    "            torch.save(params_to_save, f\"{save_path}_{current_time}_new.pth\")\n",
    "        else:\n",
    "            torch.save(params_to_save, f\"{save_path}_{current_time}.pth\")\n",
    "        \n",
    "        print(\"Model saved.\")\n",
    "\n",
    "    def _resume_model(self, path, scaler):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7a11aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     \"model_state_dict\": model.state_dict(),\n",
    "#     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "# #     \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "# #     \"scaler_state_dict\": scaler.state_dict(),\n",
    "# }, f\"../../model/checkpoints/my_model_{str(946)}_{2}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31aa4455",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2064,
     "status": "ok",
     "timestamp": 1660753117653,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "31aa4455",
    "outputId": "790097d0-16fa-439c-b121-325837a47121",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTrainingArguments(output_dir='../../model/checkpoints/', resume_from_checkpoint=None, save_model=True, num_train_epochs=2, logging_steps=250, learning_rate=5e-06, grad_clipping=None, early_stopping=True, early_stopping_mode='max', early_stopping_patience=2)\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"bert-base-cased\"\n",
    "model = GAPModel(model_name_or_path).to(device, non_blocking=True)\n",
    "\n",
    "# last_frozen_layer = 6\n",
    "\n",
    "# modules = [model.bert.embeddings, *model.bert.encoder.layer[:last_frozen_layer]]\n",
    "# for module in modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "yaml_file = \"./train_notebook.yaml\"\n",
    "# Read configuration file with all the necessary parameters\n",
    "with open(yaml_file) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "training_args = CustomTrainingArguments(**config['training_args'])\n",
    "\n",
    "# Make sure that the learning rate is read as a number and not as a string\n",
    "training_args.learning_rate = float(training_args.learning_rate)\n",
    "print(training_args)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device=device, non_blocking=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=training_args.learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "scheduler = None\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "collator = Collator(device)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                              collate_fn=collator, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, \n",
    "                              collate_fn=collator, shuffle=False)\n",
    "\n",
    "trainer = Trainer(model, training_args, \n",
    "                  train_dataloader, valid_dataloader, \n",
    "                  criterion, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7c8f03a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 735134,
     "status": "ok",
     "timestamp": 1660753855156,
     "user": {
      "displayName": "Florin Cuconasu",
      "userId": "13345521608196528334"
     },
     "user_tz": -120
    },
    "id": "e7c8f03a",
    "outputId": "8349c872-e0c2-4428-f04b-6ed2f8496ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "----------------------------------------------------------------------------\n",
      "| epoch   1/2   | train_loss: 0.969 | valid_loss: 0.726 | valid_acc: 0.664 |\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "| epoch   2/2   | train_loss: 0.544 | valid_loss: 0.546 | valid_acc: 0.793 |\n",
      "----------------------------------------------------------------------------\n",
      "Training time: 11.87s\n",
      "{'train_losses': [0.9691021776199341, 0.5436927318572998], 'train_acc': [0.5457317073170732, 0.823170731707317], 'valid_losses': [0.7257445592146653, 0.5456713896531326], 'valid_acc': [0.6642857142857143, 0.7928571428571428]}\n",
      "Saving model...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "metrics_history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280693e",
   "metadata": {
    "id": "3280693e"
   },
   "outputs": [],
   "source": [
    "# 0.134, 380s\n",
    "metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b0aafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true_list = []\n",
    "# y_pred_list = []\n",
    "\n",
    "# eval_correct, total_count = 0.0, 0.0\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     dataloader = DataLoader(valid_ds, batch_size=1, collate_fn=collate_batch, shuffle=False)\n",
    "#     for (features, labels) in dataloader:\n",
    "#         predicted_labels = model(features)\n",
    "\n",
    "#         predicted_labels = predicted_labels.view(-1, predicted_labels.shape[-1])\n",
    "#         labels = labels.view(-1)\n",
    "\n",
    "#         mask = labels != 0\n",
    "#         predicted_labels = predicted_labels.argmax(1)\n",
    "#         predicted_labels = predicted_labels[mask].tolist()\n",
    "        \n",
    "#         y_pred_list.append(predicted_labels)\n",
    "        \n",
    "        \n",
    "#         labels = labels[mask].tolist()\n",
    "#         y_true_list.append(labels)\n",
    "        \n",
    "        \n",
    "# #         eval_correct += (predicted_labels == labels).sum().item()\n",
    "# #         total_count += labels.shape[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "992233ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It may happen that more than one pronoun is classify as ambiguous\n",
    "multiple_ambiguous_pronouns_mask = a[1] == 2\n",
    "\n",
    "ambiguous_pronouns_logits = a[0][multiple_ambiguous_pronouns_mask]\n",
    "\n",
    "# No ambiguity\n",
    "if len(ambiguous_pronouns_logits) <= 1:\n",
    "    print(\"RVFE\")\n",
    "\n",
    "# More than one pronoun is classify as ambiguous\n",
    "\n",
    "# Get the highest logit among the ambiguous ones\n",
    "highest_ambiguous_pronoun_logit = ambiguous_pronouns_logits.max()\n",
    "\n",
    "# Identity the position of the logit that should correspond to the ambiguous prononun class (2)\n",
    "ambiguous_pronoun_mask = a[0] == highest_ambiguous_pronoun_logit\n",
    "\n",
    "# All the predictions that are not of that class are set to the \"not ambiguous class\" (1)\n",
    "a[1][~ambiguous_pronoun_mask] = 1\n",
    "\n",
    "# However, With this function, it may happen again that we have multiple pronouns classified as ambiguous, \n",
    "# since there may be more than one logit with value = highest_ambiguous_pronoun_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78086d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17568\\1514880221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0meval_correct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcollator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCollator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "logits = []\n",
    "\n",
    "\n",
    "eval_correct, total_count = 0.0, 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    collator = Collator(device)\n",
    "    dataloader = DataLoader(valid_ds, batch_size=1, collate_fn=collator, shuffle=False)\n",
    "    for idx, (features, offsets, labels) in enumerate(dataloader):\n",
    "        predictions = model(features, offsets)\n",
    "\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        logits.append(predictions)\n",
    "\n",
    "        mask = labels != 0\n",
    "        labels = labels[mask]\n",
    "        y_true_list.append(labels.tolist())\n",
    "        \n",
    "        \n",
    "        predictions = predictions[mask]\n",
    "        maximum_logits, predicted_labels = predictions.max(1)\n",
    "\n",
    "        \n",
    "        # It may happen that more than one pronoun is classify as ambiguous\n",
    "        multiple_ambiguous_pronouns_mask = predicted_labels == 2\n",
    "        ambiguous_pronouns_logits = maximum_logits[multiple_ambiguous_pronouns_mask]\n",
    "        \n",
    "        # More than one pronoun is classify as ambiguous\n",
    "        if len(ambiguous_pronouns_logits) > 1:\n",
    "            # Get the highest logit among the ambiguous ones\n",
    "            highest_ambiguous_pronoun_logit = ambiguous_pronouns_logits.max()\n",
    "\n",
    "            # Identity the position of the logit that should correspond to the ambiguous prononun class (2)\n",
    "            ambiguous_pronoun_mask = maximum_logits == highest_ambiguous_pronoun_logit\n",
    "\n",
    "            # All the predictions that are not of that class are set to the \"not ambiguous class\" (1)\n",
    "            predicted_labels[~ambiguous_pronoun_mask] = 1\n",
    "\n",
    "            # However, it may happen again that we have multiple pronouns classified as ambiguous, \n",
    "            # since there may be more than one logit with value = highest_ambiguous_pronoun_logit\n",
    "        \n",
    "        \n",
    "        # When the model predicts that all the pronouns are not ambiguous (no class 2)\n",
    "        if not torch.any(predicted_labels == 2):\n",
    "            # Try to select the most probable ambiguous pronoun\n",
    "            probable_ambiguous_index = predictions[:,-1].argmax()\n",
    "            predicted_labels[probable_ambiguous_index] = 2\n",
    "        \n",
    "        \n",
    "        y_pred_list.append(predicted_labels.tolist())\n",
    "        \n",
    "        \n",
    "        label_ambiguous_mask = labels == 2\n",
    "        eval_correct += (labels[label_ambiguous_mask] == predicted_labels[label_ambiguous_mask]).sum().item()\n",
    "        total_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912f03e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17568\\3163332516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtotal_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'total_count' is not defined"
     ]
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae4af2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17568\\1567107447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_correct' is not defined"
     ]
    }
   ],
   "source": [
    "eval_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c1436200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118942731277533"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "83c33ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9118942731277533"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_correct / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "5ff402f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 1])\n",
    "t2 = torch.tensor([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e2d0aed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e8b5fd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(t1 != t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6318eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambigous = t1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72efff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t1[ambigous] == t2[ambigous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78d8c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "30a47a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "logits = []\n",
    "\n",
    "eval_correct, total_count = 0.0, 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dataloader = DataLoader(valid_ds, batch_size=1, collate_fn=collate_batch, shuffle=False)\n",
    "    for (features, offsets, labels) in dataloader:\n",
    "        predicted_labels = model(features, offsets)\n",
    "\n",
    "        predicted_labels = predicted_labels.view(-1, predicted_labels.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        \n",
    "        logits.append(predicted_labels)\n",
    "\n",
    "        mask = labels != 0\n",
    "        predictions = predicted_labels.argmax(1)\n",
    "        predictions = predictions[mask].tolist()\n",
    "        \n",
    "        y_pred_list.append(predictions)\n",
    "        \n",
    "        \n",
    "        labels = labels[mask].tolist()\n",
    "        y_true_list.append(labels)\n",
    "        \n",
    "        \n",
    "#         eval_correct += (predicted_labels == labels).sum().item()\n",
    "#         total_count += labels.shape[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d7616b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "28822a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False], device='cuda:0')"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[4]= False\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "427c9bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3443,  4.5501, -1.0523],\n",
       "        [-3.3901,  4.5434, -0.9574],\n",
       "        [-3.3125,  4.5226, -1.0784],\n",
       "        [-3.6061,  4.0081, -0.1762],\n",
       "        [-3.3012,  4.5141, -1.1606],\n",
       "        [-3.3566,  4.4317, -1.0823],\n",
       "        [-3.7792,  3.9054, -0.0202],\n",
       "        [-3.8278,  3.0091,  1.3090],\n",
       "        [-3.5415,  3.3520,  0.2863]], device='cuda:0')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7e718c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = logits[206].max(1)\n",
    "maximum_logits, predicted_classes = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "386d725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5501, 4.5434, 4.5226, 4.0081, 4.5141, 4.4317, 3.9054, 3.0091, 3.3520],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c9bcca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_logits = torch.tensor([1,1,4,2])\n",
    "predicted_labels = torch.tensor([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a29811c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 4, 2],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.stack([maximum_logits, predicted_classes], dim=0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4538a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ef\n"
     ]
    }
   ],
   "source": [
    "if not torch.any(predicted_labels == 2):\n",
    "    print(\"ef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c944741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# When the model predicts that all the pronouns are not ambiguous (all class 1)\n",
    "if len(predicted_classes) == predicted_classes.sum().item():\n",
    "    # Try to select the most probable ambiguous one\n",
    "    probable_ambiguous_index = logits[184][:,-1].argmax()\n",
    "    \n",
    "    print(predicted_classes[probable_ambiguous_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "2d3d4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may happen that more than one pronoun is classify as ambiguous\n",
    "multiple_ambiguous_pronouns_mask = a[1] == 2\n",
    "\n",
    "ambiguous_pronouns_logits = a[0][multiple_ambiguous_pronouns_mask]\n",
    "\n",
    "# No ambiguity\n",
    "if len(ambiguous_pronouns_logits) <= 1:\n",
    "    print(\"RVFE\")\n",
    "\n",
    "# More than one pronoun is classify as ambiguous\n",
    "\n",
    "# Get the highest logit among the ambiguous ones\n",
    "highest_ambiguous_pronoun_logit = ambiguous_pronouns_logits.max()\n",
    "\n",
    "# Identity the position of the logit that should correspond to the ambiguous prononun class (2)\n",
    "ambiguous_pronoun_mask = a[0] == highest_ambiguous_pronoun_logit\n",
    "\n",
    "# All the predictions that are not of that class are set to the \"not ambiguous class\" (1)\n",
    "a[1][~ambiguous_pronoun_mask] = 1\n",
    "\n",
    "# However, With this function, it may happen again that we have multiple pronouns classified as ambiguous, \n",
    "# since there may be more than one logit with value = highest_ambiguous_pronoun_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "79eb4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0ec5aa6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambiguous_pronouns_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fb962562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] == ambiguous_pronouns_logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fe0a3956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][multiple_ambiguous_pronouns_mask][ambiguous_pronouns_logits.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "359a5f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 2])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "fe19cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "torch.index_select(predicted_classes[multiple_ambiguous_pronouns_mask], dim=0, index=torch.tensor([1]))\n",
    "\n",
    "torch.select(predicted_classes[multiple_ambiguous_pronouns_mask], dim=0, index=torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "09b1a1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.index_select(a[1][multiple_ambiguous_pronouns_mask], dim=0, index=ambiguous_pronouns_logits.argmax())\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "335c8afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "10353ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.select(a[1][multiple_ambiguous_pronouns_mask], dim=0, index=ambiguous_pronouns_logits.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b388c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the highest logit among the ambiguous ones\n",
    "ambiguous_pronoun_logit = a[0][multiple_ambiguous_pronouns_mask].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0d1caea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True], device='cuda:0')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identity the position of the logit that should correspond to the ambiguous prononun class (2)\n",
    "ambiguous_pronoun_mask = a[0] == ambiguous_pronoun_logit\n",
    "ambiguous_pronoun_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c63ef1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the predictions that are not of that class are set to not ambiguous (1)\n",
    "a[1][~ambiguous_pronoun_mask] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8df6a085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2], device='cuda:0')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fbc6297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pron = a[0][ambiguous].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a06462dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True,  True], device='cuda:0')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "01497eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = a[0][ambiguous].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d246f07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][ambiguous].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "33c8230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  True], device='cuda:0')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro = a[0] == m\n",
    "pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f9b51434",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1][~pro] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5901dc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 2], device='cuda:0')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "833ce8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.7404, 4.1840, 2.3523, 3.0847], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if a[0][ambiguous].shape != 1:\n",
    "    print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1024a1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].argmax(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5748038c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 1]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2a209ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 2], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "208c36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pred_ = 0\n",
    "for idx, pred_list in enumerate(y_pred_list):\n",
    "    c = 0\n",
    "    for num in pred_list:\n",
    "        if num == 2:\n",
    "            c += 1\n",
    "            \n",
    "    if c > 1:\n",
    "        print(idx)\n",
    "        count_pred_ += 1\n",
    "        \n",
    "    elif c == 0:\n",
    "        print(idx)\n",
    "        count_pred_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "927fefd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad8e9ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 2, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_list[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a8f92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Pred\n",
      "Id: 67\n",
      "2 1\n",
      "Id: 67\n",
      "1 2\n",
      "Id: 87\n",
      "1 2\n",
      "Id: 87\n",
      "2 1\n",
      "Id: 98\n",
      "1 2\n",
      "Id: 98\n",
      "2 1\n",
      "Id: 110\n",
      "2 1\n",
      "Id: 110\n",
      "1 2\n",
      "Id: 136\n",
      "1 2\n",
      "Id: 136\n",
      "2 1\n",
      "Id: 156\n",
      "1 2\n",
      "Id: 156\n",
      "2 1\n",
      "Id: 163\n",
      "1 2\n",
      "Id: 163\n",
      "2 1\n",
      "Id: 171\n",
      "1 2\n",
      "Id: 171\n",
      "2 1\n",
      "Id: 189\n",
      "1 2\n",
      "Id: 189\n",
      "2 1\n",
      "Id: 197\n",
      "2 1\n",
      "Id: 197\n",
      "1 2\n",
      "Id: 207\n",
      "2 1\n",
      "Id: 207\n",
      "1 2\n",
      "Id: 219\n",
      "1 2\n",
      "Id: 219\n",
      "2 1\n",
      "Id: 225\n",
      "2 1\n",
      "Id: 225\n",
      "1 2\n",
      "Id: 233\n",
      "1 2\n",
      "Id: 233\n",
      "2 1\n",
      "Id: 255\n",
      "1 2\n",
      "Id: 255\n",
      "2 1\n",
      "Id: 269\n",
      "2 1\n",
      "Id: 269\n",
      "1 2\n",
      "Id: 272\n",
      "1 2\n",
      "Id: 272\n",
      "2 1\n",
      "Id: 275\n",
      "1 2\n",
      "Id: 275\n",
      "2 1\n",
      "Id: 287\n",
      "1 2\n",
      "Id: 287\n",
      "2 1\n",
      "Id: 300\n",
      "1 2\n",
      "Id: 300\n",
      "2 1\n",
      "Id: 328\n",
      "2 1\n",
      "Id: 328\n",
      "1 2\n",
      "Id: 338\n",
      "1 2\n",
      "Id: 338\n",
      "2 1\n",
      "Id: 348\n",
      "2 1\n",
      "Id: 348\n",
      "1 2\n",
      "Id: 355\n",
      "2 1\n",
      "Id: 355\n",
      "1 2\n",
      "Id: 365\n",
      "2 1\n",
      "Id: 365\n",
      "1 2\n",
      "Id: 372\n",
      "1 2\n",
      "Id: 372\n",
      "2 1\n",
      "Id: 380\n",
      "2 1\n",
      "Id: 380\n",
      "1 2\n",
      "Id: 397\n",
      "1 2\n",
      "Id: 397\n",
      "2 1\n",
      "Id: 400\n",
      "1 2\n",
      "Id: 400\n",
      "2 1\n",
      "Id: 403\n",
      "1 2\n",
      "Id: 403\n",
      "2 1\n",
      "Id: 420\n",
      "1 2\n",
      "Id: 420\n",
      "2 1\n",
      "Id: 423\n",
      "1 2\n",
      "Id: 423\n",
      "2 1\n",
      "Id: 426\n",
      "2 1\n",
      "Id: 426\n",
      "1 2\n",
      "Id: 429\n",
      "2 1\n",
      "Id: 429\n",
      "1 2\n",
      "Id: 447\n",
      "1 2\n",
      "Id: 447\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "print(\"True\", \"Pred\")\n",
    "count = 0\n",
    "count_wrong = 0\n",
    "for sentence_id, (true, pred) in enumerate(zip(y_true_list, y_pred_list)):\n",
    "\n",
    "    for idx, elem in enumerate(true):\n",
    "        if (elem == 2 and pred[idx] == 1) or (elem == 1 and pred[idx] == 2):\n",
    "#         if pred[idx] != elem:\n",
    "            print(\"Id:\", sentence_id)\n",
    "            print(elem, pred[idx])\n",
    "            count_wrong += 1\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "    count += 1\n",
    "    \n",
    "#     if count == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e271dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd07b832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8458149779735683"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - count_wrong / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9febb1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8458149779735683"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - count_wrong / len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "29cf2011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for i, num in enumerate(y_true_list[172]):\n",
    "    if num == 2:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "34657e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for idx, n in enumerate(y_pred_list[172]):\n",
    "    if n == 2:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "6bc39dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds) - eval_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "e5b5657b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b13971",
   "metadata": {},
   "source": [
    "## Observation: The model fails when the coreferenced mentions are both wrong? (valid 405)\n",
    "\n",
    "## Also fails when the mention comes after the pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "fc28b349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                                                                                                                                                                                                                                                validation-173\n",
       "text          Lizzie also is very sure Sasha is dead. She also shows a disliking towards Tyreese. After Lizzie kills Mika, Carol and Tyreese realize that she is too psychotic to be kept around & had to be executed in order for Judith to be safe, so Carol leads her out to the fields to look at the flowers, and as Lizzie tried to apologize for having threatened Carol and Tyreese, Carol executed her.\n",
       "pron                                                                                                                                                                                                                                                                                                                                                                                                         her\n",
       "p_offset                                                                                                                                                                                                                                                                                                                                                                                                     247\n",
       "entity_A                                                                                                                                                                                                                                                                                                                                                                                                  Lizzie\n",
       "offset_A                                                                                                                                                                                                                                                                                                                                                                                                     300\n",
       "is_coref_A                                                                                                                                                                                                                                                                                                                                                                                                  True\n",
       "entity_B                                                                                                                                                                                                                                                                                                                                                                                                   Carol\n",
       "offset_B                                                                                                                                                                                                                                                                                                                                                                                                     348\n",
       "is_coref_B                                                                                                                                                                                                                                                                                                                                                                                                 False\n",
       "url                                                                                                                                                                                                                                                                                                                                 http://en.wikipedia.org/wiki/List_of_The_Walking_Dead_(TV_series)_characters\n",
       "Name: 172, dtype: object"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.iloc[172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "39b0b27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'her out to the fields to look at the flowers, and as Lizzie tried to apologize for having threatened Carol and Tyreese, Carol executed her.'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['text'][172][247:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9eacf29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 172\n",
    "tokenizer.convert_ids_to_tokens(valid_ds[index][0])[valid_ds[index][1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f7f88",
   "metadata": {},
   "source": [
    "# Remove double punctuation symbols. -- '' (valid_ds 64 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f9277db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid['text'][64].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "240ce35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(valid_ds[67][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "d59e6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7543,  4.1640, -0.2318],\n",
       "        [-3.6449,  1.0342,  3.1476],\n",
       "        [-3.5492,  3.7941, -0.2013]], device='cuda:0')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "49ea8506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0198, 0.0193, 0.0199, 0.0195, 0.0249, 0.0259, 0.6350, 0.1268, 0.0598,\n",
       "        0.0491], device='cuda:0')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.softmax(logits[184][:,2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5211e48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4585, -1.4833, -1.4543, -1.4747, -1.2275, -1.1908,  2.0098,  0.3989,\n",
       "        -0.3531, -0.5495], device='cuda:0')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[184][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3790cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "tokens_labels = []\n",
    "scores = []\n",
    "\n",
    "for idx, feature in enumerate(valid_ds):\n",
    "    tokens = feature[0]\n",
    "    offsets = feature[1]\n",
    "    \n",
    "    tokens_labels.append(tokenizer.convert_ids_to_tokens(tokens))\n",
    "    \n",
    "    score = [0 for _ in range(len(tokens))]\n",
    "    \n",
    "    for i_off, off in enumerate(offsets):\n",
    "        score[off] =  round(F.softmax(logits[idx][:,2], dim=0)[i_off].item(), 5)\n",
    "  \n",
    "    scores.append(score)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e8657ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 48, 61, 75, 99]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[98][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9d0c795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores[447]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "129ad16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 171\n",
    "\n",
    "figure(figsize=(30, 10), dpi=80)\n",
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=tokens_labels[index], y=scores[index], ci=None)\n",
    "\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "ax.grid(True)\n",
    "\n",
    "plt.title('Start Word Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a20cf6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              validation-88\n",
       "text          Jenkins reaction to those comments saw him reach the semi final of the 2010 European Championship Darts, losing narrowly again to Phil Taylor in a classic 11-10, but showing a welcome return to form. Jenkins had a decent 2010 Grand Slam of Darts, beating Tony O'Shea in the 2nd round before succombing to a final leg decider to James Wade. Following his Grand Slam of Darts quarter final defeat to Wade, Jenkins said in an interview that he believed players raise their game when they play him, as an example, Raymond van Barneveld struggled for form during the premier league campaign, but managed to beat Jenkins twice and hit a nine darter aganist him.\n",
       "pron                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      him\n",
       "p_offset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  490\n",
       "entity_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Raymond van Barneveld\n",
       "offset_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  510\n",
       "is_coref_A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              False\n",
       "entity_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Jenkins\n",
       "offset_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  607\n",
       "is_coref_B                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               True\n",
       "url                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                http://en.wikipedia.org/wiki/Terry_Jenkins\n",
       "Name: 87, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.iloc[87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1a2a7354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'In',\n",
       " '1924',\n",
       " 'Arthur',\n",
       " 'Raymond',\n",
       " 'Hi',\n",
       " '##bbe',\n",
       " '##rt',\n",
       " 'was',\n",
       " 'born',\n",
       " 'in',\n",
       " 'End',\n",
       " '##er',\n",
       " '##by',\n",
       " ',',\n",
       " 'Leicestershire',\n",
       " ',',\n",
       " 'the',\n",
       " 'son',\n",
       " 'of',\n",
       " 'Canon',\n",
       " 'H',\n",
       " '.',\n",
       " 'V',\n",
       " '.',\n",
       " 'Hi',\n",
       " '##bbe',\n",
       " '##rt',\n",
       " '(',\n",
       " 'd',\n",
       " '.',\n",
       " '1980',\n",
       " ')',\n",
       " 'and',\n",
       " 'his',\n",
       " 'wife',\n",
       " 'Maud',\n",
       " '##e',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'educated',\n",
       " 'at',\n",
       " 'Ra',\n",
       " '##dley',\n",
       " 'College',\n",
       " ',',\n",
       " 'before',\n",
       " 'he',\n",
       " 'went',\n",
       " 'up',\n",
       " 'to',\n",
       " 'Or',\n",
       " '##iel',\n",
       " 'College',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Oxford',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'awarded',\n",
       " 'the',\n",
       " 'degrees',\n",
       " 'of',\n",
       " 'B',\n",
       " '.',\n",
       " 'A',\n",
       " '.',\n",
       " 'and',\n",
       " 'later',\n",
       " 'MA',\n",
       " '.',\n",
       " 'He',\n",
       " 'left',\n",
       " 'Or',\n",
       " '##iel',\n",
       " 'College',\n",
       " 'to',\n",
       " 'join',\n",
       " 'the',\n",
       " 'Army',\n",
       " ',',\n",
       " 'where',\n",
       " 'a',\n",
       " 'sergeant',\n",
       " 'major',\n",
       " 'referred',\n",
       " 'to',\n",
       " 'Hi',\n",
       " '##bbe',\n",
       " '##rt',\n",
       " 'as',\n",
       " 'Christopher',\n",
       " 'Robin',\n",
       " 'based',\n",
       " 'upon',\n",
       " 'his',\n",
       " 'youthful',\n",
       " 'looks',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_labels[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "169cc6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1140"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[87][0][104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "71f54e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'him'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(1140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665abfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output.view(-1, output.shape[-1])\n",
    "labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8c74e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = labels != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b75363f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l = labels[mask].tolist() \n",
    "l = labels[mask]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf023c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.argmax(1)\n",
    "# p = pred[mask].tolist()\n",
    "p = pred[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18c4a500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p == l).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "316eef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d69cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function taken from the 'evaluate.py' script\n",
    "def flat_list(l: List[List[Any]]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "        A single list containing all elements that\n",
    "        were in the input list.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    l: List[List[Any]]\n",
    "        A list of lists of any type\n",
    "    \"\"\"\n",
    "    return [_e for e in l for _e in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "599df696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9973)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import F1Score\n",
    "\n",
    "f1 = F1Score(3)\n",
    "f1(torch.tensor(flat_list(y_pred_list)), torch.tensor(flat_list(y_true_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223155d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb3d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e9715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb954d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149b3ad",
   "metadata": {
    "id": "4149b3ad",
    "outputId": "bb0ab979-3747-4941-f533-1e06ca29d047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "| LR: 1.000e-05 |\n",
      "| epoch   2/10  |\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "epochs = 10\n",
    "s = 0.00001\n",
    "print('-' * 17)\n",
    "print(f\"| LR: {s:.3e} |\")\n",
    "print(f'| epoch {epoch+1:>3d}/{epochs:<3d} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569d8f9",
   "metadata": {
    "id": "8569d8f9",
    "outputId": "57936374-9bc5-47ed-a694-4ad2953e0ea5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(metrics_history['train_acc'])\n",
    "plt.plot(metrics_history['valid_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847996d",
   "metadata": {
    "id": "e847996d",
    "outputId": "931b0268-2a3b-443c-dc6c-525121e52d23"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_linear_schedule_with_warmup() missing 3 required positional arguments: 'optimizer', 'num_warmup_steps', and 'num_training_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21352\\2506784198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: get_linear_schedule_with_warmup() missing 3 required positional arguments: 'optimizer', 'num_warmup_steps', and 'num_training_steps'"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc7055",
   "metadata": {
    "id": "bfcc7055",
    "outputId": "91099e06-378b-4b8b-c1d2-5cd2fbd75b71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_resumed = GAPModel(model_name_or_path).to(device, non_blocking=True)\n",
    "optimizer_resumed = torch.optim.Adam(model_resumed.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3799f",
   "metadata": {
    "id": "f9a3799f"
   },
   "outputs": [],
   "source": [
    "path = \"../../model/checkpoints/my_model_861_6.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683896dc",
   "metadata": {
    "id": "683896dc"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(path, map_location=device)\n",
    "model_resumed.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_resumed.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0617e0d",
   "metadata": {
    "id": "e0617e0d"
   },
   "outputs": [],
   "source": [
    "# del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d185d",
   "metadata": {
    "id": "a02d185d",
    "outputId": "a19a33b5-747b-414b-f51a-15fdae74ea22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    initial_lr: 5e-06\n",
       "    lr: 2.5e-06\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_resumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d971b",
   "metadata": {
    "id": "dd7d971b",
    "outputId": "45dc87cc-1d6a-4de3-8cfc-8d18e52d9139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c3b5f",
   "metadata": {
    "id": "da5c3b5f"
   },
   "outputs": [],
   "source": [
    "scheduler_resumed = torch.optim.lr_scheduler.StepLR(optimizer_resumed, step_size=4, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d60c3a",
   "metadata": {
    "id": "b3d60c3a"
   },
   "outputs": [],
   "source": [
    "scheduler_resumed.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48209e",
   "metadata": {
    "id": "dd48209e",
    "outputId": "6dedfb7b-a93e-4b32-9275-77da7aad5749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../model/checkpoints\\my_model_861_6.pth\n",
      "861_6\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "directory = \"../../model/checkpoints/*.pth\"\n",
    "files = glob.glob(directory)\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    print(file[-9:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d78d01",
   "metadata": {
    "id": "01d78d01",
    "outputId": "d8d325cb-1780-40be-a878-d38f59c6a88e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 42, 44, 44, 61]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(train_ds[0][0])\n",
    "# tokens[train_ds[0][1][0] - 1]\n",
    "train_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad9afe",
   "metadata": {
    "id": "36ad9afe",
    "outputId": "6b9cc51f-8378-4a0a-f39f-a9f85115b9f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheryl', 'cassidy']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[42:44]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095dabb5",
   "metadata": {
    "id": "095dabb5"
   },
   "source": [
    "A List with your predictions.\n",
    "\n",
    "Each prediction is a tuple, composed by two tuples:\n",
    "(ambigous_pronoun, ambiguous_pronoun_offset), (coreferent_entity, coreferent_entity_offset))\n",
    "\n",
    "for example:\n",
    "    [(('her', 274), ('Pauline', 418))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf34075",
   "metadata": {
    "id": "6bf34075"
   },
   "source": [
    "I need to know the original positions.\n",
    "\n",
    "The model return the label prediction (0 1 2)\n",
    "\n",
    "If pred == 0 => A\n",
    "\n",
    "elif pred == 1 => B\n",
    "\n",
    "else => --\n",
    "\n",
    "I have to know:\n",
    "1. The pronoun and its pos\n",
    "2. The predicted entity and its pos\n",
    "\n",
    "\n",
    "1. I can retrieve the pronoun through the last offset in offsets after encoding the ids.\n",
    "    I also need its original position.\n",
    "    \n",
    "2. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d6256",
   "metadata": {
    "id": "180d6256"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path: str) -> List[Dict]:\n",
    "    samples: List[Dict] = []\n",
    "    pron_counter = Counter()\n",
    "    with open(path) as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            (\n",
    "                id,\n",
    "                text,\n",
    "                pron,\n",
    "                p_offset,\n",
    "                entity_A,\n",
    "                offset_A,\n",
    "                is_coref_A,\n",
    "                entity_B,\n",
    "                offset_B,\n",
    "                is_coref_B,\n",
    "                url,\n",
    "            ) = line.strip().split(\"\\t\")\n",
    "            pron_counter[pron.lower()] += 1\n",
    "            samples.append(\n",
    "                {\n",
    "                    \"id\": id,\n",
    "                    \"text\": text,\n",
    "                    \"pron\": pron,\n",
    "                    \"p_offset\": int(p_offset),\n",
    "                    \"entity_A\": entity_A,\n",
    "                    \"offset_A\": int(offset_A),\n",
    "                    \"is_coref_A\": is_coref_A,\n",
    "                    \"entity_B\": entity_B,\n",
    "                    \"offset_B\": int(offset_B),\n",
    "                    \"is_coref_B\": is_coref_B,\n",
    "                    \"url\": url,\n",
    "                }\n",
    "            )\n",
    "    print(pron_counter)\n",
    "    return samples, pron_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8bee5",
   "metadata": {
    "id": "b8a8bee5"
   },
   "outputs": [],
   "source": [
    "train_path = \"../../model/data/train.tsv\"\n",
    "valid_path = \"../../model/data/dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5e64b",
   "metadata": {
    "id": "4ab5e64b",
    "outputId": "067c9bad-ec3a-4414-8989-e1fdaf39e8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'his': 904, 'her': 773, 'he': 610, 'she': 555, 'him': 157})\n",
      "Counter({'her': 140, 'his': 108, 'he': 93, 'she': 87, 'him': 26})\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_pron_counter = read_dataset(train_path)\n",
    "valid_dataset, valid_pron_counter = read_dataset(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc6e74",
   "metadata": {
    "id": "7cbc6e74",
    "outputId": "611ea31b-3883-4644-c1df-057ce66f683e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train-1',\n",
       " 'text': \"Zoe Telford -- played the police officer girlfriend of Simon, Maggie. Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again. Phoebe Thomas played Cheryl Cassidy, Pauline's friend and also a year 11 pupil in Simon's class. Dumped her boyfriend following Simon's advice after he wouldn't have sex with her but later realised this was due to him catching crabs off her friend Pauline.\",\n",
       " 'pron': 'her',\n",
       " 'p_offset': 274,\n",
       " 'entity_A': 'Cheryl Cassidy',\n",
       " 'offset_A': 191,\n",
       " 'is_coref_A': 'TRUE',\n",
       " 'entity_B': 'Pauline',\n",
       " 'offset_B': 207,\n",
       " 'is_coref_B': 'FALSE',\n",
       " 'url': 'http://en.wikipedia.org/wiki/List_of_Teachers_(UK_TV_series)_characters'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635bc0e",
   "metadata": {
    "id": "e635bc0e"
   },
   "outputs": [],
   "source": [
    "def get_entity_and_offset_from_id(label_id, sentence):\n",
    "    if label_id == 0: # Entity A\n",
    "        return sentence['entity_A'], sentence['offset_A']\n",
    "    elif label_id == 1: # Entity B\n",
    "        return sentence['entity_B'], sentence['offset_B']\n",
    "    else: # Neither\n",
    "        return None, None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00c15d",
   "metadata": {
    "id": "3a00c15d",
    "outputId": "2390d1d4-9f8b-4102-ee5b-1d15fa04e839"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entity_and_offset_from_id(2, train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca534a6",
   "metadata": {
    "id": "6ca534a6"
   },
   "outputs": [],
   "source": [
    "def predict(model, sentences: List[Dict]) -> List[Tuple[Tuple[str, int], Tuple[str, int]]]:\n",
    "    df = pd.DataFrame(sentences)\n",
    "    \n",
    "    # tokenizer will be self.tokenizer in the final implementation\n",
    "    dataset = GAPDataset(df, tokenizer)\n",
    "    collator = Collator(device)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    #self.model.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataloader = DataLoader(dataset, batch_size=1, \n",
    "                                collate_fn=collator, shuffle=False)\n",
    "        \n",
    "        for (features, offsets, labels), sentence in zip(dataloader, sentences):\n",
    "            \n",
    "#             predictions = self.model(features, offsets).argmax(1).item()\n",
    "            predicted_label_id = model(features, offsets).argmax(1).item()\n",
    "            pred_entity, pred_entity_offset = get_entity_and_offset_from_id(predicted_label_id, sentence)\n",
    "            pron, pron_offset = sentence['pron'],  sentence['p_offset']\n",
    "            \n",
    "            predictions.append(((pron, pron_offset), (pred_entity, pred_entity_offset)))\n",
    "            \n",
    "    return predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427614f",
   "metadata": {
    "id": "c427614f"
   },
   "outputs": [],
   "source": [
    "pred = predict(model_resumed, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba0ffa",
   "metadata": {
    "id": "9bba0ffa"
   },
   "outputs": [],
   "source": [
    "gold_values = []\n",
    "for sentence in valid_dataset:\n",
    "    gold_both_wrong = sentence[\"is_coref_A\"] == \"FALSE\" and sentence[\"is_coref_B\"] == \"FALSE\"\n",
    "    if gold_both_wrong:\n",
    "        gold_entity_offset = None\n",
    "        gold_entity = None\n",
    "    else:\n",
    "        gold_entity_offset = (\n",
    "            sentence[\"offset_A\"] if sentence[\"is_coref_A\"] == \"TRUE\" else sentence[\"offset_B\"]\n",
    "        )\n",
    "        gold_entity = (\n",
    "            sentence[\"entity_A\"] if sentence[\"is_coref_A\"] == \"TRUE\" else sentence[\"entity_B\"]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    gold_values.append(((sentence['pron'], sentence['p_offset']),(gold_entity, gold_entity_offset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94057fda",
   "metadata": {
    "id": "94057fda",
    "outputId": "dd206596-b432-4181-f96d-97e7a2f3f1ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {}\n",
    "\n",
    "a.setdefault(\"age\", 0)\n",
    "a.setdefault(\"age\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ba563",
   "metadata": {
    "id": "940ba563",
    "outputId": "dd0b78b1-dc5f-4023-dd76-677df3ffe869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 0, 'a': 0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['a'] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c72b7",
   "metadata": {
    "id": "b86c72b7"
   },
   "outputs": [],
   "source": [
    "wrong = []\n",
    "for p, g in zip(pred, gold_values):\n",
    "    if g[1][0] != p[1][0]:\n",
    "        wrong.append((g, p))\n",
    "#     print(p[1], g[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f3f38",
   "metadata": {
    "id": "5d5f3f38"
   },
   "outputs": [],
   "source": [
    "pr = wrong[0][0][0][0]\n",
    "get_gender(\"she\")\n",
    "\n",
    "gender = {}\n",
    "\n",
    "for pair in wrong:\n",
    "    pr = pair[0][0][0]\n",
    "    gender.setdefault(get_gender(pr.lower()), 0)\n",
    "    gender[get_gender(pr.lower())] += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef2627",
   "metadata": {
    "id": "19ef2627",
    "outputId": "39bd69ff-30b3-4107-9796-c65d11499e76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 35, 0: 34}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d475e6f",
   "metadata": {
    "id": "2d475e6f",
    "outputId": "b09b0735-78cd-41d6-f2ab-a67274d07c31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'validation-2',\n",
       " 'text': \"Kathleen Nott was born in Camberwell, London. Her father, Philip, was a lithographic printer, and her mother, Ellen, ran a boarding house in Brixton; Kathleen was their third daughter. She was educated at Mary Datchelor Girls' School (now closed), London, before attending King's College, London.\",\n",
       " 'pron': 'She',\n",
       " 'p_offset': 185,\n",
       " 'entity_A': 'Ellen',\n",
       " 'offset_A': 110,\n",
       " 'is_coref_A': 'FALSE',\n",
       " 'entity_B': 'Kathleen',\n",
       " 'offset_B': 150,\n",
       " 'is_coref_B': 'TRUE',\n",
       " 'url': 'http://en.wikipedia.org/wiki/Kathleen_Nott'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429bd56",
   "metadata": {
    "id": "f429bd56"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(predictions_s, samples):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, label in zip(predictions_s, samples):\n",
    "        gold_pron_offset = label[\"p_offset\"]\n",
    "        pred_pron_offset = pred[0][1] if len(pred[0]) > 0 else None\n",
    "        gold_pron = label[\"pron\"]\n",
    "        pred_pron = pred[0][0] if len(pred[0]) > 0 else None\n",
    "        gold_both_wrong = label[\"is_coref_A\"] == \"FALSE\" and label[\"is_coref_B\"] == \"FALSE\"\n",
    "        pred_entity_offset = pred[1][1] if len(pred[1]) > 0 else None\n",
    "        pred_entity = pred[1][0] if len(pred[1]) > 0 else None\n",
    "              \n",
    "        if gold_both_wrong:\n",
    "            if pred_entity is None and gold_pron_offset == pred_pron_offset and gold_pron == pred_pron:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            gold_entity_offset = (\n",
    "                label[\"offset_A\"] if label[\"is_coref_A\"] == \"TRUE\" else label[\"offset_B\"]\n",
    "            )\n",
    "            gold_entity = (\n",
    "                label[\"entity_A\"] if label[\"is_coref_A\"] == \"TRUE\" else label[\"entity_B\"]\n",
    "            )\n",
    "            if (\n",
    "                gold_pron_offset == pred_pron_offset\n",
    "                and gold_pron == pred_pron\n",
    "                and gold_entity_offset == pred_entity_offset\n",
    "                and gold_entity == pred_entity\n",
    "            ):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    print(f\"# instances: {total}\")\n",
    "    acc = float(correct) / total\n",
    "    print(f\"# accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970c3d9",
   "metadata": {
    "id": "f970c3d9",
    "outputId": "a73a7b89-4175-45f2-ce1b-1c88e4ee808e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances: 454\n",
      "# accuracy: 0.8568\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(pred, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908b386",
   "metadata": {
    "id": "3908b386"
   },
   "outputs": [],
   "source": [
    "from arguments import CustomTrainingArguments\n",
    "a = CustomTrainingArguments(output_dir=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff3c45",
   "metadata": {
    "id": "aaff3c45",
    "outputId": "a920bfef-2e7b-405d-e992-5756cf1ea819"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTrainingArguments(output_dir='c', resume_from_checkpoint=None, save_model=False, num_train_epochs=3, logging_steps=250, learning_rate=0.0005, grad_clipping=None, early_stopping=False, early_stopping_mode='max', early_stopping_patience=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ae21f",
   "metadata": {
    "id": "0b7ae21f",
    "outputId": "22c6c145-e93b-4a94-f7fb-47600fbbc5cc"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'resume_from_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9356\\1665919785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomTrainingArguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training_args'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'resume_from_checkpoint'"
     ]
    }
   ],
   "source": [
    "from arguments import CustomTrainingArguments\n",
    "\n",
    "yaml_file = \"./train.yaml\"\n",
    "\n",
    "# Read configuration file with all the necessary parameters\n",
    "with open(yaml_file) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "arg = CustomTrainingArguments(**config['training_args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9ff37",
   "metadata": {
    "id": "d9e9ff37",
    "outputId": "0b3baa1d-1449-4619-deea-490de73200d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTrainingArguments(output_dir='../../model/checkpoints/', num_train_epochs=6, logging_steps=250, save_model=False, learning_rate=5e-06, grad_clipping=None, early_stopping=False, early_stopping_mode='max', early_stopping_patience=0)\n"
     ]
    }
   ],
   "source": [
    "print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6450b",
   "metadata": {
    "id": "c3a6450b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
