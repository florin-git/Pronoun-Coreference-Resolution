{"cells":[{"cell_type":"markdown","id":"1d985175","metadata":{"id":"1d985175"},"source":["This is the principal notebook where are described the Baseline and the Multiple Choice implementations."]},{"cell_type":"code","execution_count":null,"id":"wIOz40JYJddq","metadata":{"collapsed":true,"id":"wIOz40JYJddq"},"outputs":[],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":null,"id":"3244dc15","metadata":{"id":"3244dc15"},"outputs":[],"source":["import transformers\n","from transformers import (\n","    AutoTokenizer,\n","    BertModel,\n","    logging\n",")\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n","\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","import random\n","import time\n","import math\n","import yaml\n","from typing import *\n","from datetime import datetime\n","from collections import namedtuple\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import GradScaler\n","\n","SEED = 10\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","\n","# Display the entire text\n","pd.set_option(\"display.max_colwidth\", None)\n","logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":null,"id":"58276aa2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1662994192854,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"58276aa2","outputId":"f723dd3a-9b0e-467e-d4e7-cc22c2bac883"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"id":"kYGIp3MoJug2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56889,"status":"ok","timestamp":1662994249738,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"kYGIp3MoJug2","outputId":"09f960c6-2747-429e-a4ef-1f5f0b4042cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["# For Colab\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","curr_location = \"/content/drive/MyDrive/Colab Notebooks/Deep Learning/NLP/nlp2022-hw3/hw3/stud/\"\n","os.chdir(curr_location)"]},{"cell_type":"code","execution_count":null,"id":"QFqvmoaqhbyn","metadata":{"id":"QFqvmoaqhbyn"},"outputs":[],"source":["curr_location = \"H:/My Drive/Colab Notebooks/Deep Learning/NLP/nlp2022-hw3/hw3/stud\"\n","os.chdir(curr_location)"]},{"cell_type":"code","execution_count":null,"id":"fM1EDyRLifxE","metadata":{"id":"fM1EDyRLifxE"},"outputs":[],"source":["from arguments import *"]},{"cell_type":"markdown","id":"yxUo-C4RjyOY","metadata":{"id":"yxUo-C4RjyOY"},"source":["I defined custom arguments for the model and the training process in the arguments.py, by taking inspiration from the Hugginface [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) class.\n","\n","\n","In this way, all the arguments will be read from this file and it is therefore easy to organized them. Moreover, using [HfArgumentParser](https://huggingface.co/docs/transformers/v4.21.3/en/internal/trainer_utils#transformers.HfArgumentParser) we can turn these classes\n","into argparse arguments to be able to specify them on\n","the command line."]},{"cell_type":"code","execution_count":null,"id":"PUaqDLdTYJ1r","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1662994254005,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"PUaqDLdTYJ1r","outputId":"3ac8c086-093d-4f21-ab12-a41e11d0e4cc"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bert-large-uncased'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["yaml_file = \"./train_notebook2.yaml\"\n","# Read configuration file with all the necessary parameters\n","with open(yaml_file) as file:\n","    config = yaml.safe_load(file)\n","    \n","model_args = ModelArguments(**config['model_args'])\n","model_name_or_path = model_args.model_name_or_path\n","model_name_or_path"]},{"cell_type":"code","execution_count":null,"id":"3633ef51","metadata":{"id":"3633ef51"},"outputs":[],"source":["train_clean_path = \"../../model/data/train_clean.tsv\"\n","valid_clean_path = \"../../model/data/valid_clean.tsv\""]},{"cell_type":"code","execution_count":null,"id":"B1CvSmUrKc1y","metadata":{"id":"B1CvSmUrKc1y"},"outputs":[],"source":["df_train = pd.read_csv(filepath_or_buffer=train_clean_path, sep=\"\\t\")\n","df_valid = pd.read_csv(filepath_or_buffer=valid_clean_path, sep=\"\\t\")"]},{"cell_type":"markdown","id":"45e260db","metadata":{"id":"45e260db"},"source":["The `GAP_Dataset` class has many configuration parameters, since I have tried different approaches. I decided not to split the class in different subclasses because a particular configuration only requires to change few lines of code. "]},{"cell_type":"markdown","id":"0bd728a4","metadata":{"id":"0bd728a4"},"source":["The idea behind the **multiple choice** dataset is taken from the paper [Gendered Pronoun Resolution using BERT and an Extractive Question Answering Formulation](https://aclanthology.org/W19-3819.pdf).\n","\n","\n","In the **multiple choice** approach, each sample feature is composed by *num_choices* lists (in this case 3), where an option has the following structure:\n","    \n","CLS + context + SEP + ambiguous pronoun \"is\" + option + SEP\n","\n","where:\n","- **context** is the text passage\n","- **option** is one of A’s name, B’s name or the word \"neither\" if the pronoun in the example doesn’t co-refer with A and B."]},{"cell_type":"code","execution_count":null,"id":"fc50ed87","metadata":{"id":"fc50ed87"},"outputs":[],"source":["class GAP_Dataset(Dataset):\n","    \"\"\"\n","    Custom GAP dataset for multiple choice or sequence classfication.\n","    \n","    Parameters\n","    ----------\n","    df: pd.DataFrame\n","        A dataframe from the GAP dataset.\n","        \n","    tokenizer: PreTrainedTokenizerBase\n","        The tokenizer used to preprocess the data.\n","        \n","    tag_labels: Dict[str, str]\n","        A dictionary containing as values the tags that will be inserted\n","        to delimit an entity or a pronoun.\n","        \n","    multiple_choice: bool\n","        Whether to instantiate a dataset for multiple choice\n","        classification or not.\n","        \n","    keep_tags: bool\n","        If true the tags added to text are kept even after\n","        the tokenization process.\n","        \n","    truncate_up_to_pron: bool\n","        If false the text is not truncated. \n","        Otherwise, the text will be truncated at the last tag inserted, \n","        that can delimit either an entity or the amiguous pronoun, \n","        with the addition of the ambigous pronoun at the end.\n","  \n","    labeled: bool\n","        If the dataset also contains the labels.\n","        \n","    cleaned: bool\n","        Whether the GAP dataframe is already cleaned or not.\n","    \"\"\"\n","    def __init__(\n","        self, \n","        df: pd.DataFrame, \n","        tokenizer: PreTrainedTokenizerBase, \n","        tag_labels: Dict[str, str], \n","        multiple_choice: bool=False,\n","        keep_tags: bool=False, \n","        truncate_up_to_pron: bool=True, \n","        labeled: bool=True, \n","        cleaned: bool=True\n","    ):\n","        \n","        if not cleaned:\n","             self.clean_dataframe(df)\n","\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        self.multiple_choice = multiple_choice\n","        self.keep_tags = keep_tags\n","        self.truncate_up_to_pron = truncate_up_to_pron\n","        self.labeled = labeled\n","        \n","        self.tag_labels = tag_labels\n","        self._init_tag_labels()\n","        \n","        self.samples = []\n","        self._convert_tokens_to_ids()\n","        \n","    @staticmethod\n","    def clean_text(text: str):\n","        text = text.translate(str.maketrans(\"`\", \"'\"))\n","        return text\n","\n","    def clean_dataframe(self, df: pd.DataFrame):\n","        df['text'] = df['text'].map(self.clean_text)\n","        df['entity_A'] = df['entity_A'].map(self.clean_text) \n","        df['entity_B'] = df['entity_B'].map(self.clean_text) \n","        \n","    \n","    def _init_tag_labels(self):\n","        self.pronoun_tag = self.tag_labels['pronoun_tag']\n","        self.start_A_tag = self.tag_labels['start_A_tag']\n","        self.end_A_tag = self.tag_labels['end_A_tag']\n","        self.start_B_tag = self.tag_labels['start_B_tag']\n","        self.end_B_tag = self.tag_labels['end_B_tag']\n","\n","\n","    @staticmethod\n","    def get_class_id(is_coref_A: Union[str, bool], is_coref_B: Union[str, bool]) -> int:\n","        \"\"\"\n","        Returns\n","        -------\n","            An integer representing the class of a input sentence.\n","            The class id is:\n","            - 0 if the pronoun references entity A\n","            - 1 if the pronoun references entity B\n","            - 2 if the pronoun references neither A nor B\n","        \"\"\"\n","        if is_coref_A in [\"TRUE\", True]:\n","            return 0\n","        elif is_coref_B in [\"TRUE\", True]:\n","            return 1\n","        else:\n","            return 2\n","        \n","    def _convert_tokens_to_ids(self):\n","        CLS = [self.tokenizer.cls_token]\n","        SEP = [self.tokenizer.sep_token]\n","\n","        Sample = namedtuple(\"Sample\", ['tokens', 'offsets'])\n","        if self.labeled:\n","            Sample = namedtuple(\"Sample\", Sample._fields + (\"labels\",))\n","\n","        for _, row in self.df.iterrows():\n","            tokens, offsets = self._tokenize(row)\n","            \n","            if self.multiple_choice:\n","                final_tokens = self._create_multiple_choices(tokens, offsets)\n","            else:\n","                tokens_to_convert = CLS + tokens + SEP\n","                final_tokens = self.tokenizer.convert_tokens_to_ids(tokens_to_convert)\n","                \n","            sample = {'tokens': final_tokens,\n","                      'offsets': self._get_offsets_list(offsets)}\n","\n","            if self.labeled:\n","                sample['labels'] = self.get_class_id(row['is_coref_A'], row['is_coref_B'])\n","\n","            sample_namedtuple = Sample(**sample)\n","            self.samples.append(sample_namedtuple)\n","\n","            \n","    def _create_multiple_choices(self, tokens: List[int], offsets: Dict[str, List[int]]) -> List[int]:\n","        CLS = [self.tokenizer.cls_token]\n","        SEP = [self.tokenizer.sep_token]\n","        \n","        pronoun = tokens[offsets[self.pronoun_tag][0]]\n","        A_entity = tokens[offsets[self.start_A_tag][0]:offsets[self.end_A_tag][0]]\n","        B_entity = tokens[offsets[self.start_B_tag][0]:offsets[self.end_B_tag][0]]\n","        \n","        final_tokens = []\n","        \n","        first = CLS + tokens + SEP + [pronoun, \"is\"] + A_entity + SEP\n","        second = CLS + tokens + SEP + [pronoun, \"is\"] + B_entity + SEP   \n","        third = CLS + tokens + SEP + [pronoun, \"is\", \"neither\"] + SEP\n","        \n","        tokens_to_convert = [first, second, third]\n","        final_tokens = [self.tokenizer.convert_tokens_to_ids(choice) for choice in tokens_to_convert]\n","\n","        return final_tokens\n","    \n","\n","    def _get_offsets_list(self, offsets: Dict[str, List[int]]) -> List[int]:\n","        # 1 is added for the introduction of the CLS token\n","        offsets_A = [offsets[self.start_A_tag][0] + 1, offsets[self.end_A_tag][0] + 1]\n","        offsets_B = [offsets[self.start_B_tag][0] + 1, offsets[self.end_B_tag][0] + 1]\n","\n","        return offsets_A + offsets_B + [offsets[self.pronoun_tag][0] + 1] \n","\n","        \n","    def _insert_tag(self, text: str, offsets: Tuple[int, int], \n","                    start_tag: str, end_tag: str = None) -> str:\n","        start_off, end_off = offsets \n","\n","        # Starting tag only\n","        if end_tag is None:\n","            text = text[:start_off] + start_tag + text[start_off:]\n","            return text\n","\n","        text = text[:start_off] + start_tag + text[start_off:end_off] + end_tag + text[end_off:]\n","        return text\n","\n","    \"\"\"\n","    The methods '_delimit_mentions' and '_tokenize' are inspired \n","    by the work of `rakeshchada` in the repository \n","    \"https://github.com/rakeshchada/corefqa/blob/master/CorefSeq.ipynb\"\n","    \"\"\"\n","    def _delimit_mentions(self, row: pd.Series) -> str:\n","        text = row['text']\n","        pronoun = row['pron']\n","        A_entity = row['entity_A']\n","        B_entity = row['entity_B']\n","\n","        # Sort the offsets in ascending order\n","        break_points = sorted([\n","            (self.pronoun_tag, row['p_offset']),\n","            (self.start_A_tag, row['offset_A']),\n","            (self.end_A_tag, row['offset_A'] + len(A_entity)),\n","            (self.start_B_tag, row['offset_B']),\n","            (self.end_B_tag, row['offset_B'] + len(B_entity)),\n","        ], key=lambda x: x[1])\n","\n","        # When a new tag is inserted, the offset of the next tag\n","        # changes by the length of the inserted tag.\n","        len_added_tags = 0\n","        for tag, offset in break_points:\n","            offset += len_added_tags\n","            text = self._insert_tag(text, (offset, None), tag)\n","            len_added_tags += len(tag)\n","\n","        # Truncate the text at the last tag inserted and append the pronoun at the end\n","        if self.truncate_up_to_pron:\n","            text = text[:offset+len(tag)] + pronoun\n","        \n","        return text\n","\n","    def _tokenize(self, row: pd.Series) -> Tuple[List[int], Dict[str, List[int]]]: \n","        \"\"\"\n","        Tokenize the text.\n","        If keep_tags is True, also the tags are tokenized.\n","        \"\"\"\n","        \n","        tokens = []\n","        tag_labels = self.tag_labels\n","        offsets = {tag: [] for tag in tag_labels.values()}\n","\n","        text = self._delimit_mentions(row)\n","\n","        # Also the tags are added to the tokens\n","        if self.keep_tags:\n","            for token in self.tokenizer.tokenize(text):    \n","                tokens.append(token)\n","\n","                if token in [*tag_labels.values()]:\n","                    if \"/\" in token: # End token\n","                        offsets[token].append(len(tokens)-1)\n","                    else:\n","                        offsets[token].append(len(tokens)) \n","        \n","        # The tags are skipped\n","        else:\n","            for token in self.tokenizer.tokenize(text):              \n","                if token in [*tag_labels.values()]:\n","                    offsets[token].append(len(tokens)) \n","                    continue\n","                tokens.append(token)\n","        \n","        return tokens, offsets\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        return self.samples[idx]"]},{"cell_type":"code","execution_count":null,"id":"18fe9e9c","metadata":{"id":"18fe9e9c"},"outputs":[],"source":["tag_labels = {\n","    \"pronoun_tag\": \"<p>\",\n","    \"start_A_tag\": \"<a>\",\n","    \"end_A_tag\": \"</a>\",\n","    \"start_B_tag\": \"<b>\",\n","    \"end_B_tag\": \"</b>\"\n","}"]},{"cell_type":"code","execution_count":null,"id":"xJq1Zg3tYmNs","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1662994263821,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"xJq1Zg3tYmNs","outputId":"fb7cce43-a425-4b14-8647-bb8419a33a4d"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bert-large-uncased'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_name_or_path = model_args.tokenizer\n","if tokenizer_name_or_path is None:\n","    tokenizer_name_or_path = model_name_or_path\n","tokenizer_name_or_path"]},{"cell_type":"markdown","id":"da7fedef","metadata":{"id":"da7fedef"},"source":["I specify to the tokenizer not to split the tag labels that will be used to delimit the relevant tokens, such as entities and the pronoun."]},{"cell_type":"code","execution_count":null,"id":"e6844631","metadata":{"id":"e6844631"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, never_split=list(tag_labels.values()))\n","tokenizer.add_tokens(list(tag_labels.values()), special_tokens=True)"]},{"cell_type":"code","execution_count":null,"id":"7997cbe1","metadata":{"id":"7997cbe1","scrolled":true},"outputs":[],"source":["train_ds = GAP_Dataset(df_train, tokenizer, tag_labels)\n","valid_ds = GAP_Dataset(df_valid, tokenizer, tag_labels)"]},{"cell_type":"code","execution_count":null,"id":"40072db5","metadata":{"id":"40072db5"},"outputs":[],"source":["train_ds = GAP_Dataset(df_train, tokenizer, tag_labels, multiple_choice=True)\n","valid_ds = GAP_Dataset(df_valid, tokenizer, tag_labels, multiple_choice=True)"]},{"cell_type":"code","execution_count":null,"id":"28e9d2f0","metadata":{"id":"28e9d2f0"},"outputs":[],"source":["def compute_max_len(sequences: Union[List[List[int]], Tuple[List[int]]], truncate_len: int) -> int:\n","    \"\"\"\n","    Computes the maximum length in the sequences.         \n","    \"\"\"\n","    max_len = min(\n","        max((len(x) for x in sequences)),\n","        truncate_len\n","    )\n","    return max_len\n","    \n","def pad_sequence(sequences: Union[List[List[int]], Tuple[List[int]]], max_len: int, pad: int) -> np.ndarray:\n","    \"\"\"\n","    Returns\n","    -------\n","        A numpy array padded with the 'pad' value until \n","        the 'max_len' length. \n","\n","    Parameters\n","    ----------\n","    sequences: Union[List[List[int]], Tuple[List[int]]]\n","        A list or tuple of lists. \n","\n","    max_len: int\n","        The length to which the input is padded.\n","\n","    pad: int\n","        The padding value.\n","    \"\"\"\n","    array_sequences = np.full((len(sequences), max_len), pad, dtype=np.int64)\n","\n","    # Padding\n","    for i, sequence in enumerate(sequences):\n","        array_sequences[i, :len(sequence)] = sequence\n","\n","    return array_sequences"]},{"cell_type":"code","execution_count":null,"id":"56ccbad5","metadata":{"id":"56ccbad5"},"outputs":[],"source":["class Collator:\n","    \"\"\"\n","    Collator for Sequence Classification.\n","    \n","    Returns\n","    -------\n","        A dictionary of tensors of the batch sequences in input.\n","\n","    Parameters\n","    ----------\n","    device: str\n","        Where (CPU/GPU) to load the features.\n","        \n","    pad: int\n","        The padding value.\n","\n","    truncate_len: int\n","        Maximum length possible in the batch.\n","\n","    labeled: bool\n","        If the batch also contains the labels.\n","    \"\"\"\n","    def __init__(self, device: str, pad: int=0, \n","                 truncate_len: int=512, labeled=True):\n","        self.device = device\n","        self.pad = pad\n","        self.truncate_len = truncate_len\n","        self.labeled = labeled\n","\n","    def __call__(self, batch: list) -> Dict[str, torch.Tensor]:\n","        \n","        if self.labeled:\n","            batch_features, batch_offsets, batch_labels = zip(*batch)\n","\n","        else:\n","            batch_features, batch_offsets = zip(*batch)\n","\n","        max_len = compute_max_len(batch_features, self.truncate_len)\n","\n","        collate_sample = {}\n","\n","        # Features\n","        padded_features = pad_sequence(batch_features, max_len, self.pad)\n","        features_tensor = torch.tensor(padded_features, device=self.device)\n","        collate_sample['features'] = features_tensor\n","\n","        # Offsets\n","        offsets_tensor = torch.tensor(batch_offsets, device=self.device)\n","        collate_sample['offsets'] = offsets_tensor\n","\n","        if not self.labeled:\n","            return collate_sample\n","\n","        # Labels\n","        labels_tensor = torch.tensor(\n","            batch_labels, dtype=torch.uint8, device=self.device)\n","        collate_sample['labels'] = labels_tensor\n","\n","        return collate_sample"]},{"cell_type":"markdown","id":"7e7094c6","metadata":{"id":"7e7094c6"},"source":["The collator for multiple choice classification has to flatten the input features in order to pad/truncate the lists. After those operations the features are reshape to the original shape, i.e., `[bacth_size, num_choices]`."]},{"cell_type":"code","execution_count":null,"id":"aa6382aa","metadata":{"id":"aa6382aa"},"outputs":[],"source":["class Collator_Multi_Choice:\n","    \"\"\"\n","    Collator for Multiple Choice Classification.\n","    \n","    Returns\n","    -------\n","        A dictionary of tensors of the batch sequences in input.\n","\n","    Parameters\n","    ----------\n","    device: str\n","        Where (CPU/GPU) to load the features.\n","        \n","    pad: int\n","        The padding value.\n","\n","    truncate_len: int\n","        Maximum length possible in the batch.\n","\n","    labeled: bool\n","        If the batch also contains the labels.\n","    \"\"\"\n","    def __init__(self, device: str, pad: int=0, \n","                 truncate_len: int=512, labeled=True):\n","        self.device = device\n","        self.pad = pad\n","        self.truncate_len = truncate_len\n","        self.labeled = labeled\n","        \n","    def __call__(self, batch: list) -> Dict[str, torch.Tensor]:\n","        if self.labeled:\n","            batch_features, _, batch_labels = zip(*batch)\n","\n","        else:\n","            batch_features, _ = zip(*batch)\n","        \n","        batch_size = len(batch_features)\n","        num_choices = len(batch_features[0])\n","\n","        collate_sample = {}\n","\n","        # Features\n","        # batch_features shape: batch_size x num_choices\n","        flattened_batch_features = sum(batch_features, [])\n","        # flattened_batch_features shape: batch_size * num_choices\n","        max_len = compute_max_len(flattened_batch_features, self.truncate_len)\n","\n","        padded_flattened_features = pad_sequence(flattened_batch_features, max_len, self.pad)\n","        flattened_features_tensor = torch.tensor(padded_flattened_features, device=self.device)\n","        features_tensor = flattened_features_tensor.view(batch_size, num_choices, -1)\n","\n","        collate_sample['features'] = features_tensor\n","\n","        if not self.labeled:\n","            return collate_sample\n","\n","        # Labels\n","        labels_tensor = torch.tensor(\n","            batch_labels, dtype=torch.uint8, device=self.device)\n","        collate_sample['labels'] = labels_tensor\n","\n","        return collate_sample"]},{"cell_type":"code","execution_count":null,"id":"8185e7a8","metadata":{"id":"8185e7a8"},"outputs":[],"source":["class Entity_Resolution_Head(nn.Module):\n","    def __init__(self, bert_hidden_size: int, args: ModelArguments):\n","        super().__init__()\n","\n","        self.args = args\n","        self.bert_hidden_size = bert_hidden_size\n","\n","        \n","        input_size = bert_hidden_size * 3\n","        if args.output_strategy == \"concat\":\n","            input_size *= 4\n","        \n","        self.ffnn = nn.Sequential(\n","            nn.Linear(input_size, args.head_hidden_size),\n","            nn.BatchNorm1d(args.head_hidden_size),\n","            nn.LeakyReLU(),\n","            nn.Dropout(args.dropout),\n","        )\n","        \n","        self.classifier = nn.Linear(args.head_hidden_size, args.num_output)\n","\n","    def forward(self, bert_outputs, offsets):\n","        assert bert_outputs.shape[2] == self.bert_hidden_size\n","        embeddings = self._retrieve_entities_and_pron_embeddings(bert_outputs,\n","                                                           offsets)\n","\n","        x = self.ffnn(embeddings)\n","        output = self.classifier(x)\n","        return output\n","    \n","    def _retrieve_entities_and_pron_embeddings(self, bert_embeddings: torch.Tensor,\n","                                               entities_and_pron_offsets: torch.Tensor):\n","        \n","        # bert_embeddings shape: batch_size x seq_length x embed_dim\n","        # entities_and_pron_offsets shape: batch_size x 5\n","        \n","        embeddings_A = []\n","        embeddings_B = []\n","        embeddings_pron = []\n","\n","        # Consider embeddings and offsets in each batch separately\n","        for embeddings, off in zip(bert_embeddings, entities_and_pron_offsets):\n","            # The offsets of mention A are the first and the second\n","            # in the 'off' tensor\n","            offsets_ent_A = range(off[0], off[1])\n","            # The offsets of mention B are the third and the fourth\n","            # in the 'off' tensor\n","            offsets_ent_B = range(off[2], off[3])\n","            # The offset of the pronoun is the last in the 'off' tensor\n","            offset_pron = off[-1]\n","\n","            # The embedding of a mention is the mean of\n","            # all the subtokens embeddings that represent it\n","            embeddings_A.append(embeddings[offsets_ent_A].mean(dim=0))\n","            embeddings_B.append(embeddings[offsets_ent_B].mean(dim=0))\n","            embeddings_pron.append(embeddings[offset_pron])\n","\n","        # Merge outputs\n","        merged_entities_and_pron_embeddings = torch.cat([\n","            torch.stack(embeddings_A, dim=0),\n","            torch.stack(embeddings_B, dim=0),\n","            torch.stack(embeddings_pron, dim=0)\n","        ], dim=1)\n","\n","        # shape: batch_size x (embedding_dim * 3)\n","        return merged_entities_and_pron_embeddings"]},{"cell_type":"code","execution_count":null,"id":"eb7c679d","metadata":{"id":"eb7c679d"},"outputs":[],"source":["class CR_Model(nn.Module):\n","    \"\"\"The main model.\"\"\"\n","\n","    def __init__(self, bert_model: str, tokenizer, args: ModelArguments):\n","        super().__init__()\n","\n","        self.args = args\n","\n","        if bert_model in {\"bert-base-uncased\", \"bert-base-cased\"}:\n","            self.bert_hidden_size = 768\n","        elif bert_model in {\"bert-large-uncased\", \"bert-large-cased\"}:\n","            self.bert_hidden_size = 1024\n","        else:\n","            self.bert_hidden_size = args.bert_hidden_size\n","\n","        self.bert = BertModel.from_pretrained(\n","            bert_model).to(device, non_blocking=True)\n","\n","        # If the tag tokens (e.g., <p>, <a> etc.) are present in the features,\n","        # the embedding dimension of the bert embeddings must be changed\n","        # to be compliant with the new size of the tokenizer vocabulary. \n","        if args.resize_embeddings:\n","            self.bert.resize_token_embeddings(len(tokenizer.vocab))\n","\n","        self.head = Entity_Resolution_Head(self.bert_hidden_size, args).to(\n","            device, non_blocking=True) \n","    \n","    def forward(self, sample):\n","        x = sample['features']\n","        x_offsets = sample['offsets']\n","\n","        bert_outputs = self.bert(\n","            x, attention_mask=(x > 0).long(),\n","            token_type_ids=None, output_hidden_states=True)\n","\n","        if self.args.output_strategy == \"last\":\n","            out = bert_outputs.last_hidden_state\n","\n","        elif self.args.output_strategy == \"concat\":\n","            out = torch.cat([bert_outputs.hidden_states[x] for x in [-1, -2, -3, -4]], dim=-1)\n","\n","        elif self.args.output_strategy == \"sum\":\n","            layers_to_sum = torch.stack([bert_outputs.hidden_states[x] for x in [-1, -2, -3, -4]], dim=0)\n","            out = torch.sum(layers_to_sum, dim=0)\n","\n","        else:\n","            raise ValueError(\"Unsupported output strategy.\")\n","\n","        head_outputs = self.head(out, x_offsets)\n","        return head_outputs"]},{"cell_type":"code","execution_count":null,"id":"6nkQ_PtpZG7b","metadata":{"id":"6nkQ_PtpZG7b"},"outputs":[],"source":["class Multi_Choice_Head(nn.Module):\n","    def __init__(self, bert_hidden_size: int, args: ModelArguments):\n","        super().__init__()\n","\n","        self.args = args\n","        self.bert_hidden_size = bert_hidden_size\n","\n","        self.normalize = nn.BatchNorm1d(bert_hidden_size)\n","        self.dropout = nn.Dropout(0.4)\n","        self.classifier = nn.Linear(bert_hidden_size, 1)\n","\n","    def forward(self, pooled_output, num_choices):\n","        pooled_output = self.normalize(pooled_output)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        \n","        output = logits.view(-1, num_choices)\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"47I1-pGHa2S4","metadata":{"id":"47I1-pGHa2S4"},"outputs":[],"source":["class Multi_Choice_Model(nn.Module):\n","    \"\"\"The main model.\"\"\"\n","\n","    def __init__(self, bert_model: str, tokenizer, args: ModelArguments):\n","        super().__init__()\n","\n","        self.args = args\n","\n","        if bert_model in {\"bert-base-uncased\", \"bert-base-cased\"}:\n","            self.bert_hidden_size = 768\n","        elif bert_model in {\"bert-large-uncased\", \"bert-large-cased\"}:\n","            self.bert_hidden_size = 1024\n","        else:\n","            self.bert_hidden_size = args.bert_hidden_size\n","\n","        self.bert = BertModel.from_pretrained(\n","            bert_model).to(device, non_blocking=True)\n","\n","        # If the tag tokens (e.g., <p>, <a> etc.) are present in the features,\n","        # the embedding dimension of the bert embeddings must be changed\n","        # to be compliant with the new size of the tokenizer vocabulary. \n","        if args.resize_embeddings:\n","            self.bert.resize_token_embeddings(len(tokenizer.vocab))\n","\n","        self.head = Multi_Choice_Head(self.bert_hidden_size, args).to(\n","            device, non_blocking=True) \n","    \n","    def forward(self, sample):\n","        x = sample['features']\n","        num_choices = x.shape[1]\n","        x = x.view(-1, x.shape[-1])\n","\n","        bert_outputs = self.bert(\n","            x, attention_mask=(x > 0).long(),\n","            token_type_ids=None, output_hidden_states=True)\n","\n","        out = bert_outputs.pooler_output\n","\n","        head_outputs = self.head(out, num_choices)\n","        return head_outputs"]},{"cell_type":"markdown","id":"82e07a34","metadata":{"id":"82e07a34"},"source":["Since the Transformer models are pretty heavy, I tried to optimize the training by applying some best practices from this [blog](https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6).\n","\n","In particular, I tried to:\n","- avoid unnecessary data transfer between CPU and GPU by directly loading tensors to the correct device.\n","- use `tensor.to(non_blocking=True) `when it's applicable to overlap data transfers and kernel execution.\n","- use mixed precision for forward pass with `torch.autocast`.\n","- set gradients to `None` instead of zero."]},{"cell_type":"markdown","id":"21a41065","metadata":{"id":"21a41065"},"source":["**Gradient Scaling**\n","\n","If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\n","\n","To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\n","\n","The method `step(optimizer, *args, **kwargs)` internally invokes `unscale_(optimizer)`and if no inf/NaN gradients are found, invokes `optimizer.step()` using the unscaled gradients. Otherwise `optimizer.step()` is skipped to avoid corrupting the params.\n","\n","\\**Note for Gradient Clipping*\n","\n","If you wish to modify the gradients (like in gradient clipping), you should unscale them first. If you attempted to clip *without* unscaling, the gradients' norm magnitude would also be scaled, so your requested threshold would be invalid."]},{"cell_type":"markdown","id":"3445c9b5","metadata":{"id":"3445c9b5"},"source":["I took inspiration from the Hugginface [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class for mine. "]},{"cell_type":"code","execution_count":null,"id":"b6d33065","metadata":{"id":"b6d33065"},"outputs":[],"source":["class Trainer:    \n","    def __init__(\n","        self,\n","        device: str,\n","        model: nn.Module,\n","        args: CustomTrainingArguments,\n","        train_dataloader: DataLoader,\n","        valid_dataloader: DataLoader,\n","        criterion: torch.nn,\n","        optimizer: torch.optim.Optimizer,\n","        scheduler: torch.optim.lr_scheduler = None,\n","    ):\n","        \n","        self.model = model\n","        self.train_dataloader = train_dataloader\n","        self.valid_dataloader = valid_dataloader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.device = device\n","        \n","        assert args is not None, \"No training arguments passed!\"\n","        self.args = args\n","        \n","    def train(self):\n","        args = self.args\n","        valid_dataloader = self.valid_dataloader\n","        epochs = args.num_train_epochs\n","        \n","        train_losses = []\n","        train_acc_list = []\n","        valid_losses = []\n","        valid_acc_list = []\n","        \n","        if args.use_early_stopping:\n","            patience_counter = 0 \n","\n","        scaler = GradScaler() if args.use_scaler else None\n","\n","        training_start_time = time.time()\n","        print(\"\\nTraining...\")\n","        for epoch in range(epochs):\n","            train_loss, train_acc = self._inner_training_loop(scaler)\n","            train_losses.append(train_loss)\n","            train_acc_list.append(train_acc)\n","\n","            valid_loss, valid_acc = self.evaluate(valid_dataloader)\n","            valid_losses.append(valid_loss)\n","            valid_acc_list.append(valid_acc)\n","\n","            if self.scheduler is not None:\n","                self._print_sceduler_lr()\n","                self.scheduler.step()\n","\n","            self._print_epoch_log(epoch, epochs, train_loss, valid_loss, valid_acc)\n","\n","            if args.use_early_stopping and len(valid_acc_list) >= 2:\n","                stop, patience_counter = self._early_stopping(patience_counter, epoch, valid_acc_list)\n","                if stop:\n","                    break\n","        \n","        training_time = time.time() - training_start_time\n","        print(f'Training time: {self._print_time(training_time)}')\n","\n","        metrics_history = {\n","            \"train_losses\": train_losses,\n","            \"train_acc\": train_acc_list,\n","            \"valid_losses\": valid_losses,\n","            \"valid_acc\": valid_acc_list,\n","        }\n","\n","#         print(metrics_history)\n","        if args.save_model:\n","            self._save_model(args.task_type, epoch, valid_acc, scaler, metrics_history)\n","    \n","        return metrics_history\n","\n","    def _inner_training_loop(self, scaler):\n","        args = self.args\n","        train_dataloader = self.train_dataloader\n","        \n","        train_loss = 0.0\n","        train_correct, total_count = 0.0, 0.0\n","\n","        self.model.train()\n","        for step, sample in enumerate(train_dataloader):\n","            ### Empty gradients ###\n","            self.optimizer.zero_grad(set_to_none=True)\n","            \n","            ### Forward ###\n","            if scaler is None:\n","                predictions = self.model(sample)\n","                labels = sample['labels']\n","                loss = self.criterion(predictions, labels)\n","            \n","            else:\n","                with torch.autocast(device_type=self.device):\n","                    predictions = self.model(sample)\n","                    labels = sample['labels']\n","                    loss = self.criterion(predictions, labels)\n","\n","            train_correct += (predictions.argmax(1) == labels).sum().item()\n","            total_count += labels.shape[0]\n","            \n","            ### Backward  ###\n","            if scaler is None:\n","                loss.backward()\n","            else: \n","                # Backward pass without mixed precision\n","                # It's not recommended to use mixed precision for backward pass\n","                # Because we need more precise loss\n","                scaler.scale(loss).backward()\n","            \n","            if args.grad_clipping is not None:\n","                if scaler is not None:\n","                    scaler.unscale_(self.optimizer)\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.grad_clipping)\n","            \n","            ### Update weights ### \n","            if scaler is None:\n","                self.optimizer.step()\n","            else:\n","                scaler.step(self.optimizer)\n","                scaler.update()\n","\n","            train_loss += loss.item()\n","            \n","            if step % args.logging_steps == args.logging_steps - 1:\n","                running_loss = train_loss / (step + 1)  \n","                running_acc = train_correct / total_count\n","                self._print_step_log(step, running_loss, running_acc)\n","        \n","        return train_loss / len(train_dataloader), train_correct / total_count\n","\n","\n","    def evaluate(self, eval_dataloader):\n","        valid_loss = 0.0\n","        eval_correct, total_count = 0, 0\n","        \n","        self.model.eval()\n","        with torch.no_grad():\n","            for sample in eval_dataloader:\n","                \n","                predictions = self.model(sample)\n","                labels = sample['labels']\n","                \n","                loss = self.criterion(predictions, labels)\n","                valid_loss += loss.item()\n","\n","                eval_correct += (predictions.argmax(1) == labels).sum().item()\n","                total_count += labels.shape[0]\n","        \n","        return valid_loss / len(eval_dataloader), eval_correct / total_count\n","    \n","    def _early_stopping(self, patience_counter, epoch, valid_acc_list):\n","        args = self.args\n","\n","        # stop = args.early_stopping_mode == 'min' and epoch > 0 and valid_acc_list[-1] > valid_acc_list[-2]\n","        stop = args.early_stopping_mode == 'max' and epoch > 0 and valid_acc_list[-1] < valid_acc_list[-2]\n","        if stop:\n","            if patience_counter >= args.early_stopping_patience:\n","                print('Early stop.')\n","                return stop, patience_counter\n","            else:\n","                print('-- Patience.\\n')\n","                patience_counter += 1\n","\n","        return False, patience_counter   \n","    \n","    def _print_time(self, s):\n","        m = math.floor(s / 60)\n","        s -= m * 60\n","        return '%dm %ds' % (m, s)\n","\n","    def _print_sceduler_lr(self):\n","        print('-' * 17)\n","        print(f\"| LR: {self.scheduler.get_last_lr()[0]:.3e} |\")\n","\n","    def _print_step_log(self, step, running_loss, running_acc):\n","        print(f'\\t| step {step+1:4d}/{len(self.train_dataloader):d} | train_loss: {running_loss:.3f} | ' \\\n","                f'train_acc: {running_acc:.3f} |')\n","\n","    def _print_epoch_log(self, epoch, epochs, train_loss, valid_loss, valid_acc):\n","        print('-' * 76)\n","        print(f'| epoch {epoch+1:>3d}/{epochs:<3d} | train_loss: {train_loss:.3f} | ' \\\n","                f'valid_loss: {valid_loss:.3f} | valid_acc: {valid_acc:.3f} |')\n","        print('-' * 76)\n","        \n","    \n","    def _save_model(self, task_type, epoch, valid_acc, scaler, metrics_history):\n","        print(\"Saving model...\")\n","        params_to_save = {\n","            \"epoch\": epoch,\n","            \"model_state_dict\": self.model.state_dict(),\n","            \"optimizer_state_dict\": self.optimizer.state_dict(),\n","            \"metrics_history\": metrics_history,\n","        }\n","        \n","        if self.scheduler is not None:\n","            params_to_save[\"scheduler_state_dict\"] = self.scheduler.state_dict()\n","            \n","        if scaler is not None:\n","            params_to_save[\"scaler_state_dict\"] = scaler.state_dict()\n","            \n","        save_path = f\"{self.args.output_dir}my_model{str(task_type)}_{str(valid_acc)[2:5]}_{epoch+1}\"\n","        now = datetime.now()\n","        current_time = now.strftime(\"%H-%M-%S\")\n","        \n","        if os.path.exists(f\"{save_path}_{current_time}.pth\"):\n","            torch.save(params_to_save, f\"{save_path}_{current_time}_new.pth\")\n","        else:\n","            torch.save(params_to_save, f\"{save_path}_{current_time}.pth\")\n","        \n","        print(\"Model saved.\")\n"]},{"cell_type":"code","execution_count":null,"id":"VqIdwts8ibLC","metadata":{"id":"VqIdwts8ibLC"},"outputs":[],"source":["def freeze_weights(modules):\n","    for module in modules:\n","        for param in module.parameters():\n","            if hasattr(param, 'requires_grad'):\n","                param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"id":"31aa4455","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6888,"status":"ok","timestamp":1662994367173,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"31aa4455","outputId":"c84f7ea8-4518-4aca-d2e5-17331840b300","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CustomTrainingArguments(output_dir='../../model/checkpoints/', task_type=3, save_model=True, num_train_epochs=3, logging_steps=250, learning_rate=8e-06, grad_clipping=None, use_early_stopping=True, early_stopping_mode='max', early_stopping_patience=2, use_scaler=True)\n"]}],"source":["model = CR_Model(model_name_or_path, tokenizer, model_args).to(device, non_blocking=True)\n","\n","last_frozen_layer = 12\n","modules = [model.bert.embeddings, *model.bert.encoder.layer[:last_frozen_layer]]\n","# modules = [*model.bert.encoder.layer[:last_frozen_layer]]\n","freeze_weights(modules)\n","\n","yaml_file = \"./train_notebook2.yaml\"\n","# Read configuration file with all the necessary parameters\n","with open(yaml_file) as file:\n","    config = yaml.safe_load(file)\n","    \n","training_args = CustomTrainingArguments(**config['training_args'])\n","\n","# Make sure that the learning rate is read as a number and not as a string\n","training_args.learning_rate = float(training_args.learning_rate)\n","print(training_args)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device=device, non_blocking=True)\n","optimizer = torch.optim.Adam(model.parameters(), lr=training_args.learning_rate)\n","# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n","scheduler = None\n","\n","batch_size = 4\n","\n","collator = Collator(device)\n","train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n","                              collate_fn=collator, shuffle=True)\n","valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, \n","                              collate_fn=collator, shuffle=False)\n","\n","trainer = Trainer(str(device), model, training_args, \n","                  train_dataloader, valid_dataloader, \n","                  criterion, optimizer, scheduler)\n"]},{"cell_type":"code","execution_count":null,"id":"raIciVFcb9-_","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6839,"status":"ok","timestamp":1662931525530,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"raIciVFcb9-_","outputId":"fc58e92b-d111-4a93-9602-068f3e3d49c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["CustomTrainingArguments(output_dir='../../model/checkpoints/', task_type=3, save_model=True, num_train_epochs=3, logging_steps=250, learning_rate=8e-06, grad_clipping=None, use_early_stopping=True, early_stopping_mode='max', early_stopping_patience=2, use_scaler=True)\n"]}],"source":["model = Multi_Choice_Model(model_name_or_path, tokenizer, model_args).to(device, non_blocking=True)\n","\n","last_frozen_layer = 12\n","modules = [model.bert.embeddings, *model.bert.encoder.layer[:last_frozen_layer]]\n","freeze_weights(modules)\n","\n","yaml_file = \"./train_notebook2.yaml\"\n","# Read configuration file with all the necessary parameters\n","with open(yaml_file) as file:\n","    config = yaml.safe_load(file)\n","    \n","training_args = CustomTrainingArguments(**config['training_args'])\n","\n","# Make sure that the learning rate is read as a number and not as a string\n","training_args.learning_rate = float(training_args.learning_rate)\n","print(training_args)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device=device, non_blocking=True)\n","optimizer = torch.optim.Adam(model.parameters(), lr=training_args.learning_rate)\n","# scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n","scheduler = None\n","\n","batch_size = 4\n","\n","collator = Collator_Multi_Choice(device)\n","train_dataloader = DataLoader(train_ds, batch_size=batch_size, \n","                              collate_fn=collator, shuffle=True)\n","valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, \n","                              collate_fn=collator, shuffle=False)\n","\n","trainer = Trainer(str(device), model, training_args, \n","                  train_dataloader, valid_dataloader, \n","                  criterion, optimizer, scheduler)\n"]},{"cell_type":"code","execution_count":null,"id":"47ce7693","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484236,"status":"ok","timestamp":1662994851394,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"47ce7693","outputId":"c71dff4a-c940-4fd6-81ca-c82b77e3fcb1","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training...\n","\t| step  250/750 | train_loss: 1.141 | train_acc: 0.326 |\n","\t| step  500/750 | train_loss: 1.015 | train_acc: 0.469 |\n","\t| step  750/750 | train_loss: 0.942 | train_acc: 0.537 |\n","----------------------------------------------------------------------------\n","| epoch   1/3   | train_loss: 0.942 | valid_loss: 0.603 | valid_acc: 0.769 |\n","----------------------------------------------------------------------------\n","\t| step  250/750 | train_loss: 0.633 | train_acc: 0.755 |\n","\t| step  500/750 | train_loss: 0.621 | train_acc: 0.762 |\n","\t| step  750/750 | train_loss: 0.605 | train_acc: 0.773 |\n","----------------------------------------------------------------------------\n","| epoch   2/3   | train_loss: 0.605 | valid_loss: 0.447 | valid_acc: 0.844 |\n","----------------------------------------------------------------------------\n","\t| step  250/750 | train_loss: 0.498 | train_acc: 0.829 |\n","\t| step  500/750 | train_loss: 0.508 | train_acc: 0.821 |\n","\t| step  750/750 | train_loss: 0.492 | train_acc: 0.826 |\n","----------------------------------------------------------------------------\n","| epoch   3/3   | train_loss: 0.492 | valid_loss: 0.365 | valid_acc: 0.885 |\n","----------------------------------------------------------------------------\n","Training time: 7m 53s\n","Saving model...\n","Model saved.\n"]}],"source":["metrics_history = trainer.train()"]},{"cell_type":"markdown","id":"b8e96824","metadata":{"id":"b8e96824"},"source":["With the following three cells I applied K fold Cross validation for some tests. I did not perform other investigation since it is really time consuming with BERT. "]},{"cell_type":"code","execution_count":null,"id":"kXdSjw6viTEH","metadata":{"id":"kXdSjw6viTEH"},"outputs":[],"source":["def reset_weights(module):\n","    '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","    '''\n","    for layer in module.children():\n","        if hasattr(layer, 'reset_parameters'):\n","            layer.reset_parameters()"]},{"cell_type":"code","execution_count":null,"id":"e67cca95","metadata":{"id":"e67cca95"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from torch.utils.data import SubsetRandomSampler, ConcatDataset\n","\n","dataset = ConcatDataset([train_ds, valid_ds])\n","\n","k = 5\n","splits = KFold(n_splits=k, shuffle=True, random_state=SEED)\n","\n","fold_metrics = {}"]},{"cell_type":"code","execution_count":null,"id":"34ddc016","metadata":{"id":"34ddc016","scrolled":true},"outputs":[],"source":["for fold, (train_idx, valid_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n","\n","    print(f'Fold {fold + 1}')\n","\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    batch_size = 4\n","    \n","    collator = Collator(device)\n","    train_dataloader = DataLoader(dataset, batch_size=batch_size, \n","                                  collate_fn=collator, sampler=train_sampler)\n","    valid_dataloader = DataLoader(dataset, batch_size=batch_size, \n","                                  collate_fn=collator, sampler=valid_sampler)\n","    \n","\n","    model = CR_Model(model_name_or_path, tokenizer, model_args).to(device, non_blocking=True)\n","    model.apply(reset_weights)\n","\n","    last_frozen_layer = 12\n","\n","    modules = [model.bert.embeddings, *model.bert.encoder.layer[:last_frozen_layer]]\n","    modules = [*model.bert.encoder.layer[:last_frozen_layer]]\n","    for module in modules:\n","        for param in module.parameters():\n","            param.requires_grad = False\n","\n","    criterion = torch.nn.CrossEntropyLoss().to(device=device, non_blocking=True)\n","    training_args.learning_rate = float(training_args.learning_rate)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=training_args.learning_rate)\n","    # scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1)\n","#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n","    scheduler = None\n","\n","    \n","    trainer = Trainer(str(device), model, training_args, \n","                  train_dataloader, valid_dataloader, \n","                  criterion, optimizer, scheduler)\n","    fold_metrics[fold+1] = trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"bfcc7055","metadata":{"id":"bfcc7055"},"outputs":[],"source":["model = CR_Model(model_name_or_path, tokenizer, model_args).to(device, non_blocking=True)\n","# optimizer_resumed = torch.optim.Adam(model_resumed.parameters())"]},{"cell_type":"code","execution_count":null,"id":"f9a3799f","metadata":{"id":"f9a3799f"},"outputs":[],"source":["path = \"../../model/checkpoints/my_model3_830_5_19-26-41.pth\""]},{"cell_type":"code","execution_count":null,"id":"683896dc","metadata":{"id":"683896dc"},"outputs":[],"source":["checkpoint = torch.load(path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","# optimizer_resumed.load_state_dict(checkpoint['optimizer_state_dict'])\n","metrics_resumed = checkpoint['metrics_history']"]},{"cell_type":"code","execution_count":null,"id":"8569d8f9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":418,"status":"error","timestamp":1662925530899,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"8569d8f9","outputId":"af05a43a-db87-4efa-9503-4dfeb6ae62fc"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from matplotlib import pyplot as plt\n","plt.plot(metrics_history['train_acc'])\n","plt.plot(metrics_history['valid_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"61cd9f17","metadata":{"id":"61cd9f17"},"outputs":[],"source":["from gap_utils import *"]},{"cell_type":"code","execution_count":null,"id":"_4jBSSzrljZJ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1662917171116,"user":{"displayName":"Florin Cuconasu","userId":"13345521608196528334"},"user_tz":-120},"id":"_4jBSSzrljZJ","outputId":"dba7b85d-bde0-415e-b020-8e0d0a42bf67"},"outputs":[{"name":"stdout","output_type":"stream","text":["Counter({'her': 140, 'his': 108, 'he': 93, 'she': 87, 'him': 26})\n"]}],"source":["valid_path = \"../../model/data/dev.tsv\"\n","valid_dataset, valid_pron_counter = read_dataset(valid_path)"]},{"cell_type":"code","execution_count":null,"id":"6ca534a6","metadata":{"id":"6ca534a6"},"outputs":[],"source":["def predict(model, sentences: List[Dict], tokenizer, tag_labels, device) -> List[Tuple[Tuple[str, int], Tuple[str, int]]]:\n","    df = pd.DataFrame(sentences)\n","\n","    # tokenizer will be self.tokenizer in the final implementation\n","    dataset = GAP_Dataset(df, tokenizer, tag_labels, labeled=False, cleaned=False)\n","    collator = Collator(device, labeled=False)\n","\n","    predictions = []\n","    \n","    #self.model.eval()\n","    model.eval()\n","    with torch.no_grad():\n","        dataloader = DataLoader(dataset, batch_size=1, \n","                                collate_fn=collator, shuffle=False)\n","        \n","        for sample, sentence in zip(dataloader, sentences):\n","            \n","            predicted_label_id = model(sample).argmax(1).item()\n","            pred_entity, pred_entity_offset = get_entity_and_offset_from_id(predicted_label_id, sentence)\n","            pron, pron_offset = sentence['pron'], sentence['p_offset']\n","            \n","            predictions.append(((pron, pron_offset), (pred_entity, pred_entity_offset)))\n","            \n","    return predictions\n","            "]},{"cell_type":"code","execution_count":null,"id":"v-UxrxXwcd-p","metadata":{"id":"v-UxrxXwcd-p"},"outputs":[],"source":["def multi_choice_predict(model, sentences: List[Dict], tokenizer, tag_labels, device) -> List[Tuple[Tuple[str, int], Tuple[str, int]]]:\n","    df = pd.DataFrame(sentences)\n","\n","    # tokenizer will be self.tokenizer in the final implementation\n","    dataset = GAP_Dataset(df, tokenizer, tag_labels, multiple_choice=True, labeled=False, cleaned=False)\n","    collator = Collator_Multi_Choice(device, labeled=False)\n","\n","    predictions = []\n","    \n","    #self.model.eval()\n","    model.eval()\n","    with torch.no_grad():\n","        dataloader = DataLoader(dataset, batch_size=1, \n","                                collate_fn=collator, shuffle=False)\n","        \n","        for sample, sentence in zip(dataloader, sentences):\n","            \n","            predicted_label_id = model(sample).argmax(1).item()\n","            pred_entity, pred_entity_offset = get_entity_and_offset_from_id(predicted_label_id, sentence)\n","            pron, pron_offset = sentence['pron'], sentence['p_offset']\n","            \n","            predictions.append(((pron, pron_offset), (pred_entity, pred_entity_offset)))\n","            \n","    return predictions\n","            "]},{"cell_type":"code","execution_count":null,"id":"fMBuE0KkloQe","metadata":{"id":"fMBuE0KkloQe"},"outputs":[],"source":["pred = predict(model, valid_dataset, tokenizer, tag_labels, device)"]},{"cell_type":"code","execution_count":null,"id":"krtLQFwwlswQ","metadata":{"id":"krtLQFwwlswQ"},"outputs":[],"source":["compute_metrics(pred, valid_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('DL')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"bf37a2529baf03c803266b8d55d553bda8f92a60c47b8d9f7bdcfc02cbd55fef"}}},"nbformat":4,"nbformat_minor":5}
